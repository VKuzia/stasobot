{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ea9868f",
   "metadata": {},
   "source": [
    "# Char-based text generation using RNN\n",
    "\n",
    "This notebook contains the code to create, train and save models for simple text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bce9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea7db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_alphabet(text, alphabet):\n",
    "    \"\"\"Leaves text sequence with non-alphabet symbols dropped\"\"\"\n",
    "    alphabet_set = set(alphabet)\n",
    "    return ''.join([character for character in text if character in alphabet_set])\n",
    "\n",
    "def split_data(text, seq_length, stride):\n",
    "    \"\"\"Divides given text into samples of given length with a given stride\"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(0, len(text) - seq_length - 1, stride):\n",
    "        inputs.append(text[i : i + seq_length])\n",
    "        targets.append(text[i + 1 : i + seq_length + 1])\n",
    "    return inputs, targets\n",
    "\n",
    "def integerify(list_of_strings):\n",
    "    \"\"\"Translates chars to their integer-class representation\"\"\"\n",
    "    result = []\n",
    "    for string in list_of_strings:\n",
    "        result.append([char_to_index[x] for x in string])\n",
    "    return result\n",
    "\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    \"\"\"Applies one-hot encodding to a given sequence. Is used to create classification labels\"\"\"\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    return one_hot\n",
    "\n",
    "def collate_function(batch):\n",
    "    \"\"\"Creates a torch.Tensor based on batch contents\"\"\"\n",
    "    sample_list = integerify([first for first, second in batch])\n",
    "    label_list = integerify([second for first, second in batch])\n",
    "    return torch.tensor(sample_list), torch.tensor(label_list)\n",
    "\n",
    "def get_data_from_file(path, alphabet, seq_length, batch_size, stride, train_size = 0.75):\n",
    "    \"\"\"Loads the data from a given file and produces train|validation dataloaders\"\"\"\n",
    "    with open(path, \"r\") as text_file:\n",
    "        text = text_file.read()\n",
    "    text = filter_by_alphabet(text, alphabet)\n",
    "    text_inputs, text_targets = split_data(text, seq_length, stride)\n",
    "    data = list(zip(text_inputs, text_targets))\n",
    "    display(len(data))\n",
    "    train_size = int(train_size * len(data))\n",
    "    val_size = len(data) - train_size\n",
    "    train_data, val_data = torch.utils.data.random_split(data, [train_size, val_size])\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_function,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    validation_dataloader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_function,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_dataloader, validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f323134",
   "metadata": {},
   "outputs": [],
   "source": [
    "special = '$' # is used as a message delimiter\n",
    "alphabet='абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ .,!?\\n-\"'\n",
    "# alphabet = alphabet + special\n",
    "char_to_index = {alphabet[i]:i for i in range(len(alphabet))}\n",
    "\n",
    "seq_length = 120\n",
    "batch_size = 64\n",
    "stride = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a386bf",
   "metadata": {},
   "source": [
    "Loading data from a given file. You might change \"dataset.txt\" to your txt file to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c354ea0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12741343"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataloader, validation_dataloader = get_data_from_file('dataset.txt', alphabet, seq_length, batch_size, stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51845fe8",
   "metadata": {},
   "source": [
    "Next section defines an RNN based on LSTM layers. Default parameters are chosen in a way to fit \n",
    "sequences of length 120-150 into my GTX 1650Ti (4 GB). You might change these if you have more GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d777e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, chars_num, n_hidden=512, n_layers=4, drop_prob=0.4):\n",
    "        super().__init__()\n",
    "        self.chars_num = chars_num\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            chars_num, n_hidden, n_layers, dropout=drop_prob, batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(n_hidden, chars_num)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.linear(out)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initializes hidden state with zeros.\"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (\n",
    "            weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "            weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "        )\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782624c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    net,\n",
    "    train_data,\n",
    "    val_data,\n",
    "    full_train,\n",
    "    epochs=10,\n",
    "    batches_per_epoch=100,\n",
    "    batch_size=64,\n",
    "    seq_length=100,\n",
    "    lr=0.001,\n",
    "    clip=5,\n",
    "    val_frac=0.1,\n",
    "    print_every=10,\n",
    "):\n",
    "    \"\"\"Performs trainig cycle on a given model with a given data\"\"\"\n",
    "    net.train()\n",
    "    for p in net.lstm.parameters():\n",
    "        p.requires_grad = full_train\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    counter = 0\n",
    "    n_chars = net.chars_num\n",
    "    for e in range(epochs):\n",
    "        for _ in range(batches_per_epoch):\n",
    "            h = net.init_hidden(batch_size)\n",
    "            x, y = next(iter(train_data))\n",
    "            x = one_hot_encode(x, len(alphabet))\n",
    "            y = one_hot_encode(y, len(alphabet)).reshape(-1, len(alphabet))\n",
    "            inputs, targets = torch.from_numpy(x).cuda(), torch.from_numpy(y).cuda()\n",
    "            h = tuple([each.data for each in h])\n",
    "            net.zero_grad()\n",
    "            output, h = net(inputs, h)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                x, y = next(iter(val_data))\n",
    "                x = one_hot_encode(x, len(alphabet))\n",
    "                y = one_hot_encode(y, len(alphabet)).reshape(-1, len(alphabet))\n",
    "                inputs, targets = torch.from_numpy(x).cuda(), torch.from_numpy(y).cuda()\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output, targets)\n",
    "                val_losses.append(val_loss.item())\n",
    "                net.train()\n",
    "\n",
    "                print(\n",
    "                    \"Epoch: {}/{}...\".format(e + 1, epochs),\n",
    "                    \"Step: {}...\".format(counter),\n",
    "                    \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                    \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
    "                )\n",
    "\n",
    "            counter += 1\n",
    "        torch.save(net.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e730d782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, temperature=1, top_k=None):\n",
    "        \"\"\"\n",
    "        Predicts the next char, based on current hidden state and current char.\n",
    "        Temperature is a parameter defining the sharpness of softmax layer in final classification.\n",
    "        If temperature is high enough, it will produce only frequent phrases used by dataset \n",
    "        (as maximum of out layer becomes far more probable to be chosen),\n",
    "        If temperature is low, it will make model's output totaly random (as it smoothes softmax layer results totaly).\n",
    "        top_k is a parameter defining how many char variants should the model consider in each classification step.\n",
    "        \"\"\"\n",
    "        x = np.array(integerify([char]))\n",
    "        x = one_hot_encode(x, len(alphabet))\n",
    "        inputs = torch.from_numpy(x).cuda()\n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = net(inputs, h)\n",
    "        out = torch.exp(temperature * out)\n",
    "        p = torch.nn.functional.softmax(out, dim=1).data.cpu()\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        return alphabet[char], h\n",
    "    \n",
    "    \n",
    "def sample(net, size, prime, temperature = 1, top_k=None):    \n",
    "    \"\"\"Produces the text generated after given prime text. Uses predict in a char-based manner.\"\"\"\n",
    "    net.eval()\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, temperature, top_k=top_k)\n",
    "    chars.append(char)\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, temperature, top_k=top_k)\n",
    "        chars.append(char)\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82d7a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CharRNN(len(alphabet), 1024, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "877b4e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f3c1d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(\n",
    "    net,\n",
    "    full_train=True,\n",
    "    train_data=train_dataloader,\n",
    "    val_data=validation_dataloader,\n",
    "    epochs=20,\n",
    "    batches_per_epoch = 1000,\n",
    "    batch_size=batch_size,\n",
    "    seq_length=seq_length,\n",
    "    lr=0.01,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cabf547",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'output_net.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e063db5",
   "metadata": {},
   "source": [
    "Sampling via trained network with given text start, temperature and top_k parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample(net, 400, prime='Однажды в', temperature = 0.4, top_k=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample(net, 600, prime='В одном городе', temperature = 0.7, top_k=7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
