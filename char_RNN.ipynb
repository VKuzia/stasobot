{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bce9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea7db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_alphabet(text, alphabet):\n",
    "    alphabet_set = set(alphabet)\n",
    "    return ''.join([character for character in text if character in alphabet_set])\n",
    "\n",
    "def split_data(text, seq_length, stride):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(0, len(text) - seq_length - 1, stride):\n",
    "        inputs.append(text[i : i + seq_length])\n",
    "        targets.append(text[i + 1 : i + seq_length + 1])\n",
    "    return inputs, targets\n",
    "\n",
    "def integerify(list_of_strings):\n",
    "    result = []\n",
    "    for string in list_of_strings:\n",
    "        result.append([char_to_index[x] for x in string])\n",
    "    return result\n",
    "\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    return one_hot\n",
    "\n",
    "def collate_function(batch):\n",
    "    sample_list = integerify([first for first, second in batch])\n",
    "    label_list = integerify([second for first, second in batch])\n",
    "    return torch.tensor(sample_list), torch.tensor(label_list)\n",
    "\n",
    "def get_data_from_file(path, alphabet, seq_length, batch_size, stride, train_size = 0.75):\n",
    "    with open(path, \"r\") as text_file:\n",
    "        text = text_file.read()\n",
    "    text = filter_by_alphabet(text, alphabet)\n",
    "    text_inputs, text_targets = split_data(text, seq_length, stride)\n",
    "    data = list(zip(text_inputs, text_targets))\n",
    "    display(len(data))\n",
    "    train_size = int(train_size * len(data))\n",
    "    val_size = len(data) - train_size\n",
    "    train_data, val_data = torch.utils.data.random_split(data, [train_size, val_size])\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_function,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    validation_dataloader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_function,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_dataloader, validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f323134",
   "metadata": {},
   "outputs": [],
   "source": [
    "special = '$'\n",
    "alphabet='абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ .,!?\\n-\"'\n",
    "# alphabet = alphabet + special\n",
    "char_to_index = {alphabet[i]:i for i in range(len(alphabet))}\n",
    "\n",
    "seq_length = 120\n",
    "batch_size = 64\n",
    "stride = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84eb38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c354ea0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12741343"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataloader, validation_dataloader = get_data_from_file('dataset.txt', alphabet, seq_length, batch_size, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d777e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, chars_num, n_hidden=512, n_layers=4, drop_prob=0.4):\n",
    "        super().__init__()\n",
    "        self.chars_num = chars_num\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            chars_num, n_hidden, n_layers, dropout=drop_prob, batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(n_hidden, chars_num)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.linear(out)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (\n",
    "            weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "            weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "        )\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782624c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    net,\n",
    "    train_data,\n",
    "    val_data,\n",
    "    full_train,\n",
    "    epochs=10,\n",
    "    batches_per_epoch=100,\n",
    "    batch_size=64,\n",
    "    seq_length=100,\n",
    "    lr=0.001,\n",
    "    clip=5,\n",
    "    val_frac=0.1,\n",
    "    print_every=10,\n",
    "):\n",
    "    net.train()\n",
    "    for p in net.lstm.parameters():\n",
    "        p.requires_grad = full_train\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    counter = 0\n",
    "    n_chars = net.chars_num\n",
    "    for e in range(epochs):\n",
    "        for _ in range(batches_per_epoch):\n",
    "            # initialize hidden state\n",
    "            h = net.init_hidden(batch_size)\n",
    "            x, y = next(iter(train_data))\n",
    "            x = one_hot_encode(x, len(alphabet))\n",
    "            y = one_hot_encode(y, len(alphabet)).reshape(-1, len(alphabet))\n",
    "            inputs, targets = torch.from_numpy(x).cuda(), torch.from_numpy(y).cuda()\n",
    "            h = tuple([each.data for each in h])\n",
    "            net.zero_grad()\n",
    "            output, h = net(inputs, h)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                x, y = next(iter(val_data))\n",
    "                x = one_hot_encode(x, len(alphabet))\n",
    "                y = one_hot_encode(y, len(alphabet)).reshape(-1, len(alphabet))\n",
    "                inputs, targets = torch.from_numpy(x).cuda(), torch.from_numpy(y).cuda()\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output, targets)\n",
    "                val_losses.append(val_loss.item())\n",
    "                net.train()\n",
    "\n",
    "                print(\n",
    "                    \"Epoch: {}/{}...\".format(e + 1, epochs),\n",
    "                    \"Step: {}...\".format(counter),\n",
    "                    \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                    \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
    "                )\n",
    "\n",
    "            counter += 1\n",
    "        torch.save(net.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e730d782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, temperature=1, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        x = np.array(integerify([char]))\n",
    "        x = one_hot_encode(x, len(alphabet))\n",
    "        inputs = torch.from_numpy(x).cuda()\n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = net(inputs, h)\n",
    "        out = torch.exp(temperature * out)\n",
    "        p = torch.nn.functional.softmax(out, dim=1).data.cpu()\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        return alphabet[char], h\n",
    "    \n",
    "    \n",
    "def sample(net, size, prime, temperature = 1, top_k=None):    \n",
    "    net.eval()\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, temperature, top_k=top_k)\n",
    "    chars.append(char)\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, temperature, top_k=top_k)\n",
    "        chars.append(char)\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82d7a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CharRNN(len(alphabet), 1024, 4)\n",
    "# net.load_state_dict(torch.load('anek_rnn_2.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "877b4e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42f3c1d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 0... Loss: 4.3613... Val Loss: 3.5323\n",
      "Epoch: 1/20... Step: 10... Loss: 3.7824... Val Loss: 12.3307\n",
      "Epoch: 1/20... Step: 20... Loss: 3.3800... Val Loss: 7.7811\n",
      "Epoch: 1/20... Step: 30... Loss: 3.2478... Val Loss: 3.4283\n",
      "Epoch: 1/20... Step: 40... Loss: 3.2979... Val Loss: 3.2997\n",
      "Epoch: 1/20... Step: 50... Loss: 3.2151... Val Loss: 3.4866\n",
      "Epoch: 1/20... Step: 60... Loss: 3.2087... Val Loss: 3.6405\n",
      "Epoch: 1/20... Step: 70... Loss: 3.2290... Val Loss: 3.6183\n",
      "Epoch: 1/20... Step: 80... Loss: 3.1987... Val Loss: 3.7429\n",
      "Epoch: 1/20... Step: 90... Loss: 3.2078... Val Loss: 3.5197\n",
      "Epoch: 1/20... Step: 100... Loss: 3.1927... Val Loss: 3.5773\n",
      "Epoch: 1/20... Step: 110... Loss: 3.2165... Val Loss: 3.8504\n",
      "Epoch: 1/20... Step: 120... Loss: 3.2180... Val Loss: 3.9165\n",
      "Epoch: 1/20... Step: 130... Loss: 3.2262... Val Loss: 3.7716\n",
      "Epoch: 1/20... Step: 140... Loss: 3.2301... Val Loss: 3.8685\n",
      "Epoch: 1/20... Step: 150... Loss: 3.2346... Val Loss: 3.9357\n",
      "Epoch: 1/20... Step: 160... Loss: 3.1932... Val Loss: 3.8089\n",
      "Epoch: 1/20... Step: 170... Loss: 3.2450... Val Loss: 3.8134\n",
      "Epoch: 1/20... Step: 180... Loss: 3.2300... Val Loss: 3.9092\n",
      "Epoch: 1/20... Step: 190... Loss: 3.2107... Val Loss: 3.9144\n",
      "Epoch: 1/20... Step: 200... Loss: 3.1917... Val Loss: 3.8683\n",
      "Epoch: 1/20... Step: 210... Loss: 3.2366... Val Loss: 3.8817\n",
      "Epoch: 1/20... Step: 220... Loss: 3.2154... Val Loss: 3.8190\n",
      "Epoch: 1/20... Step: 230... Loss: 3.2028... Val Loss: 3.9590\n",
      "Epoch: 1/20... Step: 240... Loss: 3.2111... Val Loss: 4.0444\n",
      "Epoch: 1/20... Step: 250... Loss: 3.2225... Val Loss: 3.8960\n",
      "Epoch: 1/20... Step: 260... Loss: 3.2226... Val Loss: 3.7428\n",
      "Epoch: 1/20... Step: 270... Loss: 3.2473... Val Loss: 3.7606\n",
      "Epoch: 1/20... Step: 280... Loss: 3.2604... Val Loss: 4.0131\n",
      "Epoch: 1/20... Step: 290... Loss: 3.2201... Val Loss: 3.8693\n",
      "Epoch: 1/20... Step: 300... Loss: 3.2234... Val Loss: 3.7798\n",
      "Epoch: 1/20... Step: 310... Loss: 3.2250... Val Loss: 3.8515\n",
      "Epoch: 1/20... Step: 320... Loss: 3.1936... Val Loss: 3.9420\n",
      "Epoch: 1/20... Step: 330... Loss: 3.1948... Val Loss: 3.8227\n",
      "Epoch: 1/20... Step: 340... Loss: 3.1950... Val Loss: 3.7028\n",
      "Epoch: 1/20... Step: 350... Loss: 3.2127... Val Loss: 3.7509\n",
      "Epoch: 1/20... Step: 360... Loss: 3.2278... Val Loss: 3.6974\n",
      "Epoch: 1/20... Step: 370... Loss: 3.2071... Val Loss: 3.7999\n",
      "Epoch: 1/20... Step: 380... Loss: 3.2113... Val Loss: 3.8505\n",
      "Epoch: 1/20... Step: 390... Loss: 3.2331... Val Loss: 3.7949\n",
      "Epoch: 1/20... Step: 400... Loss: 3.2084... Val Loss: 3.6820\n",
      "Epoch: 1/20... Step: 410... Loss: 3.2321... Val Loss: 3.5940\n",
      "Epoch: 1/20... Step: 420... Loss: 3.2093... Val Loss: 3.7182\n",
      "Epoch: 1/20... Step: 430... Loss: 3.2307... Val Loss: 3.7928\n",
      "Epoch: 1/20... Step: 440... Loss: 3.2232... Val Loss: 3.6870\n",
      "Epoch: 1/20... Step: 450... Loss: 3.2003... Val Loss: 3.5789\n",
      "Epoch: 1/20... Step: 460... Loss: 3.2184... Val Loss: 3.5619\n",
      "Epoch: 1/20... Step: 470... Loss: 3.2245... Val Loss: 3.5306\n",
      "Epoch: 1/20... Step: 480... Loss: 3.2523... Val Loss: 3.5932\n",
      "Epoch: 1/20... Step: 490... Loss: 3.2132... Val Loss: 3.5718\n",
      "Epoch: 1/20... Step: 500... Loss: 3.1872... Val Loss: 3.5323\n",
      "Epoch: 1/20... Step: 510... Loss: 3.2249... Val Loss: 3.5023\n",
      "Epoch: 1/20... Step: 520... Loss: 3.2161... Val Loss: 3.4656\n",
      "Epoch: 1/20... Step: 530... Loss: 3.2055... Val Loss: 3.5182\n",
      "Epoch: 1/20... Step: 540... Loss: 3.2135... Val Loss: 3.5116\n",
      "Epoch: 1/20... Step: 550... Loss: 3.2141... Val Loss: 3.4465\n",
      "Epoch: 1/20... Step: 560... Loss: 3.2096... Val Loss: 3.4207\n",
      "Epoch: 1/20... Step: 570... Loss: 3.2151... Val Loss: 3.4331\n",
      "Epoch: 1/20... Step: 580... Loss: 3.2171... Val Loss: 3.4190\n",
      "Epoch: 1/20... Step: 590... Loss: 3.2091... Val Loss: 3.3526\n",
      "Epoch: 1/20... Step: 600... Loss: 3.2110... Val Loss: 3.3325\n",
      "Epoch: 1/20... Step: 610... Loss: 3.2142... Val Loss: 3.3606\n",
      "Epoch: 1/20... Step: 620... Loss: 3.1878... Val Loss: 3.3670\n",
      "Epoch: 1/20... Step: 630... Loss: 3.2255... Val Loss: 3.3914\n",
      "Epoch: 1/20... Step: 640... Loss: 3.2158... Val Loss: 3.3561\n",
      "Epoch: 1/20... Step: 650... Loss: 3.2168... Val Loss: 3.2989\n",
      "Epoch: 1/20... Step: 660... Loss: 3.2252... Val Loss: 3.2945\n",
      "Epoch: 1/20... Step: 670... Loss: 3.2004... Val Loss: 3.2943\n",
      "Epoch: 1/20... Step: 680... Loss: 3.2405... Val Loss: 3.3031\n",
      "Epoch: 1/20... Step: 690... Loss: 3.2006... Val Loss: 3.2897\n",
      "Epoch: 1/20... Step: 700... Loss: 3.2364... Val Loss: 3.2915\n",
      "Epoch: 1/20... Step: 710... Loss: 3.2121... Val Loss: 3.3128\n",
      "Epoch: 1/20... Step: 720... Loss: 3.2122... Val Loss: 3.3059\n",
      "Epoch: 1/20... Step: 730... Loss: 3.2127... Val Loss: 3.2786\n",
      "Epoch: 1/20... Step: 740... Loss: 3.2043... Val Loss: 3.2394\n",
      "Epoch: 1/20... Step: 750... Loss: 3.2312... Val Loss: 3.2686\n",
      "Epoch: 1/20... Step: 760... Loss: 3.2365... Val Loss: 3.2511\n",
      "Epoch: 1/20... Step: 770... Loss: 3.1865... Val Loss: 3.2617\n",
      "Epoch: 1/20... Step: 780... Loss: 3.2360... Val Loss: 3.2560\n",
      "Epoch: 1/20... Step: 790... Loss: 3.2083... Val Loss: 3.2452\n",
      "Epoch: 1/20... Step: 800... Loss: 3.2097... Val Loss: 3.2180\n",
      "Epoch: 1/20... Step: 810... Loss: 3.2130... Val Loss: 3.2335\n",
      "Epoch: 1/20... Step: 820... Loss: 3.2259... Val Loss: 3.2638\n",
      "Epoch: 1/20... Step: 830... Loss: 3.1846... Val Loss: 3.2326\n",
      "Epoch: 1/20... Step: 840... Loss: 3.1992... Val Loss: 3.2421\n",
      "Epoch: 1/20... Step: 850... Loss: 3.2265... Val Loss: 3.2253\n",
      "Epoch: 1/20... Step: 860... Loss: 3.1986... Val Loss: 3.2304\n",
      "Epoch: 1/20... Step: 870... Loss: 3.1937... Val Loss: 3.2104\n",
      "Epoch: 1/20... Step: 880... Loss: 3.2114... Val Loss: 3.2362\n",
      "Epoch: 1/20... Step: 890... Loss: 3.2343... Val Loss: 3.2356\n",
      "Epoch: 1/20... Step: 900... Loss: 3.2400... Val Loss: 3.2251\n",
      "Epoch: 1/20... Step: 910... Loss: 3.2391... Val Loss: 3.2192\n",
      "Epoch: 1/20... Step: 920... Loss: 3.2226... Val Loss: 3.2417\n",
      "Epoch: 1/20... Step: 930... Loss: 3.1913... Val Loss: 3.2110\n",
      "Epoch: 1/20... Step: 940... Loss: 3.2095... Val Loss: 3.2151\n",
      "Epoch: 1/20... Step: 950... Loss: 3.2061... Val Loss: 3.2203\n",
      "Epoch: 1/20... Step: 960... Loss: 3.2120... Val Loss: 3.2475\n",
      "Epoch: 1/20... Step: 970... Loss: 3.2199... Val Loss: 3.2382\n",
      "Epoch: 1/20... Step: 980... Loss: 3.2380... Val Loss: 3.2020\n",
      "Epoch: 1/20... Step: 990... Loss: 3.2174... Val Loss: 3.2392\n",
      "Epoch: 2/20... Step: 1000... Loss: 3.2005... Val Loss: 3.2451\n",
      "Epoch: 2/20... Step: 1010... Loss: 3.2120... Val Loss: 3.2247\n",
      "Epoch: 2/20... Step: 1020... Loss: 3.2087... Val Loss: 3.2224\n",
      "Epoch: 2/20... Step: 1030... Loss: 3.2044... Val Loss: 3.2073\n",
      "Epoch: 2/20... Step: 1040... Loss: 3.1831... Val Loss: 3.2191\n",
      "Epoch: 2/20... Step: 1050... Loss: 3.2115... Val Loss: 3.2047\n",
      "Epoch: 2/20... Step: 1060... Loss: 3.2128... Val Loss: 3.1986\n",
      "Epoch: 2/20... Step: 1070... Loss: 3.2230... Val Loss: 3.2043\n",
      "Epoch: 2/20... Step: 1080... Loss: 3.1930... Val Loss: 3.2040\n",
      "Epoch: 2/20... Step: 1090... Loss: 3.1867... Val Loss: 3.2281\n",
      "Epoch: 2/20... Step: 1100... Loss: 3.2045... Val Loss: 3.2047\n",
      "Epoch: 2/20... Step: 1110... Loss: 3.2211... Val Loss: 3.2123\n",
      "Epoch: 2/20... Step: 1120... Loss: 3.2151... Val Loss: 3.1994\n",
      "Epoch: 2/20... Step: 1130... Loss: 3.1997... Val Loss: 3.2185\n",
      "Epoch: 2/20... Step: 1140... Loss: 3.2196... Val Loss: 3.2007\n",
      "Epoch: 2/20... Step: 1150... Loss: 3.1980... Val Loss: 3.1891\n",
      "Epoch: 2/20... Step: 1160... Loss: 3.2207... Val Loss: 3.2075\n",
      "Epoch: 2/20... Step: 1170... Loss: 3.2189... Val Loss: 3.2074\n",
      "Epoch: 2/20... Step: 1180... Loss: 3.2112... Val Loss: 3.1994\n",
      "Epoch: 2/20... Step: 1190... Loss: 3.1937... Val Loss: 3.2140\n",
      "Epoch: 2/20... Step: 1200... Loss: 3.1992... Val Loss: 3.2256\n",
      "Epoch: 2/20... Step: 1210... Loss: 3.2040... Val Loss: 3.2152\n",
      "Epoch: 2/20... Step: 1220... Loss: 3.2039... Val Loss: 3.1997\n",
      "Epoch: 2/20... Step: 1230... Loss: 3.1904... Val Loss: 3.2390\n",
      "Epoch: 2/20... Step: 1240... Loss: 3.2028... Val Loss: 3.2183\n",
      "Epoch: 2/20... Step: 1250... Loss: 3.1886... Val Loss: 3.2021\n",
      "Epoch: 2/20... Step: 1260... Loss: 3.2080... Val Loss: 3.1897\n",
      "Epoch: 2/20... Step: 1270... Loss: 3.2056... Val Loss: 3.1972\n",
      "Epoch: 2/20... Step: 1280... Loss: 3.2163... Val Loss: 3.2084\n",
      "Epoch: 2/20... Step: 1290... Loss: 3.2092... Val Loss: 3.2050\n",
      "Epoch: 2/20... Step: 1300... Loss: 3.2005... Val Loss: 3.1963\n",
      "Epoch: 2/20... Step: 1310... Loss: 3.1996... Val Loss: 3.1984\n",
      "Epoch: 2/20... Step: 1320... Loss: 3.2172... Val Loss: 3.2083\n",
      "Epoch: 2/20... Step: 1330... Loss: 3.1917... Val Loss: 3.2235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20... Step: 1340... Loss: 3.2115... Val Loss: 3.2104\n",
      "Epoch: 2/20... Step: 1350... Loss: 3.2263... Val Loss: 3.1996\n",
      "Epoch: 2/20... Step: 1360... Loss: 3.2073... Val Loss: 3.1936\n",
      "Epoch: 2/20... Step: 1370... Loss: 3.2135... Val Loss: 3.1874\n",
      "Epoch: 2/20... Step: 1380... Loss: 3.2159... Val Loss: 3.2013\n",
      "Epoch: 2/20... Step: 1390... Loss: 3.2242... Val Loss: 3.1707\n",
      "Epoch: 2/20... Step: 1400... Loss: 3.2158... Val Loss: 3.2036\n",
      "Epoch: 2/20... Step: 1410... Loss: 3.1928... Val Loss: 3.1841\n",
      "Epoch: 2/20... Step: 1420... Loss: 3.2258... Val Loss: 3.2198\n",
      "Epoch: 2/20... Step: 1430... Loss: 3.2071... Val Loss: 3.2143\n",
      "Epoch: 2/20... Step: 1440... Loss: 3.2139... Val Loss: 3.2205\n",
      "Epoch: 2/20... Step: 1450... Loss: 3.2056... Val Loss: 3.2369\n",
      "Epoch: 2/20... Step: 1460... Loss: 3.2204... Val Loss: 3.2171\n",
      "Epoch: 2/20... Step: 1470... Loss: 3.1977... Val Loss: 3.2094\n",
      "Epoch: 2/20... Step: 1480... Loss: 3.2117... Val Loss: 3.1978\n",
      "Epoch: 2/20... Step: 1490... Loss: 3.1916... Val Loss: 3.2054\n",
      "Epoch: 2/20... Step: 1500... Loss: 3.2316... Val Loss: 3.2385\n",
      "Epoch: 2/20... Step: 1510... Loss: 3.2007... Val Loss: 3.2238\n",
      "Epoch: 2/20... Step: 1520... Loss: 3.1962... Val Loss: 3.1795\n",
      "Epoch: 2/20... Step: 1530... Loss: 3.2112... Val Loss: 3.2116\n",
      "Epoch: 2/20... Step: 1540... Loss: 3.2091... Val Loss: 3.2052\n",
      "Epoch: 2/20... Step: 1550... Loss: 3.2272... Val Loss: 3.2059\n",
      "Epoch: 2/20... Step: 1560... Loss: 3.1986... Val Loss: 3.1892\n",
      "Epoch: 2/20... Step: 1570... Loss: 3.1819... Val Loss: 3.2243\n",
      "Epoch: 2/20... Step: 1580... Loss: 3.2073... Val Loss: 3.1979\n",
      "Epoch: 2/20... Step: 1590... Loss: 3.2123... Val Loss: 3.1993\n",
      "Epoch: 2/20... Step: 1600... Loss: 3.1882... Val Loss: 3.2269\n",
      "Epoch: 2/20... Step: 1610... Loss: 3.2294... Val Loss: 3.2155\n",
      "Epoch: 2/20... Step: 1620... Loss: 3.2246... Val Loss: 3.2032\n",
      "Epoch: 2/20... Step: 1630... Loss: 3.2021... Val Loss: 3.2009\n",
      "Epoch: 2/20... Step: 1640... Loss: 3.1836... Val Loss: 3.1910\n",
      "Epoch: 2/20... Step: 1650... Loss: 3.1928... Val Loss: 3.2107\n",
      "Epoch: 2/20... Step: 1660... Loss: 3.2008... Val Loss: 3.1990\n",
      "Epoch: 2/20... Step: 1670... Loss: 3.2088... Val Loss: 3.2149\n",
      "Epoch: 2/20... Step: 1680... Loss: 3.2165... Val Loss: 3.2105\n",
      "Epoch: 2/20... Step: 1690... Loss: 3.2018... Val Loss: 3.2148\n",
      "Epoch: 2/20... Step: 1700... Loss: 3.1929... Val Loss: 3.1932\n",
      "Epoch: 2/20... Step: 1710... Loss: 3.2337... Val Loss: 3.2056\n",
      "Epoch: 2/20... Step: 1720... Loss: 3.2010... Val Loss: 3.2301\n",
      "Epoch: 2/20... Step: 1730... Loss: 3.1871... Val Loss: 3.1717\n",
      "Epoch: 2/20... Step: 1740... Loss: 3.1900... Val Loss: 3.1998\n",
      "Epoch: 2/20... Step: 1750... Loss: 3.1817... Val Loss: 3.2091\n",
      "Epoch: 2/20... Step: 1760... Loss: 3.2350... Val Loss: 3.1753\n",
      "Epoch: 2/20... Step: 1770... Loss: 3.1870... Val Loss: 3.1854\n",
      "Epoch: 2/20... Step: 1780... Loss: 3.2104... Val Loss: 3.2014\n",
      "Epoch: 2/20... Step: 1790... Loss: 3.1915... Val Loss: 3.2002\n",
      "Epoch: 2/20... Step: 1800... Loss: 3.2043... Val Loss: 3.1893\n",
      "Epoch: 2/20... Step: 1810... Loss: 3.2041... Val Loss: 3.1813\n",
      "Epoch: 2/20... Step: 1820... Loss: 3.2096... Val Loss: 3.1884\n",
      "Epoch: 2/20... Step: 1830... Loss: 3.2047... Val Loss: 3.1853\n",
      "Epoch: 2/20... Step: 1840... Loss: 3.1976... Val Loss: 3.1854\n",
      "Epoch: 2/20... Step: 1850... Loss: 3.2349... Val Loss: 3.2143\n",
      "Epoch: 2/20... Step: 1860... Loss: 3.1872... Val Loss: 3.2293\n",
      "Epoch: 2/20... Step: 1870... Loss: 3.2088... Val Loss: 3.2037\n",
      "Epoch: 2/20... Step: 1880... Loss: 3.2031... Val Loss: 3.2331\n",
      "Epoch: 2/20... Step: 1890... Loss: 3.2044... Val Loss: 3.2336\n",
      "Epoch: 2/20... Step: 1900... Loss: 3.2006... Val Loss: 3.1824\n",
      "Epoch: 2/20... Step: 1910... Loss: 3.1890... Val Loss: 3.1953\n",
      "Epoch: 2/20... Step: 1920... Loss: 3.2254... Val Loss: 3.2071\n",
      "Epoch: 2/20... Step: 1930... Loss: 3.1880... Val Loss: 3.1891\n",
      "Epoch: 2/20... Step: 1940... Loss: 3.2167... Val Loss: 3.2075\n",
      "Epoch: 2/20... Step: 1950... Loss: 3.2089... Val Loss: 3.1988\n",
      "Epoch: 2/20... Step: 1960... Loss: 3.2248... Val Loss: 3.1988\n",
      "Epoch: 2/20... Step: 1970... Loss: 3.1990... Val Loss: 3.1804\n",
      "Epoch: 2/20... Step: 1980... Loss: 3.2228... Val Loss: 3.1907\n",
      "Epoch: 2/20... Step: 1990... Loss: 3.2226... Val Loss: 3.2352\n",
      "Epoch: 3/20... Step: 2000... Loss: 3.2181... Val Loss: 3.2126\n",
      "Epoch: 3/20... Step: 2010... Loss: 3.2094... Val Loss: 3.1988\n",
      "Epoch: 3/20... Step: 2020... Loss: 3.1781... Val Loss: 3.2272\n",
      "Epoch: 3/20... Step: 2030... Loss: 3.1950... Val Loss: 3.2108\n",
      "Epoch: 3/20... Step: 2040... Loss: 3.2299... Val Loss: 3.2088\n",
      "Epoch: 3/20... Step: 2050... Loss: 3.1874... Val Loss: 3.2221\n",
      "Epoch: 3/20... Step: 2060... Loss: 3.1940... Val Loss: 3.2235\n",
      "Epoch: 3/20... Step: 2070... Loss: 3.2092... Val Loss: 3.2131\n",
      "Epoch: 3/20... Step: 2080... Loss: 3.2108... Val Loss: 3.2082\n",
      "Epoch: 3/20... Step: 2090... Loss: 3.2216... Val Loss: 3.2141\n",
      "Epoch: 3/20... Step: 2100... Loss: 3.2125... Val Loss: 3.2078\n",
      "Epoch: 3/20... Step: 2110... Loss: 3.2204... Val Loss: 3.2045\n",
      "Epoch: 3/20... Step: 2120... Loss: 3.2156... Val Loss: 3.2223\n",
      "Epoch: 3/20... Step: 2130... Loss: 3.1966... Val Loss: 3.1684\n",
      "Epoch: 3/20... Step: 2140... Loss: 3.2037... Val Loss: 3.1949\n",
      "Epoch: 3/20... Step: 2150... Loss: 3.2026... Val Loss: 3.1922\n",
      "Epoch: 3/20... Step: 2160... Loss: 3.2031... Val Loss: 3.2250\n",
      "Epoch: 3/20... Step: 2170... Loss: 3.1993... Val Loss: 3.2225\n",
      "Epoch: 3/20... Step: 2180... Loss: 3.1986... Val Loss: 3.2304\n",
      "Epoch: 3/20... Step: 2190... Loss: 3.2222... Val Loss: 3.1960\n",
      "Epoch: 3/20... Step: 2200... Loss: 3.2117... Val Loss: 3.2159\n",
      "Epoch: 3/20... Step: 2210... Loss: 3.2233... Val Loss: 3.2015\n",
      "Epoch: 3/20... Step: 2220... Loss: 3.1957... Val Loss: 3.1897\n",
      "Epoch: 3/20... Step: 2230... Loss: 3.2178... Val Loss: 3.2049\n",
      "Epoch: 3/20... Step: 2240... Loss: 3.2108... Val Loss: 3.1931\n",
      "Epoch: 3/20... Step: 2250... Loss: 3.1802... Val Loss: 3.1993\n",
      "Epoch: 3/20... Step: 2260... Loss: 3.2296... Val Loss: 3.2128\n",
      "Epoch: 3/20... Step: 2270... Loss: 3.2014... Val Loss: 3.2306\n",
      "Epoch: 3/20... Step: 2280... Loss: 3.2140... Val Loss: 3.2201\n",
      "Epoch: 3/20... Step: 2290... Loss: 3.2102... Val Loss: 3.1850\n",
      "Epoch: 3/20... Step: 2300... Loss: 3.1977... Val Loss: 3.2264\n",
      "Epoch: 3/20... Step: 2310... Loss: 3.2049... Val Loss: 3.1926\n",
      "Epoch: 3/20... Step: 2320... Loss: 3.2207... Val Loss: 3.2096\n",
      "Epoch: 3/20... Step: 2330... Loss: 3.2130... Val Loss: 3.2305\n",
      "Epoch: 3/20... Step: 2340... Loss: 3.2095... Val Loss: 3.1880\n",
      "Epoch: 3/20... Step: 2350... Loss: 3.1923... Val Loss: 3.2061\n",
      "Epoch: 3/20... Step: 2360... Loss: 3.1904... Val Loss: 3.2296\n",
      "Epoch: 3/20... Step: 2370... Loss: 3.2023... Val Loss: 3.2380\n",
      "Epoch: 3/20... Step: 2380... Loss: 3.2128... Val Loss: 3.1895\n",
      "Epoch: 3/20... Step: 2390... Loss: 3.2201... Val Loss: 3.2230\n",
      "Epoch: 3/20... Step: 2400... Loss: 3.2298... Val Loss: 3.2160\n",
      "Epoch: 3/20... Step: 2410... Loss: 3.2366... Val Loss: 3.2004\n",
      "Epoch: 3/20... Step: 2420... Loss: 3.1936... Val Loss: 3.2044\n",
      "Epoch: 3/20... Step: 2430... Loss: 3.2118... Val Loss: 3.2070\n",
      "Epoch: 3/20... Step: 2440... Loss: 3.2115... Val Loss: 3.2368\n",
      "Epoch: 3/20... Step: 2450... Loss: 3.2180... Val Loss: 3.2094\n",
      "Epoch: 3/20... Step: 2460... Loss: 3.2034... Val Loss: 3.2146\n",
      "Epoch: 3/20... Step: 2470... Loss: 3.1912... Val Loss: 3.1889\n",
      "Epoch: 3/20... Step: 2480... Loss: 3.2258... Val Loss: 3.1914\n",
      "Epoch: 3/20... Step: 2490... Loss: 3.1925... Val Loss: 3.2042\n",
      "Epoch: 3/20... Step: 2500... Loss: 3.2063... Val Loss: 3.1997\n",
      "Epoch: 3/20... Step: 2510... Loss: 3.1600... Val Loss: 3.2042\n",
      "Epoch: 3/20... Step: 2520... Loss: 3.2063... Val Loss: 3.1861\n",
      "Epoch: 3/20... Step: 2530... Loss: 3.2181... Val Loss: 3.2118\n",
      "Epoch: 3/20... Step: 2540... Loss: 3.2181... Val Loss: 3.2087\n",
      "Epoch: 3/20... Step: 2550... Loss: 3.1922... Val Loss: 3.2032\n",
      "Epoch: 3/20... Step: 2560... Loss: 3.2169... Val Loss: 3.1940\n",
      "Epoch: 3/20... Step: 2570... Loss: 3.2064... Val Loss: 3.2119\n",
      "Epoch: 3/20... Step: 2580... Loss: 3.2265... Val Loss: 3.2075\n",
      "Epoch: 3/20... Step: 2590... Loss: 3.2144... Val Loss: 3.1711\n",
      "Epoch: 3/20... Step: 2600... Loss: 3.1876... Val Loss: 3.1858\n",
      "Epoch: 3/20... Step: 2610... Loss: 3.1842... Val Loss: 3.2188\n",
      "Epoch: 3/20... Step: 2620... Loss: 3.1808... Val Loss: 3.2214\n",
      "Epoch: 3/20... Step: 2630... Loss: 3.2187... Val Loss: 3.2073\n",
      "Epoch: 3/20... Step: 2640... Loss: 3.2026... Val Loss: 3.2049\n",
      "Epoch: 3/20... Step: 2650... Loss: 3.2203... Val Loss: 3.1907\n",
      "Epoch: 3/20... Step: 2660... Loss: 3.2311... Val Loss: 3.2064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20... Step: 2670... Loss: 3.2206... Val Loss: 3.1819\n",
      "Epoch: 3/20... Step: 2680... Loss: 3.1932... Val Loss: 3.2267\n",
      "Epoch: 3/20... Step: 2690... Loss: 3.1955... Val Loss: 3.2146\n",
      "Epoch: 3/20... Step: 2700... Loss: 3.2222... Val Loss: 3.2185\n",
      "Epoch: 3/20... Step: 2710... Loss: 3.2137... Val Loss: 3.1670\n",
      "Epoch: 3/20... Step: 2720... Loss: 3.1868... Val Loss: 3.2180\n",
      "Epoch: 3/20... Step: 2730... Loss: 3.2060... Val Loss: 3.1939\n",
      "Epoch: 3/20... Step: 2740... Loss: 3.1949... Val Loss: 3.2022\n",
      "Epoch: 3/20... Step: 2750... Loss: 3.1975... Val Loss: 3.1790\n",
      "Epoch: 3/20... Step: 2760... Loss: 3.1976... Val Loss: 3.1940\n",
      "Epoch: 3/20... Step: 2770... Loss: 3.1961... Val Loss: 3.1886\n",
      "Epoch: 3/20... Step: 2780... Loss: 3.2222... Val Loss: 3.2187\n",
      "Epoch: 3/20... Step: 2790... Loss: 3.2085... Val Loss: 3.2115\n",
      "Epoch: 3/20... Step: 2800... Loss: 3.1834... Val Loss: 3.1972\n",
      "Epoch: 3/20... Step: 2810... Loss: 3.2136... Val Loss: 3.2123\n",
      "Epoch: 3/20... Step: 2820... Loss: 3.1860... Val Loss: 3.2116\n",
      "Epoch: 3/20... Step: 2830... Loss: 3.1944... Val Loss: 3.1850\n",
      "Epoch: 3/20... Step: 2840... Loss: 3.2031... Val Loss: 3.2266\n",
      "Epoch: 3/20... Step: 2850... Loss: 3.2176... Val Loss: 3.2101\n",
      "Epoch: 3/20... Step: 2860... Loss: 3.1797... Val Loss: 3.1999\n",
      "Epoch: 3/20... Step: 2870... Loss: 3.1955... Val Loss: 3.1938\n",
      "Epoch: 3/20... Step: 2880... Loss: 3.2199... Val Loss: 3.2006\n",
      "Epoch: 3/20... Step: 2890... Loss: 3.1914... Val Loss: 3.1960\n",
      "Epoch: 3/20... Step: 2900... Loss: 3.2356... Val Loss: 3.2018\n",
      "Epoch: 3/20... Step: 2910... Loss: 3.1973... Val Loss: 3.1998\n",
      "Epoch: 3/20... Step: 2920... Loss: 3.1982... Val Loss: 3.2131\n",
      "Epoch: 3/20... Step: 2930... Loss: 3.2090... Val Loss: 3.2038\n",
      "Epoch: 3/20... Step: 2940... Loss: 3.2326... Val Loss: 3.2216\n",
      "Epoch: 3/20... Step: 2950... Loss: 3.1814... Val Loss: 3.2000\n",
      "Epoch: 3/20... Step: 2960... Loss: 3.2049... Val Loss: 3.2142\n",
      "Epoch: 3/20... Step: 2970... Loss: 3.1972... Val Loss: 3.2235\n",
      "Epoch: 3/20... Step: 2980... Loss: 3.2099... Val Loss: 3.1996\n",
      "Epoch: 3/20... Step: 2990... Loss: 3.2000... Val Loss: 3.1917\n",
      "Epoch: 4/20... Step: 3000... Loss: 3.2032... Val Loss: 3.1855\n",
      "Epoch: 4/20... Step: 3010... Loss: 3.2128... Val Loss: 3.1956\n",
      "Epoch: 4/20... Step: 3020... Loss: 3.2222... Val Loss: 3.1972\n",
      "Epoch: 4/20... Step: 3030... Loss: 3.1770... Val Loss: 3.2067\n",
      "Epoch: 4/20... Step: 3040... Loss: 3.1990... Val Loss: 3.1875\n",
      "Epoch: 4/20... Step: 3050... Loss: 3.1858... Val Loss: 3.2043\n",
      "Epoch: 4/20... Step: 3060... Loss: 3.2262... Val Loss: 3.2046\n",
      "Epoch: 4/20... Step: 3070... Loss: 3.1945... Val Loss: 3.2231\n",
      "Epoch: 4/20... Step: 3080... Loss: 3.1952... Val Loss: 3.2175\n",
      "Epoch: 4/20... Step: 3090... Loss: 3.2157... Val Loss: 3.2219\n",
      "Epoch: 4/20... Step: 3100... Loss: 3.2059... Val Loss: 3.2200\n",
      "Epoch: 4/20... Step: 3110... Loss: 3.1979... Val Loss: 3.2116\n",
      "Epoch: 4/20... Step: 3120... Loss: 3.1982... Val Loss: 3.1993\n",
      "Epoch: 4/20... Step: 3130... Loss: 3.2182... Val Loss: 3.2100\n",
      "Epoch: 4/20... Step: 3140... Loss: 3.1832... Val Loss: 3.1853\n",
      "Epoch: 4/20... Step: 3150... Loss: 3.2072... Val Loss: 3.2092\n",
      "Epoch: 4/20... Step: 3160... Loss: 3.2070... Val Loss: 3.1755\n",
      "Epoch: 4/20... Step: 3170... Loss: 3.1923... Val Loss: 3.2133\n",
      "Epoch: 4/20... Step: 3180... Loss: 3.2134... Val Loss: 3.2091\n",
      "Epoch: 4/20... Step: 3190... Loss: 3.2083... Val Loss: 3.2187\n",
      "Epoch: 4/20... Step: 3200... Loss: 3.2126... Val Loss: 3.2071\n",
      "Epoch: 4/20... Step: 3210... Loss: 3.1883... Val Loss: 3.2026\n",
      "Epoch: 4/20... Step: 3220... Loss: 3.2021... Val Loss: 3.2168\n",
      "Epoch: 4/20... Step: 3230... Loss: 3.2086... Val Loss: 3.1900\n",
      "Epoch: 4/20... Step: 3240... Loss: 3.1917... Val Loss: 3.2218\n",
      "Epoch: 4/20... Step: 3250... Loss: 3.1949... Val Loss: 3.2181\n",
      "Epoch: 4/20... Step: 3260... Loss: 3.2010... Val Loss: 3.2274\n",
      "Epoch: 4/20... Step: 3270... Loss: 3.2125... Val Loss: 3.2168\n",
      "Epoch: 4/20... Step: 3280... Loss: 3.1754... Val Loss: 3.1911\n",
      "Epoch: 4/20... Step: 3290... Loss: 3.2169... Val Loss: 3.2159\n",
      "Epoch: 4/20... Step: 3300... Loss: 3.2129... Val Loss: 3.1959\n",
      "Epoch: 4/20... Step: 3310... Loss: 3.1970... Val Loss: 3.2188\n",
      "Epoch: 4/20... Step: 3320... Loss: 3.2289... Val Loss: 3.2005\n",
      "Epoch: 4/20... Step: 3330... Loss: 3.1875... Val Loss: 3.2074\n",
      "Epoch: 4/20... Step: 3340... Loss: 3.2137... Val Loss: 3.1975\n",
      "Epoch: 4/20... Step: 3350... Loss: 3.2245... Val Loss: 3.1879\n",
      "Epoch: 4/20... Step: 3360... Loss: 3.2072... Val Loss: 3.2036\n",
      "Epoch: 4/20... Step: 3370... Loss: 3.2233... Val Loss: 3.2282\n",
      "Epoch: 4/20... Step: 3380... Loss: 3.1931... Val Loss: 3.1861\n",
      "Epoch: 4/20... Step: 3390... Loss: 3.2174... Val Loss: 3.1954\n",
      "Epoch: 4/20... Step: 3400... Loss: 3.1986... Val Loss: 3.1888\n",
      "Epoch: 4/20... Step: 3410... Loss: 3.1936... Val Loss: 3.2016\n",
      "Epoch: 4/20... Step: 3420... Loss: 3.2033... Val Loss: 3.2076\n",
      "Epoch: 4/20... Step: 3430... Loss: 3.2083... Val Loss: 3.1827\n",
      "Epoch: 4/20... Step: 3440... Loss: 3.1936... Val Loss: 3.2011\n",
      "Epoch: 4/20... Step: 3450... Loss: 3.2148... Val Loss: 3.1917\n",
      "Epoch: 4/20... Step: 3460... Loss: 3.2174... Val Loss: 3.1962\n",
      "Epoch: 4/20... Step: 3470... Loss: 3.2094... Val Loss: 3.2061\n",
      "Epoch: 4/20... Step: 3480... Loss: 3.2124... Val Loss: 3.2230\n",
      "Epoch: 4/20... Step: 3490... Loss: 3.2164... Val Loss: 3.2203\n",
      "Epoch: 4/20... Step: 3500... Loss: 3.2254... Val Loss: 3.1985\n",
      "Epoch: 4/20... Step: 3510... Loss: 3.2156... Val Loss: 3.2047\n",
      "Epoch: 4/20... Step: 3520... Loss: 3.2060... Val Loss: 3.2003\n",
      "Epoch: 4/20... Step: 3530... Loss: 3.1963... Val Loss: 3.1928\n",
      "Epoch: 4/20... Step: 3540... Loss: 3.1870... Val Loss: 3.1962\n",
      "Epoch: 4/20... Step: 3550... Loss: 3.2261... Val Loss: 3.1891\n",
      "Epoch: 4/20... Step: 3560... Loss: 3.1948... Val Loss: 3.1995\n",
      "Epoch: 4/20... Step: 3570... Loss: 3.1917... Val Loss: 3.2000\n",
      "Epoch: 4/20... Step: 3580... Loss: 3.2059... Val Loss: 3.1980\n",
      "Epoch: 4/20... Step: 3590... Loss: 3.2053... Val Loss: 3.2259\n",
      "Epoch: 4/20... Step: 3600... Loss: 3.2256... Val Loss: 3.2057\n",
      "Epoch: 4/20... Step: 3610... Loss: 3.2310... Val Loss: 3.1935\n",
      "Epoch: 4/20... Step: 3620... Loss: 3.1885... Val Loss: 3.2085\n",
      "Epoch: 4/20... Step: 3630... Loss: 3.2325... Val Loss: 3.2159\n",
      "Epoch: 4/20... Step: 3640... Loss: 3.2047... Val Loss: 3.2301\n",
      "Epoch: 4/20... Step: 3650... Loss: 3.1930... Val Loss: 3.2033\n",
      "Epoch: 4/20... Step: 3660... Loss: 3.1905... Val Loss: 3.2316\n",
      "Epoch: 4/20... Step: 3670... Loss: 3.2027... Val Loss: 3.2139\n",
      "Epoch: 4/20... Step: 3680... Loss: 3.1957... Val Loss: 3.2086\n",
      "Epoch: 4/20... Step: 3690... Loss: 3.2303... Val Loss: 3.1676\n",
      "Epoch: 4/20... Step: 3700... Loss: 3.2107... Val Loss: 3.2028\n",
      "Epoch: 4/20... Step: 3710... Loss: 3.2135... Val Loss: 3.1892\n",
      "Epoch: 4/20... Step: 3720... Loss: 3.1922... Val Loss: 3.1902\n",
      "Epoch: 4/20... Step: 3730... Loss: 3.1993... Val Loss: 3.2274\n",
      "Epoch: 4/20... Step: 3740... Loss: 3.2137... Val Loss: 3.2197\n",
      "Epoch: 4/20... Step: 3750... Loss: 3.2125... Val Loss: 3.1913\n",
      "Epoch: 4/20... Step: 3760... Loss: 3.1977... Val Loss: 3.2016\n",
      "Epoch: 4/20... Step: 3770... Loss: 3.2005... Val Loss: 3.2035\n",
      "Epoch: 4/20... Step: 3780... Loss: 3.1905... Val Loss: 3.2183\n",
      "Epoch: 4/20... Step: 3790... Loss: 3.2145... Val Loss: 3.2075\n",
      "Epoch: 4/20... Step: 3800... Loss: 3.2190... Val Loss: 3.2027\n",
      "Epoch: 4/20... Step: 3810... Loss: 3.2127... Val Loss: 3.1973\n",
      "Epoch: 4/20... Step: 3820... Loss: 3.2221... Val Loss: 3.2093\n",
      "Epoch: 4/20... Step: 3830... Loss: 3.1895... Val Loss: 3.2066\n",
      "Epoch: 4/20... Step: 3840... Loss: 3.1932... Val Loss: 3.1856\n",
      "Epoch: 4/20... Step: 3850... Loss: 3.1866... Val Loss: 3.1910\n",
      "Epoch: 4/20... Step: 3860... Loss: 3.2081... Val Loss: 3.2029\n",
      "Epoch: 4/20... Step: 3870... Loss: 3.2002... Val Loss: 3.1939\n",
      "Epoch: 4/20... Step: 3880... Loss: 3.1896... Val Loss: 3.2289\n",
      "Epoch: 4/20... Step: 3890... Loss: 3.2109... Val Loss: 3.2246\n",
      "Epoch: 4/20... Step: 3900... Loss: 3.2069... Val Loss: 3.2010\n",
      "Epoch: 4/20... Step: 3910... Loss: 3.2042... Val Loss: 3.2037\n",
      "Epoch: 4/20... Step: 3920... Loss: 3.1875... Val Loss: 3.2009\n",
      "Epoch: 4/20... Step: 3930... Loss: 3.1934... Val Loss: 3.1957\n",
      "Epoch: 4/20... Step: 3940... Loss: 3.2146... Val Loss: 3.2047\n",
      "Epoch: 4/20... Step: 3950... Loss: 3.2041... Val Loss: 3.2001\n",
      "Epoch: 4/20... Step: 3960... Loss: 3.1900... Val Loss: 3.2136\n",
      "Epoch: 4/20... Step: 3970... Loss: 3.2120... Val Loss: 3.1883\n",
      "Epoch: 4/20... Step: 3980... Loss: 3.1875... Val Loss: 3.1952\n",
      "Epoch: 4/20... Step: 3990... Loss: 3.2335... Val Loss: 3.2112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20... Step: 4000... Loss: 3.1836... Val Loss: 3.2109\n",
      "Epoch: 5/20... Step: 4010... Loss: 3.2098... Val Loss: 3.2103\n",
      "Epoch: 5/20... Step: 4020... Loss: 3.1965... Val Loss: 3.1995\n",
      "Epoch: 5/20... Step: 4030... Loss: 3.2273... Val Loss: 3.1964\n",
      "Epoch: 5/20... Step: 4040... Loss: 3.1976... Val Loss: 3.2015\n",
      "Epoch: 5/20... Step: 4050... Loss: 3.2078... Val Loss: 3.2054\n",
      "Epoch: 5/20... Step: 4060... Loss: 3.1705... Val Loss: 3.2354\n",
      "Epoch: 5/20... Step: 4070... Loss: 3.1958... Val Loss: 3.2459\n",
      "Epoch: 5/20... Step: 4080... Loss: 3.2040... Val Loss: 3.2043\n",
      "Epoch: 5/20... Step: 4090... Loss: 3.2063... Val Loss: 3.1964\n",
      "Epoch: 5/20... Step: 4100... Loss: 3.2044... Val Loss: 3.1956\n",
      "Epoch: 5/20... Step: 4110... Loss: 3.2022... Val Loss: 3.2032\n",
      "Epoch: 5/20... Step: 4120... Loss: 3.2135... Val Loss: 3.2043\n",
      "Epoch: 5/20... Step: 4130... Loss: 3.1934... Val Loss: 3.2277\n",
      "Epoch: 5/20... Step: 4140... Loss: 3.2090... Val Loss: 3.2017\n",
      "Epoch: 5/20... Step: 4150... Loss: 3.2140... Val Loss: 3.2118\n",
      "Epoch: 5/20... Step: 4160... Loss: 3.2046... Val Loss: 3.2302\n",
      "Epoch: 5/20... Step: 4170... Loss: 3.1978... Val Loss: 3.2114\n",
      "Epoch: 5/20... Step: 4180... Loss: 3.2080... Val Loss: 3.2210\n",
      "Epoch: 5/20... Step: 4190... Loss: 3.2180... Val Loss: 3.2010\n",
      "Epoch: 5/20... Step: 4200... Loss: 3.2090... Val Loss: 3.2072\n",
      "Epoch: 5/20... Step: 4210... Loss: 3.2176... Val Loss: 3.2140\n",
      "Epoch: 5/20... Step: 4220... Loss: 3.1885... Val Loss: 3.2048\n",
      "Epoch: 5/20... Step: 4230... Loss: 3.1986... Val Loss: 3.2151\n",
      "Epoch: 5/20... Step: 4240... Loss: 3.1887... Val Loss: 3.1941\n",
      "Epoch: 5/20... Step: 4250... Loss: 3.1962... Val Loss: 3.1980\n",
      "Epoch: 5/20... Step: 4260... Loss: 3.2104... Val Loss: 3.1842\n",
      "Epoch: 5/20... Step: 4270... Loss: 3.1926... Val Loss: 3.1671\n",
      "Epoch: 5/20... Step: 4280... Loss: 3.2084... Val Loss: 3.2065\n",
      "Epoch: 5/20... Step: 4290... Loss: 3.2114... Val Loss: 3.2121\n",
      "Epoch: 5/20... Step: 4300... Loss: 3.1939... Val Loss: 3.2041\n",
      "Epoch: 5/20... Step: 4310... Loss: 3.2118... Val Loss: 3.1999\n",
      "Epoch: 5/20... Step: 4320... Loss: 3.2168... Val Loss: 3.2128\n",
      "Epoch: 5/20... Step: 4330... Loss: 3.1851... Val Loss: 3.2226\n",
      "Epoch: 5/20... Step: 4340... Loss: 3.2023... Val Loss: 3.1840\n",
      "Epoch: 5/20... Step: 4350... Loss: 3.2129... Val Loss: 3.2120\n",
      "Epoch: 5/20... Step: 4360... Loss: 3.2103... Val Loss: 3.2053\n",
      "Epoch: 5/20... Step: 4370... Loss: 3.2142... Val Loss: 3.2023\n",
      "Epoch: 5/20... Step: 4380... Loss: 3.1894... Val Loss: 3.2216\n",
      "Epoch: 5/20... Step: 4390... Loss: 3.1939... Val Loss: 3.2225\n",
      "Epoch: 5/20... Step: 4400... Loss: 3.1744... Val Loss: 3.2081\n",
      "Epoch: 5/20... Step: 4410... Loss: 3.2296... Val Loss: 3.1851\n",
      "Epoch: 5/20... Step: 4420... Loss: 3.2214... Val Loss: 3.2049\n",
      "Epoch: 5/20... Step: 4430... Loss: 3.1945... Val Loss: 3.2117\n",
      "Epoch: 5/20... Step: 4440... Loss: 3.2209... Val Loss: 3.2370\n",
      "Epoch: 5/20... Step: 4450... Loss: 3.2120... Val Loss: 3.2185\n",
      "Epoch: 5/20... Step: 4460... Loss: 3.2223... Val Loss: 3.2003\n",
      "Epoch: 5/20... Step: 4470... Loss: 3.2149... Val Loss: 3.2179\n",
      "Epoch: 5/20... Step: 4480... Loss: 3.2113... Val Loss: 3.2206\n",
      "Epoch: 5/20... Step: 4490... Loss: 3.2265... Val Loss: 3.1901\n",
      "Epoch: 5/20... Step: 4500... Loss: 3.2207... Val Loss: 3.1676\n",
      "Epoch: 5/20... Step: 4510... Loss: 3.1991... Val Loss: 3.2262\n",
      "Epoch: 5/20... Step: 4520... Loss: 3.2020... Val Loss: 3.1971\n",
      "Epoch: 5/20... Step: 4530... Loss: 3.2041... Val Loss: 3.2017\n",
      "Epoch: 5/20... Step: 4540... Loss: 3.1925... Val Loss: 3.2020\n",
      "Epoch: 5/20... Step: 4550... Loss: 3.1961... Val Loss: 3.1933\n",
      "Epoch: 5/20... Step: 4560... Loss: 3.2196... Val Loss: 3.2102\n",
      "Epoch: 5/20... Step: 4570... Loss: 3.2114... Val Loss: 3.1921\n",
      "Epoch: 5/20... Step: 4580... Loss: 3.2084... Val Loss: 3.1851\n",
      "Epoch: 5/20... Step: 4590... Loss: 3.2008... Val Loss: 3.1960\n",
      "Epoch: 5/20... Step: 4600... Loss: 3.1999... Val Loss: 3.1986\n",
      "Epoch: 5/20... Step: 4610... Loss: 3.2036... Val Loss: 3.2065\n",
      "Epoch: 5/20... Step: 4620... Loss: 3.1971... Val Loss: 3.1815\n",
      "Epoch: 5/20... Step: 4630... Loss: 3.1996... Val Loss: 3.1914\n",
      "Epoch: 5/20... Step: 4640... Loss: 3.1997... Val Loss: 3.2127\n",
      "Epoch: 5/20... Step: 4650... Loss: 3.2115... Val Loss: 3.1854\n",
      "Epoch: 5/20... Step: 4660... Loss: 3.1958... Val Loss: 3.1835\n",
      "Epoch: 5/20... Step: 4670... Loss: 3.1820... Val Loss: 3.2102\n",
      "Epoch: 5/20... Step: 4680... Loss: 3.1904... Val Loss: 3.1949\n",
      "Epoch: 5/20... Step: 4690... Loss: 3.1937... Val Loss: 3.2125\n",
      "Epoch: 5/20... Step: 4700... Loss: 3.1745... Val Loss: 3.2174\n",
      "Epoch: 5/20... Step: 4710... Loss: 3.2168... Val Loss: 3.2101\n",
      "Epoch: 5/20... Step: 4720... Loss: 3.1850... Val Loss: 3.2008\n",
      "Epoch: 5/20... Step: 4730... Loss: 3.1872... Val Loss: 3.2048\n",
      "Epoch: 5/20... Step: 4740... Loss: 3.1940... Val Loss: 3.2118\n",
      "Epoch: 5/20... Step: 4750... Loss: 3.1997... Val Loss: 3.2122\n",
      "Epoch: 5/20... Step: 4760... Loss: 3.1926... Val Loss: 3.1898\n",
      "Epoch: 5/20... Step: 4770... Loss: 3.1967... Val Loss: 3.1790\n",
      "Epoch: 5/20... Step: 4780... Loss: 3.1970... Val Loss: 3.1907\n",
      "Epoch: 5/20... Step: 4790... Loss: 3.1957... Val Loss: 3.2227\n",
      "Epoch: 5/20... Step: 4800... Loss: 3.1889... Val Loss: 3.1551\n",
      "Epoch: 5/20... Step: 4810... Loss: 3.2156... Val Loss: 3.2016\n",
      "Epoch: 5/20... Step: 4820... Loss: 3.2130... Val Loss: 3.2036\n",
      "Epoch: 5/20... Step: 4830... Loss: 3.1723... Val Loss: 3.2148\n",
      "Epoch: 5/20... Step: 4840... Loss: 3.2045... Val Loss: 3.1848\n",
      "Epoch: 5/20... Step: 4850... Loss: 3.2170... Val Loss: 3.2028\n",
      "Epoch: 5/20... Step: 4860... Loss: 3.2027... Val Loss: 3.2041\n",
      "Epoch: 5/20... Step: 4870... Loss: 3.1926... Val Loss: 3.1962\n",
      "Epoch: 5/20... Step: 4880... Loss: 3.2214... Val Loss: 3.1862\n",
      "Epoch: 5/20... Step: 4890... Loss: 3.1922... Val Loss: 3.1922\n",
      "Epoch: 5/20... Step: 4900... Loss: 3.1948... Val Loss: 3.2057\n",
      "Epoch: 5/20... Step: 4910... Loss: 3.2067... Val Loss: 3.1984\n",
      "Epoch: 5/20... Step: 4920... Loss: 3.2243... Val Loss: 3.2318\n",
      "Epoch: 5/20... Step: 4930... Loss: 3.1980... Val Loss: 3.2138\n",
      "Epoch: 5/20... Step: 4940... Loss: 3.2168... Val Loss: 3.2056\n",
      "Epoch: 5/20... Step: 4950... Loss: 3.1852... Val Loss: 3.2058\n",
      "Epoch: 5/20... Step: 4960... Loss: 3.2030... Val Loss: 3.2094\n",
      "Epoch: 5/20... Step: 4970... Loss: 3.2194... Val Loss: 3.2059\n",
      "Epoch: 5/20... Step: 4980... Loss: 3.2042... Val Loss: 3.2141\n",
      "Epoch: 5/20... Step: 4990... Loss: 3.1926... Val Loss: 3.2006\n",
      "Epoch: 6/20... Step: 5000... Loss: 3.2245... Val Loss: 3.1959\n",
      "Epoch: 6/20... Step: 5010... Loss: 3.1851... Val Loss: 3.2096\n",
      "Epoch: 6/20... Step: 5020... Loss: 3.1980... Val Loss: 3.1843\n",
      "Epoch: 6/20... Step: 5030... Loss: 3.2088... Val Loss: 3.2126\n",
      "Epoch: 6/20... Step: 5040... Loss: 3.2010... Val Loss: 3.1950\n",
      "Epoch: 6/20... Step: 5050... Loss: 3.2371... Val Loss: 3.1967\n",
      "Epoch: 6/20... Step: 5060... Loss: 3.2206... Val Loss: 3.2054\n",
      "Epoch: 6/20... Step: 5070... Loss: 3.2148... Val Loss: 3.2180\n",
      "Epoch: 6/20... Step: 5080... Loss: 3.1814... Val Loss: 3.2158\n",
      "Epoch: 6/20... Step: 5090... Loss: 3.2026... Val Loss: 3.1933\n",
      "Epoch: 6/20... Step: 5100... Loss: 3.2013... Val Loss: 3.2006\n",
      "Epoch: 6/20... Step: 5110... Loss: 3.1775... Val Loss: 3.2107\n",
      "Epoch: 6/20... Step: 5120... Loss: 3.2284... Val Loss: 3.2025\n",
      "Epoch: 6/20... Step: 5130... Loss: 3.1821... Val Loss: 3.2291\n",
      "Epoch: 6/20... Step: 5140... Loss: 3.1844... Val Loss: 3.1905\n",
      "Epoch: 6/20... Step: 5150... Loss: 3.2029... Val Loss: 3.1910\n",
      "Epoch: 6/20... Step: 5160... Loss: 3.2124... Val Loss: 3.2099\n",
      "Epoch: 6/20... Step: 5170... Loss: 3.1938... Val Loss: 3.2100\n",
      "Epoch: 6/20... Step: 5180... Loss: 3.2191... Val Loss: 3.1995\n",
      "Epoch: 6/20... Step: 5190... Loss: 3.2188... Val Loss: 3.2074\n",
      "Epoch: 6/20... Step: 5200... Loss: 3.2008... Val Loss: 3.2002\n",
      "Epoch: 6/20... Step: 5210... Loss: 3.1743... Val Loss: 3.2283\n",
      "Epoch: 6/20... Step: 5220... Loss: 3.1973... Val Loss: 3.2165\n",
      "Epoch: 6/20... Step: 5230... Loss: 3.1709... Val Loss: 3.2003\n",
      "Epoch: 6/20... Step: 5240... Loss: 3.1881... Val Loss: 3.2112\n",
      "Epoch: 6/20... Step: 5250... Loss: 3.1977... Val Loss: 3.2074\n",
      "Epoch: 6/20... Step: 5260... Loss: 3.2252... Val Loss: 3.1817\n",
      "Epoch: 6/20... Step: 5270... Loss: 3.2035... Val Loss: 3.1977\n",
      "Epoch: 6/20... Step: 5280... Loss: 3.2027... Val Loss: 3.2124\n",
      "Epoch: 6/20... Step: 5290... Loss: 3.2029... Val Loss: 3.2001\n",
      "Epoch: 6/20... Step: 5300... Loss: 3.2275... Val Loss: 3.2134\n",
      "Epoch: 6/20... Step: 5310... Loss: 3.1915... Val Loss: 3.2148\n",
      "Epoch: 6/20... Step: 5320... Loss: 3.2177... Val Loss: 3.2152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20... Step: 5330... Loss: 3.2370... Val Loss: 3.2101\n",
      "Epoch: 6/20... Step: 5340... Loss: 3.2144... Val Loss: 3.2188\n",
      "Epoch: 6/20... Step: 5350... Loss: 3.2019... Val Loss: 3.2117\n",
      "Epoch: 6/20... Step: 5360... Loss: 3.2139... Val Loss: 3.1916\n",
      "Epoch: 6/20... Step: 5370... Loss: 3.1907... Val Loss: 3.2166\n",
      "Epoch: 6/20... Step: 5380... Loss: 3.2050... Val Loss: 3.2127\n",
      "Epoch: 6/20... Step: 5390... Loss: 3.2102... Val Loss: 3.1816\n",
      "Epoch: 6/20... Step: 5400... Loss: 3.1951... Val Loss: 3.2009\n",
      "Epoch: 6/20... Step: 5410... Loss: 3.2053... Val Loss: 3.1871\n",
      "Epoch: 6/20... Step: 5420... Loss: 3.1924... Val Loss: 3.2070\n",
      "Epoch: 6/20... Step: 5430... Loss: 3.2264... Val Loss: 3.2322\n",
      "Epoch: 6/20... Step: 5440... Loss: 3.2148... Val Loss: 3.1944\n",
      "Epoch: 6/20... Step: 5450... Loss: 3.2125... Val Loss: 3.2011\n",
      "Epoch: 6/20... Step: 5460... Loss: 3.2055... Val Loss: 3.1988\n",
      "Epoch: 6/20... Step: 5470... Loss: 3.1857... Val Loss: 3.1909\n",
      "Epoch: 6/20... Step: 5480... Loss: 3.2221... Val Loss: 3.2010\n",
      "Epoch: 6/20... Step: 5490... Loss: 3.2133... Val Loss: 3.1708\n",
      "Epoch: 6/20... Step: 5500... Loss: 3.2342... Val Loss: 3.2101\n",
      "Epoch: 6/20... Step: 5510... Loss: 3.1694... Val Loss: 3.2109\n",
      "Epoch: 6/20... Step: 5520... Loss: 3.1794... Val Loss: 3.2075\n",
      "Epoch: 6/20... Step: 5530... Loss: 3.2035... Val Loss: 3.1836\n",
      "Epoch: 6/20... Step: 5540... Loss: 3.2061... Val Loss: 3.2217\n",
      "Epoch: 6/20... Step: 5550... Loss: 3.2031... Val Loss: 3.2126\n",
      "Epoch: 6/20... Step: 5560... Loss: 3.2100... Val Loss: 3.2268\n",
      "Epoch: 6/20... Step: 5570... Loss: 3.2223... Val Loss: 3.2081\n",
      "Epoch: 6/20... Step: 5580... Loss: 3.2033... Val Loss: 3.2247\n",
      "Epoch: 6/20... Step: 5590... Loss: 3.2040... Val Loss: 3.2113\n",
      "Epoch: 6/20... Step: 5600... Loss: 3.2223... Val Loss: 3.2132\n",
      "Epoch: 6/20... Step: 5610... Loss: 3.1978... Val Loss: 3.1936\n",
      "Epoch: 6/20... Step: 5620... Loss: 3.2227... Val Loss: 3.2049\n",
      "Epoch: 6/20... Step: 5630... Loss: 3.2000... Val Loss: 3.2147\n",
      "Epoch: 6/20... Step: 5640... Loss: 3.2038... Val Loss: 3.2356\n",
      "Epoch: 6/20... Step: 5650... Loss: 3.2014... Val Loss: 3.1944\n",
      "Epoch: 6/20... Step: 5660... Loss: 3.1964... Val Loss: 3.1989\n",
      "Epoch: 6/20... Step: 5670... Loss: 3.1894... Val Loss: 3.2080\n",
      "Epoch: 6/20... Step: 5680... Loss: 3.1999... Val Loss: 3.2075\n",
      "Epoch: 6/20... Step: 5690... Loss: 3.2126... Val Loss: 3.2203\n",
      "Epoch: 6/20... Step: 5700... Loss: 3.2179... Val Loss: 3.2105\n",
      "Epoch: 6/20... Step: 5710... Loss: 3.2020... Val Loss: 3.2010\n",
      "Epoch: 6/20... Step: 5720... Loss: 3.2176... Val Loss: 3.2172\n",
      "Epoch: 6/20... Step: 5730... Loss: 3.2004... Val Loss: 3.1962\n",
      "Epoch: 6/20... Step: 5740... Loss: 3.2000... Val Loss: 3.1875\n",
      "Epoch: 6/20... Step: 5750... Loss: 3.2030... Val Loss: 3.2087\n",
      "Epoch: 6/20... Step: 5760... Loss: 3.2170... Val Loss: 3.1849\n",
      "Epoch: 6/20... Step: 5770... Loss: 3.1917... Val Loss: 3.2135\n",
      "Epoch: 6/20... Step: 5780... Loss: 3.2087... Val Loss: 3.1775\n",
      "Epoch: 6/20... Step: 5790... Loss: 3.2083... Val Loss: 3.2044\n",
      "Epoch: 6/20... Step: 5800... Loss: 3.1983... Val Loss: 3.2093\n",
      "Epoch: 6/20... Step: 5810... Loss: 3.2012... Val Loss: 3.2103\n",
      "Epoch: 6/20... Step: 5820... Loss: 3.1989... Val Loss: 3.2291\n",
      "Epoch: 6/20... Step: 5830... Loss: 3.1957... Val Loss: 3.2189\n",
      "Epoch: 6/20... Step: 5840... Loss: 3.2071... Val Loss: 3.2056\n",
      "Epoch: 6/20... Step: 5850... Loss: 3.2047... Val Loss: 3.2307\n",
      "Epoch: 6/20... Step: 5860... Loss: 3.2073... Val Loss: 3.1856\n",
      "Epoch: 6/20... Step: 5870... Loss: 3.1938... Val Loss: 3.1839\n",
      "Epoch: 6/20... Step: 5880... Loss: 3.1739... Val Loss: 3.2106\n",
      "Epoch: 6/20... Step: 5890... Loss: 3.2385... Val Loss: 3.1965\n",
      "Epoch: 6/20... Step: 5900... Loss: 3.2445... Val Loss: 3.2222\n",
      "Epoch: 6/20... Step: 5910... Loss: 3.2081... Val Loss: 3.2137\n",
      "Epoch: 6/20... Step: 5920... Loss: 3.2156... Val Loss: 3.2235\n",
      "Epoch: 6/20... Step: 5930... Loss: 3.2459... Val Loss: 3.2111\n",
      "Epoch: 6/20... Step: 5940... Loss: 3.1977... Val Loss: 3.2111\n",
      "Epoch: 6/20... Step: 5950... Loss: 3.2045... Val Loss: 3.2082\n",
      "Epoch: 6/20... Step: 5960... Loss: 3.2068... Val Loss: 3.2158\n",
      "Epoch: 6/20... Step: 5970... Loss: 3.1825... Val Loss: 3.1984\n",
      "Epoch: 6/20... Step: 5980... Loss: 3.1772... Val Loss: 3.2249\n",
      "Epoch: 6/20... Step: 5990... Loss: 3.1887... Val Loss: 3.1927\n",
      "Epoch: 7/20... Step: 6000... Loss: 3.1805... Val Loss: 3.2038\n",
      "Epoch: 7/20... Step: 6010... Loss: 3.1887... Val Loss: 3.1945\n",
      "Epoch: 7/20... Step: 6020... Loss: 3.1905... Val Loss: 3.2155\n",
      "Epoch: 7/20... Step: 6030... Loss: 3.2084... Val Loss: 3.2256\n",
      "Epoch: 7/20... Step: 6040... Loss: 3.2116... Val Loss: 3.2284\n",
      "Epoch: 7/20... Step: 6050... Loss: 3.2034... Val Loss: 3.2158\n",
      "Epoch: 7/20... Step: 6060... Loss: 3.2245... Val Loss: 3.2110\n",
      "Epoch: 7/20... Step: 6070... Loss: 3.2026... Val Loss: 3.2227\n",
      "Epoch: 7/20... Step: 6080... Loss: 3.1870... Val Loss: 3.1962\n",
      "Epoch: 7/20... Step: 6090... Loss: 3.1958... Val Loss: 3.1719\n",
      "Epoch: 7/20... Step: 6100... Loss: 3.2216... Val Loss: 3.1957\n",
      "Epoch: 7/20... Step: 6110... Loss: 3.1902... Val Loss: 3.2174\n",
      "Epoch: 7/20... Step: 6120... Loss: 3.2134... Val Loss: 3.2028\n",
      "Epoch: 7/20... Step: 6130... Loss: 3.1732... Val Loss: 3.2071\n",
      "Epoch: 7/20... Step: 6140... Loss: 3.2024... Val Loss: 3.2091\n",
      "Epoch: 7/20... Step: 6150... Loss: 3.2102... Val Loss: 3.2073\n",
      "Epoch: 7/20... Step: 6160... Loss: 3.1781... Val Loss: 3.2106\n",
      "Epoch: 7/20... Step: 6170... Loss: 3.1944... Val Loss: 3.2256\n",
      "Epoch: 7/20... Step: 6180... Loss: 3.2010... Val Loss: 3.1985\n",
      "Epoch: 7/20... Step: 6190... Loss: 3.2064... Val Loss: 3.2074\n",
      "Epoch: 7/20... Step: 6200... Loss: 3.2210... Val Loss: 3.1988\n",
      "Epoch: 7/20... Step: 6210... Loss: 3.2130... Val Loss: 3.2178\n",
      "Epoch: 7/20... Step: 6220... Loss: 3.2185... Val Loss: 3.2157\n",
      "Epoch: 7/20... Step: 6230... Loss: 3.2238... Val Loss: 3.2048\n",
      "Epoch: 7/20... Step: 6240... Loss: 3.1962... Val Loss: 3.2194\n",
      "Epoch: 7/20... Step: 6250... Loss: 3.1997... Val Loss: 3.2045\n",
      "Epoch: 7/20... Step: 6260... Loss: 3.2134... Val Loss: 3.2035\n",
      "Epoch: 7/20... Step: 6270... Loss: 3.1860... Val Loss: 3.2021\n",
      "Epoch: 7/20... Step: 6280... Loss: 3.1988... Val Loss: 3.2196\n",
      "Epoch: 7/20... Step: 6290... Loss: 3.2151... Val Loss: 3.1960\n",
      "Epoch: 7/20... Step: 6300... Loss: 3.2204... Val Loss: 3.1981\n",
      "Epoch: 7/20... Step: 6310... Loss: 3.2005... Val Loss: 3.2120\n",
      "Epoch: 7/20... Step: 6320... Loss: 3.1838... Val Loss: 3.1976\n",
      "Epoch: 7/20... Step: 6330... Loss: 3.1935... Val Loss: 3.2335\n",
      "Epoch: 7/20... Step: 6340... Loss: 3.2050... Val Loss: 3.1771\n",
      "Epoch: 7/20... Step: 6350... Loss: 3.1976... Val Loss: 3.1947\n",
      "Epoch: 7/20... Step: 6360... Loss: 3.1865... Val Loss: 3.2020\n",
      "Epoch: 7/20... Step: 6370... Loss: 3.2163... Val Loss: 3.2148\n",
      "Epoch: 7/20... Step: 6380... Loss: 3.2023... Val Loss: 3.2018\n",
      "Epoch: 7/20... Step: 6390... Loss: 3.2067... Val Loss: 3.1919\n",
      "Epoch: 7/20... Step: 6400... Loss: 3.1925... Val Loss: 3.1915\n",
      "Epoch: 7/20... Step: 6410... Loss: 3.2115... Val Loss: 3.2067\n",
      "Epoch: 7/20... Step: 6420... Loss: 3.2261... Val Loss: 3.2210\n",
      "Epoch: 7/20... Step: 6430... Loss: 3.2028... Val Loss: 3.2150\n",
      "Epoch: 7/20... Step: 6440... Loss: 3.2118... Val Loss: 3.1995\n",
      "Epoch: 7/20... Step: 6450... Loss: 3.2470... Val Loss: 3.2043\n",
      "Epoch: 7/20... Step: 6460... Loss: 3.2046... Val Loss: 3.2092\n",
      "Epoch: 7/20... Step: 6470... Loss: 3.1961... Val Loss: 3.1935\n",
      "Epoch: 7/20... Step: 6480... Loss: 3.2305... Val Loss: 3.1985\n",
      "Epoch: 7/20... Step: 6490... Loss: 3.2064... Val Loss: 3.2001\n",
      "Epoch: 7/20... Step: 6500... Loss: 3.2092... Val Loss: 3.1979\n",
      "Epoch: 7/20... Step: 6510... Loss: 3.2047... Val Loss: 3.1873\n",
      "Epoch: 7/20... Step: 6520... Loss: 3.2128... Val Loss: 3.2092\n",
      "Epoch: 7/20... Step: 6530... Loss: 3.2025... Val Loss: 3.1933\n",
      "Epoch: 7/20... Step: 6540... Loss: 3.1837... Val Loss: 3.2039\n",
      "Epoch: 7/20... Step: 6550... Loss: 3.2011... Val Loss: 3.1867\n",
      "Epoch: 7/20... Step: 6560... Loss: 3.1957... Val Loss: 3.2264\n",
      "Epoch: 7/20... Step: 6570... Loss: 3.2066... Val Loss: 3.2039\n",
      "Epoch: 7/20... Step: 6580... Loss: 3.2034... Val Loss: 3.1929\n",
      "Epoch: 7/20... Step: 6590... Loss: 3.1891... Val Loss: 3.1935\n",
      "Epoch: 7/20... Step: 6600... Loss: 3.1981... Val Loss: 3.2346\n",
      "Epoch: 7/20... Step: 6610... Loss: 3.1924... Val Loss: 3.2077\n",
      "Epoch: 7/20... Step: 6620... Loss: 3.2096... Val Loss: 3.1975\n",
      "Epoch: 7/20... Step: 6630... Loss: 3.1932... Val Loss: 3.2274\n",
      "Epoch: 7/20... Step: 6640... Loss: 3.2061... Val Loss: 3.1817\n",
      "Epoch: 7/20... Step: 6650... Loss: 3.2230... Val Loss: 3.1935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20... Step: 6660... Loss: 3.2020... Val Loss: 3.2058\n",
      "Epoch: 7/20... Step: 6670... Loss: 3.2198... Val Loss: 3.2128\n",
      "Epoch: 7/20... Step: 6680... Loss: 3.2130... Val Loss: 3.2076\n",
      "Epoch: 7/20... Step: 6690... Loss: 3.2252... Val Loss: 3.2066\n",
      "Epoch: 7/20... Step: 6700... Loss: 3.2235... Val Loss: 3.2151\n",
      "Epoch: 7/20... Step: 6710... Loss: 3.1834... Val Loss: 3.2157\n",
      "Epoch: 7/20... Step: 6720... Loss: 3.2095... Val Loss: 3.1748\n",
      "Epoch: 7/20... Step: 6730... Loss: 3.1842... Val Loss: 3.2020\n",
      "Epoch: 7/20... Step: 6740... Loss: 3.1906... Val Loss: 3.2043\n",
      "Epoch: 7/20... Step: 6750... Loss: 3.2130... Val Loss: 3.1963\n",
      "Epoch: 7/20... Step: 6760... Loss: 3.1886... Val Loss: 3.2039\n",
      "Epoch: 7/20... Step: 6770... Loss: 3.1786... Val Loss: 3.1981\n",
      "Epoch: 7/20... Step: 6780... Loss: 3.2089... Val Loss: 3.2070\n",
      "Epoch: 7/20... Step: 6790... Loss: 3.2410... Val Loss: 3.1951\n",
      "Epoch: 7/20... Step: 6800... Loss: 3.1925... Val Loss: 3.2038\n",
      "Epoch: 7/20... Step: 6810... Loss: 3.2127... Val Loss: 3.1956\n",
      "Epoch: 7/20... Step: 6820... Loss: 3.2285... Val Loss: 3.2248\n",
      "Epoch: 7/20... Step: 6830... Loss: 3.2128... Val Loss: 3.1951\n",
      "Epoch: 7/20... Step: 6840... Loss: 3.1969... Val Loss: 3.1985\n",
      "Epoch: 7/20... Step: 6850... Loss: 3.2000... Val Loss: 3.1937\n",
      "Epoch: 7/20... Step: 6860... Loss: 3.2082... Val Loss: 3.2023\n",
      "Epoch: 7/20... Step: 6870... Loss: 3.2053... Val Loss: 3.2118\n",
      "Epoch: 7/20... Step: 6880... Loss: 3.2171... Val Loss: 3.2026\n",
      "Epoch: 7/20... Step: 6890... Loss: 3.2173... Val Loss: 3.1871\n",
      "Epoch: 7/20... Step: 6900... Loss: 3.1895... Val Loss: 3.1806\n",
      "Epoch: 7/20... Step: 6910... Loss: 3.2214... Val Loss: 3.1943\n",
      "Epoch: 7/20... Step: 6920... Loss: 3.2178... Val Loss: 3.2353\n",
      "Epoch: 7/20... Step: 6930... Loss: 3.1958... Val Loss: 3.1928\n",
      "Epoch: 7/20... Step: 6940... Loss: 3.1941... Val Loss: 3.1879\n",
      "Epoch: 7/20... Step: 6950... Loss: 3.1945... Val Loss: 3.2380\n",
      "Epoch: 7/20... Step: 6960... Loss: 3.2064... Val Loss: 3.2309\n",
      "Epoch: 7/20... Step: 6970... Loss: 3.2066... Val Loss: 3.1843\n",
      "Epoch: 7/20... Step: 6980... Loss: 3.2181... Val Loss: 3.1997\n",
      "Epoch: 7/20... Step: 6990... Loss: 3.2123... Val Loss: 3.1903\n",
      "Epoch: 8/20... Step: 7000... Loss: 3.1972... Val Loss: 3.2109\n",
      "Epoch: 8/20... Step: 7010... Loss: 3.2006... Val Loss: 3.1888\n",
      "Epoch: 8/20... Step: 7020... Loss: 3.2084... Val Loss: 3.2120\n",
      "Epoch: 8/20... Step: 7030... Loss: 3.2171... Val Loss: 3.2058\n",
      "Epoch: 8/20... Step: 7040... Loss: 3.2186... Val Loss: 3.2116\n",
      "Epoch: 8/20... Step: 7050... Loss: 3.2007... Val Loss: 3.2208\n",
      "Epoch: 8/20... Step: 7060... Loss: 3.1952... Val Loss: 3.2037\n",
      "Epoch: 8/20... Step: 7070... Loss: 3.1964... Val Loss: 3.2233\n",
      "Epoch: 8/20... Step: 7080... Loss: 3.1948... Val Loss: 3.1958\n",
      "Epoch: 8/20... Step: 7090... Loss: 3.2017... Val Loss: 3.2001\n",
      "Epoch: 8/20... Step: 7100... Loss: 3.2157... Val Loss: 3.2238\n",
      "Epoch: 8/20... Step: 7110... Loss: 3.2253... Val Loss: 3.2103\n",
      "Epoch: 8/20... Step: 7120... Loss: 3.1828... Val Loss: 3.1970\n",
      "Epoch: 8/20... Step: 7130... Loss: 3.2122... Val Loss: 3.2246\n",
      "Epoch: 8/20... Step: 7140... Loss: 3.2387... Val Loss: 3.2219\n",
      "Epoch: 8/20... Step: 7150... Loss: 3.2093... Val Loss: 3.1920\n",
      "Epoch: 8/20... Step: 7160... Loss: 3.2138... Val Loss: 3.1975\n",
      "Epoch: 8/20... Step: 7170... Loss: 3.2017... Val Loss: 3.1846\n",
      "Epoch: 8/20... Step: 7180... Loss: 3.2008... Val Loss: 3.1850\n",
      "Epoch: 8/20... Step: 7190... Loss: 3.2251... Val Loss: 3.1992\n",
      "Epoch: 8/20... Step: 7200... Loss: 3.2097... Val Loss: 3.2324\n",
      "Epoch: 8/20... Step: 7210... Loss: 3.2214... Val Loss: 3.2001\n",
      "Epoch: 8/20... Step: 7220... Loss: 3.2020... Val Loss: 3.1923\n",
      "Epoch: 8/20... Step: 7230... Loss: 3.2111... Val Loss: 3.2177\n",
      "Epoch: 8/20... Step: 7240... Loss: 3.2328... Val Loss: 3.2157\n",
      "Epoch: 8/20... Step: 7250... Loss: 3.1994... Val Loss: 3.1980\n",
      "Epoch: 8/20... Step: 7260... Loss: 3.2098... Val Loss: 3.1622\n",
      "Epoch: 8/20... Step: 7270... Loss: 3.2128... Val Loss: 3.2001\n",
      "Epoch: 8/20... Step: 7280... Loss: 3.2125... Val Loss: 3.2214\n",
      "Epoch: 8/20... Step: 7290... Loss: 3.2134... Val Loss: 3.1986\n",
      "Epoch: 8/20... Step: 7300... Loss: 3.1992... Val Loss: 3.2098\n",
      "Epoch: 8/20... Step: 7310... Loss: 3.2116... Val Loss: 3.1784\n",
      "Epoch: 8/20... Step: 7320... Loss: 3.2387... Val Loss: 3.2147\n",
      "Epoch: 8/20... Step: 7330... Loss: 3.1818... Val Loss: 3.2030\n",
      "Epoch: 8/20... Step: 7340... Loss: 3.2066... Val Loss: 3.2058\n",
      "Epoch: 8/20... Step: 7350... Loss: 3.1926... Val Loss: 3.1886\n",
      "Epoch: 8/20... Step: 7360... Loss: 3.1836... Val Loss: 3.1945\n",
      "Epoch: 8/20... Step: 7370... Loss: 3.2164... Val Loss: 3.1935\n",
      "Epoch: 8/20... Step: 7380... Loss: 3.1975... Val Loss: 3.2016\n",
      "Epoch: 8/20... Step: 7390... Loss: 3.1791... Val Loss: 3.2114\n",
      "Epoch: 8/20... Step: 7400... Loss: 3.1936... Val Loss: 3.2199\n",
      "Epoch: 8/20... Step: 7410... Loss: 3.1933... Val Loss: 3.1887\n",
      "Epoch: 8/20... Step: 7420... Loss: 3.2055... Val Loss: 3.2113\n",
      "Epoch: 8/20... Step: 7430... Loss: 3.2173... Val Loss: 3.2249\n",
      "Epoch: 8/20... Step: 7440... Loss: 3.1939... Val Loss: 3.2015\n",
      "Epoch: 8/20... Step: 7450... Loss: 3.1886... Val Loss: 3.2107\n",
      "Epoch: 8/20... Step: 7460... Loss: 3.2064... Val Loss: 3.2282\n",
      "Epoch: 8/20... Step: 7470... Loss: 3.2136... Val Loss: 3.2125\n",
      "Epoch: 8/20... Step: 7480... Loss: 3.2221... Val Loss: 3.2111\n",
      "Epoch: 8/20... Step: 7490... Loss: 3.1894... Val Loss: 3.2282\n",
      "Epoch: 8/20... Step: 7500... Loss: 3.1821... Val Loss: 3.2098\n",
      "Epoch: 8/20... Step: 7510... Loss: 3.2414... Val Loss: 3.2205\n",
      "Epoch: 8/20... Step: 7520... Loss: 3.1921... Val Loss: 3.1924\n",
      "Epoch: 8/20... Step: 7530... Loss: 3.1822... Val Loss: 3.2004\n",
      "Epoch: 8/20... Step: 7540... Loss: 3.1828... Val Loss: 3.2113\n",
      "Epoch: 8/20... Step: 7550... Loss: 3.1871... Val Loss: 3.2189\n",
      "Epoch: 8/20... Step: 7560... Loss: 3.2291... Val Loss: 3.1881\n",
      "Epoch: 8/20... Step: 7570... Loss: 3.1968... Val Loss: 3.2015\n",
      "Epoch: 8/20... Step: 7580... Loss: 3.2028... Val Loss: 3.2059\n",
      "Epoch: 8/20... Step: 7590... Loss: 3.1924... Val Loss: 3.2060\n",
      "Epoch: 8/20... Step: 7600... Loss: 3.1859... Val Loss: 3.1707\n",
      "Epoch: 8/20... Step: 7610... Loss: 3.1924... Val Loss: 3.2213\n",
      "Epoch: 8/20... Step: 7620... Loss: 3.2073... Val Loss: 3.1806\n",
      "Epoch: 8/20... Step: 7630... Loss: 3.2048... Val Loss: 3.2098\n",
      "Epoch: 8/20... Step: 7640... Loss: 3.2215... Val Loss: 3.1858\n",
      "Epoch: 8/20... Step: 7650... Loss: 3.2087... Val Loss: 3.1889\n",
      "Epoch: 8/20... Step: 7660... Loss: 3.2248... Val Loss: 3.1993\n",
      "Epoch: 8/20... Step: 7670... Loss: 3.2332... Val Loss: 3.1923\n",
      "Epoch: 8/20... Step: 7680... Loss: 3.2072... Val Loss: 3.1893\n",
      "Epoch: 8/20... Step: 7690... Loss: 3.1959... Val Loss: 3.2053\n",
      "Epoch: 8/20... Step: 7700... Loss: 3.2012... Val Loss: 3.2014\n",
      "Epoch: 8/20... Step: 7710... Loss: 3.1841... Val Loss: 3.2056\n",
      "Epoch: 8/20... Step: 7720... Loss: 3.2125... Val Loss: 3.2211\n",
      "Epoch: 8/20... Step: 7730... Loss: 3.2693... Val Loss: 3.2177\n",
      "Epoch: 8/20... Step: 7740... Loss: 3.2007... Val Loss: 3.2006\n",
      "Epoch: 8/20... Step: 7750... Loss: 3.2249... Val Loss: 3.1848\n",
      "Epoch: 8/20... Step: 7760... Loss: 3.2162... Val Loss: 3.1847\n",
      "Epoch: 8/20... Step: 7770... Loss: 3.2129... Val Loss: 3.1850\n",
      "Epoch: 8/20... Step: 7780... Loss: 3.1914... Val Loss: 3.2101\n",
      "Epoch: 8/20... Step: 7790... Loss: 3.2038... Val Loss: 3.2152\n",
      "Epoch: 8/20... Step: 7800... Loss: 3.2183... Val Loss: 3.2317\n",
      "Epoch: 8/20... Step: 7810... Loss: 3.2197... Val Loss: 3.1982\n",
      "Epoch: 8/20... Step: 7820... Loss: 3.2020... Val Loss: 3.2149\n",
      "Epoch: 8/20... Step: 7830... Loss: 3.2062... Val Loss: 3.1923\n",
      "Epoch: 8/20... Step: 7840... Loss: 3.1794... Val Loss: 3.2057\n",
      "Epoch: 8/20... Step: 7850... Loss: 3.2179... Val Loss: 3.1969\n",
      "Epoch: 8/20... Step: 7860... Loss: 3.1931... Val Loss: 3.2115\n",
      "Epoch: 8/20... Step: 7870... Loss: 3.2022... Val Loss: 3.2014\n",
      "Epoch: 8/20... Step: 7880... Loss: 3.2265... Val Loss: 3.2026\n",
      "Epoch: 8/20... Step: 7890... Loss: 3.1992... Val Loss: 3.2031\n",
      "Epoch: 8/20... Step: 7900... Loss: 3.2131... Val Loss: 3.2099\n",
      "Epoch: 8/20... Step: 7910... Loss: 3.1876... Val Loss: 3.1867\n",
      "Epoch: 8/20... Step: 7920... Loss: 3.1979... Val Loss: 3.2178\n",
      "Epoch: 8/20... Step: 7930... Loss: 3.2061... Val Loss: 3.2198\n",
      "Epoch: 8/20... Step: 7940... Loss: 3.1995... Val Loss: 3.2160\n",
      "Epoch: 8/20... Step: 7950... Loss: 3.1912... Val Loss: 3.2065\n",
      "Epoch: 8/20... Step: 7960... Loss: 3.2124... Val Loss: 3.2053\n",
      "Epoch: 8/20... Step: 7970... Loss: 3.1966... Val Loss: 3.1932\n",
      "Epoch: 8/20... Step: 7980... Loss: 3.2254... Val Loss: 3.1814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20... Step: 7990... Loss: 3.2123... Val Loss: 3.2104\n",
      "Epoch: 9/20... Step: 8000... Loss: 3.2106... Val Loss: 3.2043\n",
      "Epoch: 9/20... Step: 8010... Loss: 3.1905... Val Loss: 3.2252\n",
      "Epoch: 9/20... Step: 8020... Loss: 3.2077... Val Loss: 3.1911\n",
      "Epoch: 9/20... Step: 8030... Loss: 3.1883... Val Loss: 3.1775\n",
      "Epoch: 9/20... Step: 8040... Loss: 3.2236... Val Loss: 3.2286\n",
      "Epoch: 9/20... Step: 8050... Loss: 3.2130... Val Loss: 3.2156\n",
      "Epoch: 9/20... Step: 8060... Loss: 3.1889... Val Loss: 3.2032\n",
      "Epoch: 9/20... Step: 8070... Loss: 3.1824... Val Loss: 3.1814\n",
      "Epoch: 9/20... Step: 8080... Loss: 3.2225... Val Loss: 3.2182\n",
      "Epoch: 9/20... Step: 8090... Loss: 3.2049... Val Loss: 3.2046\n",
      "Epoch: 9/20... Step: 8100... Loss: 3.2100... Val Loss: 3.1818\n",
      "Epoch: 9/20... Step: 8110... Loss: 3.1934... Val Loss: 3.2125\n",
      "Epoch: 9/20... Step: 8120... Loss: 3.2427... Val Loss: 3.2198\n",
      "Epoch: 9/20... Step: 8130... Loss: 3.1946... Val Loss: 3.2149\n",
      "Epoch: 9/20... Step: 8140... Loss: 3.1823... Val Loss: 3.2055\n",
      "Epoch: 9/20... Step: 8150... Loss: 3.2094... Val Loss: 3.2016\n",
      "Epoch: 9/20... Step: 8160... Loss: 3.2091... Val Loss: 3.1943\n",
      "Epoch: 9/20... Step: 8170... Loss: 3.2210... Val Loss: 3.2064\n",
      "Epoch: 9/20... Step: 8180... Loss: 3.2094... Val Loss: 3.2009\n",
      "Epoch: 9/20... Step: 8190... Loss: 3.2126... Val Loss: 3.2077\n",
      "Epoch: 9/20... Step: 8200... Loss: 3.1902... Val Loss: 3.2142\n",
      "Epoch: 9/20... Step: 8210... Loss: 3.1970... Val Loss: 3.2025\n",
      "Epoch: 9/20... Step: 8220... Loss: 3.2049... Val Loss: 3.2125\n",
      "Epoch: 9/20... Step: 8230... Loss: 3.2484... Val Loss: 3.2044\n",
      "Epoch: 9/20... Step: 8240... Loss: 3.1494... Val Loss: 3.2048\n",
      "Epoch: 9/20... Step: 8250... Loss: 3.2425... Val Loss: 3.2201\n",
      "Epoch: 9/20... Step: 8260... Loss: 3.1982... Val Loss: 3.2108\n",
      "Epoch: 9/20... Step: 8270... Loss: 3.2099... Val Loss: 3.2006\n",
      "Epoch: 9/20... Step: 8280... Loss: 3.1967... Val Loss: 3.1901\n",
      "Epoch: 9/20... Step: 8290... Loss: 3.1859... Val Loss: 3.2234\n",
      "Epoch: 9/20... Step: 8300... Loss: 3.1824... Val Loss: 3.2084\n",
      "Epoch: 9/20... Step: 8310... Loss: 3.2080... Val Loss: 3.1837\n",
      "Epoch: 9/20... Step: 8320... Loss: 3.1812... Val Loss: 3.2217\n",
      "Epoch: 9/20... Step: 8330... Loss: 3.1963... Val Loss: 3.1882\n",
      "Epoch: 9/20... Step: 8340... Loss: 3.2186... Val Loss: 3.1884\n",
      "Epoch: 9/20... Step: 8350... Loss: 3.2009... Val Loss: 3.2060\n",
      "Epoch: 9/20... Step: 8360... Loss: 3.2163... Val Loss: 3.1870\n",
      "Epoch: 9/20... Step: 8370... Loss: 3.2112... Val Loss: 3.2144\n",
      "Epoch: 9/20... Step: 8380... Loss: 3.2122... Val Loss: 3.1984\n",
      "Epoch: 9/20... Step: 8390... Loss: 3.1858... Val Loss: 3.2249\n",
      "Epoch: 9/20... Step: 8400... Loss: 3.2480... Val Loss: 3.1928\n",
      "Epoch: 9/20... Step: 8410... Loss: 3.2068... Val Loss: 3.2001\n",
      "Epoch: 9/20... Step: 8420... Loss: 3.2254... Val Loss: 3.1801\n",
      "Epoch: 9/20... Step: 8430... Loss: 3.2184... Val Loss: 3.1938\n",
      "Epoch: 9/20... Step: 8440... Loss: 3.1875... Val Loss: 3.2065\n",
      "Epoch: 9/20... Step: 8450... Loss: 3.2026... Val Loss: 3.2079\n",
      "Epoch: 9/20... Step: 8460... Loss: 3.1933... Val Loss: 3.1895\n",
      "Epoch: 9/20... Step: 8470... Loss: 3.1919... Val Loss: 3.2300\n",
      "Epoch: 9/20... Step: 8480... Loss: 3.2031... Val Loss: 3.2121\n",
      "Epoch: 9/20... Step: 8490... Loss: 3.2065... Val Loss: 3.2261\n",
      "Epoch: 9/20... Step: 8500... Loss: 3.1981... Val Loss: 3.2059\n",
      "Epoch: 9/20... Step: 8510... Loss: 3.2121... Val Loss: 3.1917\n",
      "Epoch: 9/20... Step: 8520... Loss: 3.2121... Val Loss: 3.2080\n",
      "Epoch: 9/20... Step: 8530... Loss: 3.1996... Val Loss: 3.2061\n",
      "Epoch: 9/20... Step: 8540... Loss: 3.1964... Val Loss: 3.2104\n",
      "Epoch: 9/20... Step: 8550... Loss: 3.1847... Val Loss: 3.2248\n",
      "Epoch: 9/20... Step: 8560... Loss: 3.2182... Val Loss: 3.2141\n",
      "Epoch: 9/20... Step: 8570... Loss: 3.2140... Val Loss: 3.1996\n",
      "Epoch: 9/20... Step: 8580... Loss: 3.1904... Val Loss: 3.2075\n",
      "Epoch: 9/20... Step: 8590... Loss: 3.2004... Val Loss: 3.1967\n",
      "Epoch: 9/20... Step: 8600... Loss: 3.2265... Val Loss: 3.1804\n",
      "Epoch: 9/20... Step: 8610... Loss: 3.2167... Val Loss: 3.1990\n",
      "Epoch: 9/20... Step: 8620... Loss: 3.1886... Val Loss: 3.2124\n",
      "Epoch: 9/20... Step: 8630... Loss: 3.2008... Val Loss: 3.1956\n",
      "Epoch: 9/20... Step: 8640... Loss: 3.1738... Val Loss: 3.1998\n",
      "Epoch: 9/20... Step: 8650... Loss: 3.2072... Val Loss: 3.1944\n",
      "Epoch: 9/20... Step: 8660... Loss: 3.2078... Val Loss: 3.2047\n",
      "Epoch: 9/20... Step: 8670... Loss: 3.1812... Val Loss: 3.2060\n",
      "Epoch: 9/20... Step: 8680... Loss: 3.2183... Val Loss: 3.1990\n",
      "Epoch: 9/20... Step: 8690... Loss: 3.1870... Val Loss: 3.2007\n",
      "Epoch: 9/20... Step: 8700... Loss: 3.1864... Val Loss: 3.2005\n",
      "Epoch: 9/20... Step: 8710... Loss: 3.1859... Val Loss: 3.2024\n",
      "Epoch: 9/20... Step: 8720... Loss: 3.2196... Val Loss: 3.2051\n",
      "Epoch: 9/20... Step: 8730... Loss: 3.1885... Val Loss: 3.2033\n",
      "Epoch: 9/20... Step: 8740... Loss: 3.1980... Val Loss: 3.1789\n",
      "Epoch: 9/20... Step: 8750... Loss: 3.2007... Val Loss: 3.2522\n",
      "Epoch: 9/20... Step: 8760... Loss: 3.2157... Val Loss: 3.2072\n",
      "Epoch: 9/20... Step: 8770... Loss: 3.1802... Val Loss: 3.2003\n",
      "Epoch: 9/20... Step: 8780... Loss: 3.1951... Val Loss: 3.1990\n",
      "Epoch: 9/20... Step: 8790... Loss: 3.2364... Val Loss: 3.2039\n",
      "Epoch: 9/20... Step: 8800... Loss: 3.1920... Val Loss: 3.2103\n",
      "Epoch: 9/20... Step: 8810... Loss: 3.2220... Val Loss: 3.1720\n",
      "Epoch: 9/20... Step: 8820... Loss: 3.2119... Val Loss: 3.2701\n",
      "Epoch: 9/20... Step: 8830... Loss: 3.1903... Val Loss: 3.2012\n",
      "Epoch: 9/20... Step: 8840... Loss: 3.1810... Val Loss: 3.2061\n",
      "Epoch: 9/20... Step: 8850... Loss: 3.2097... Val Loss: 3.2049\n",
      "Epoch: 9/20... Step: 8860... Loss: 3.2012... Val Loss: 3.2191\n",
      "Epoch: 9/20... Step: 8870... Loss: 3.2227... Val Loss: 3.2072\n",
      "Epoch: 9/20... Step: 8880... Loss: 3.1969... Val Loss: 3.2197\n",
      "Epoch: 9/20... Step: 8890... Loss: 3.2441... Val Loss: 3.2015\n",
      "Epoch: 9/20... Step: 8900... Loss: 3.2173... Val Loss: 3.2121\n",
      "Epoch: 9/20... Step: 8910... Loss: 3.1904... Val Loss: 3.2050\n",
      "Epoch: 9/20... Step: 8920... Loss: 3.1991... Val Loss: 3.1761\n",
      "Epoch: 9/20... Step: 8930... Loss: 3.2142... Val Loss: 3.2167\n",
      "Epoch: 9/20... Step: 8940... Loss: 3.2089... Val Loss: 3.2035\n",
      "Epoch: 9/20... Step: 8950... Loss: 3.2105... Val Loss: 3.2076\n",
      "Epoch: 9/20... Step: 8960... Loss: 3.2104... Val Loss: 3.2098\n",
      "Epoch: 9/20... Step: 8970... Loss: 3.1803... Val Loss: 3.2083\n",
      "Epoch: 9/20... Step: 8980... Loss: 3.2088... Val Loss: 3.2067\n",
      "Epoch: 9/20... Step: 8990... Loss: 3.2027... Val Loss: 3.2121\n",
      "Epoch: 10/20... Step: 9000... Loss: 3.1875... Val Loss: 3.1886\n",
      "Epoch: 10/20... Step: 9010... Loss: 3.2019... Val Loss: 3.2106\n",
      "Epoch: 10/20... Step: 9020... Loss: 3.2015... Val Loss: 3.2080\n",
      "Epoch: 10/20... Step: 9030... Loss: 3.1876... Val Loss: 3.1934\n",
      "Epoch: 10/20... Step: 9040... Loss: 3.2146... Val Loss: 3.2084\n",
      "Epoch: 10/20... Step: 9050... Loss: 3.1995... Val Loss: 3.2093\n",
      "Epoch: 10/20... Step: 9060... Loss: 3.2033... Val Loss: 3.1949\n",
      "Epoch: 10/20... Step: 9070... Loss: 3.1933... Val Loss: 3.2184\n",
      "Epoch: 10/20... Step: 9080... Loss: 3.1857... Val Loss: 3.2094\n",
      "Epoch: 10/20... Step: 9090... Loss: 3.2042... Val Loss: 3.2144\n",
      "Epoch: 10/20... Step: 9100... Loss: 3.2202... Val Loss: 3.1930\n",
      "Epoch: 10/20... Step: 9110... Loss: 3.2165... Val Loss: 3.2127\n",
      "Epoch: 10/20... Step: 9120... Loss: 3.1962... Val Loss: 3.1816\n",
      "Epoch: 10/20... Step: 9130... Loss: 3.1954... Val Loss: 3.2297\n",
      "Epoch: 10/20... Step: 9140... Loss: 3.2211... Val Loss: 3.2067\n",
      "Epoch: 10/20... Step: 9150... Loss: 3.2106... Val Loss: 3.1932\n",
      "Epoch: 10/20... Step: 9160... Loss: 3.1849... Val Loss: 3.2040\n",
      "Epoch: 10/20... Step: 9170... Loss: 3.2084... Val Loss: 3.2160\n",
      "Epoch: 10/20... Step: 9180... Loss: 3.2026... Val Loss: 3.2200\n",
      "Epoch: 10/20... Step: 9190... Loss: 3.1927... Val Loss: 3.2165\n",
      "Epoch: 10/20... Step: 9200... Loss: 3.2081... Val Loss: 3.1915\n",
      "Epoch: 10/20... Step: 9210... Loss: 3.1644... Val Loss: 3.2136\n",
      "Epoch: 10/20... Step: 9220... Loss: 3.2041... Val Loss: 3.1865\n",
      "Epoch: 10/20... Step: 9230... Loss: 3.1847... Val Loss: 3.1813\n",
      "Epoch: 10/20... Step: 9240... Loss: 3.2045... Val Loss: 3.2031\n",
      "Epoch: 10/20... Step: 9250... Loss: 3.2020... Val Loss: 3.1827\n",
      "Epoch: 10/20... Step: 9260... Loss: 3.1962... Val Loss: 3.2035\n",
      "Epoch: 10/20... Step: 9270... Loss: 3.2123... Val Loss: 3.1919\n",
      "Epoch: 10/20... Step: 9280... Loss: 3.1799... Val Loss: 3.1977\n",
      "Epoch: 10/20... Step: 9290... Loss: 3.1871... Val Loss: 3.2023\n",
      "Epoch: 10/20... Step: 9300... Loss: 3.2297... Val Loss: 3.2056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20... Step: 9310... Loss: 3.2251... Val Loss: 3.2102\n",
      "Epoch: 10/20... Step: 9320... Loss: 3.2128... Val Loss: 3.2032\n",
      "Epoch: 10/20... Step: 9330... Loss: 3.2043... Val Loss: 3.2033\n",
      "Epoch: 10/20... Step: 9340... Loss: 3.2089... Val Loss: 3.2135\n",
      "Epoch: 10/20... Step: 9350... Loss: 3.2005... Val Loss: 3.2052\n",
      "Epoch: 10/20... Step: 9360... Loss: 3.1990... Val Loss: 3.2094\n",
      "Epoch: 10/20... Step: 9370... Loss: 3.1973... Val Loss: 3.2021\n",
      "Epoch: 10/20... Step: 9380... Loss: 3.2073... Val Loss: 3.1924\n",
      "Epoch: 10/20... Step: 9390... Loss: 3.2073... Val Loss: 3.1939\n",
      "Epoch: 10/20... Step: 9400... Loss: 3.2026... Val Loss: 3.2164\n",
      "Epoch: 10/20... Step: 9410... Loss: 3.1974... Val Loss: 3.2043\n",
      "Epoch: 10/20... Step: 9420... Loss: 3.2314... Val Loss: 3.2089\n",
      "Epoch: 10/20... Step: 9430... Loss: 3.2003... Val Loss: 3.1634\n",
      "Epoch: 10/20... Step: 9440... Loss: 3.2232... Val Loss: 3.2202\n",
      "Epoch: 10/20... Step: 9450... Loss: 3.2165... Val Loss: 3.1959\n",
      "Epoch: 10/20... Step: 9460... Loss: 3.2281... Val Loss: 3.2108\n",
      "Epoch: 10/20... Step: 9470... Loss: 3.2089... Val Loss: 3.1818\n",
      "Epoch: 10/20... Step: 9480... Loss: 3.1894... Val Loss: 3.1957\n",
      "Epoch: 10/20... Step: 9490... Loss: 3.1846... Val Loss: 3.2288\n",
      "Epoch: 10/20... Step: 9500... Loss: 3.1992... Val Loss: 3.1911\n",
      "Epoch: 10/20... Step: 9510... Loss: 3.1940... Val Loss: 3.2143\n",
      "Epoch: 10/20... Step: 9520... Loss: 3.2168... Val Loss: 3.2049\n",
      "Epoch: 10/20... Step: 9530... Loss: 3.2224... Val Loss: 3.1959\n",
      "Epoch: 10/20... Step: 9540... Loss: 3.1992... Val Loss: 3.1876\n",
      "Epoch: 10/20... Step: 9550... Loss: 3.2190... Val Loss: 3.1933\n",
      "Epoch: 10/20... Step: 9560... Loss: 3.2151... Val Loss: 3.2096\n",
      "Epoch: 10/20... Step: 9570... Loss: 3.1946... Val Loss: 3.2229\n",
      "Epoch: 10/20... Step: 9580... Loss: 3.2105... Val Loss: 3.1919\n",
      "Epoch: 10/20... Step: 9590... Loss: 3.1828... Val Loss: 3.2077\n",
      "Epoch: 10/20... Step: 9600... Loss: 3.2114... Val Loss: 3.2137\n",
      "Epoch: 10/20... Step: 9610... Loss: 3.2333... Val Loss: 3.2126\n",
      "Epoch: 10/20... Step: 9620... Loss: 3.2380... Val Loss: 3.2212\n",
      "Epoch: 10/20... Step: 9630... Loss: 3.2193... Val Loss: 3.1993\n",
      "Epoch: 10/20... Step: 9640... Loss: 3.2059... Val Loss: 3.2043\n",
      "Epoch: 10/20... Step: 9650... Loss: 3.1957... Val Loss: 3.1751\n",
      "Epoch: 10/20... Step: 9660... Loss: 3.2180... Val Loss: 3.2212\n",
      "Epoch: 10/20... Step: 9670... Loss: 3.1913... Val Loss: 3.2145\n",
      "Epoch: 10/20... Step: 9680... Loss: 3.2164... Val Loss: 3.1970\n",
      "Epoch: 10/20... Step: 9690... Loss: 3.1888... Val Loss: 3.2099\n",
      "Epoch: 10/20... Step: 9700... Loss: 3.1829... Val Loss: 3.2056\n",
      "Epoch: 10/20... Step: 9710... Loss: 3.2057... Val Loss: 3.1954\n",
      "Epoch: 10/20... Step: 9720... Loss: 3.1892... Val Loss: 3.1894\n",
      "Epoch: 10/20... Step: 9730... Loss: 3.1995... Val Loss: 3.2152\n",
      "Epoch: 10/20... Step: 9740... Loss: 3.2036... Val Loss: 3.1875\n",
      "Epoch: 10/20... Step: 9750... Loss: 3.2225... Val Loss: 3.1809\n",
      "Epoch: 10/20... Step: 9760... Loss: 3.2261... Val Loss: 3.1852\n",
      "Epoch: 10/20... Step: 9770... Loss: 3.1857... Val Loss: 3.1989\n",
      "Epoch: 10/20... Step: 9780... Loss: 3.2071... Val Loss: 3.2053\n",
      "Epoch: 10/20... Step: 9790... Loss: 3.2012... Val Loss: 3.2160\n",
      "Epoch: 10/20... Step: 9800... Loss: 3.1876... Val Loss: 3.2006\n",
      "Epoch: 10/20... Step: 9810... Loss: 3.2127... Val Loss: 3.1936\n",
      "Epoch: 10/20... Step: 9820... Loss: 3.2145... Val Loss: 3.2020\n",
      "Epoch: 10/20... Step: 9830... Loss: 3.1891... Val Loss: 3.2434\n",
      "Epoch: 10/20... Step: 9840... Loss: 3.1745... Val Loss: 3.1998\n",
      "Epoch: 10/20... Step: 9850... Loss: 3.2185... Val Loss: 3.2085\n",
      "Epoch: 10/20... Step: 9860... Loss: 3.2236... Val Loss: 3.1926\n",
      "Epoch: 10/20... Step: 9870... Loss: 3.2128... Val Loss: 3.2008\n",
      "Epoch: 10/20... Step: 9880... Loss: 3.2013... Val Loss: 3.1961\n",
      "Epoch: 10/20... Step: 9890... Loss: 3.2123... Val Loss: 3.1936\n",
      "Epoch: 10/20... Step: 9900... Loss: 3.2251... Val Loss: 3.1964\n",
      "Epoch: 10/20... Step: 9910... Loss: 3.1941... Val Loss: 3.2064\n",
      "Epoch: 10/20... Step: 9920... Loss: 3.1976... Val Loss: 3.1908\n",
      "Epoch: 10/20... Step: 9930... Loss: 3.1945... Val Loss: 3.2227\n",
      "Epoch: 10/20... Step: 9940... Loss: 3.2154... Val Loss: 3.2095\n",
      "Epoch: 10/20... Step: 9950... Loss: 3.2049... Val Loss: 3.2036\n",
      "Epoch: 10/20... Step: 9960... Loss: 3.2154... Val Loss: 3.1781\n",
      "Epoch: 10/20... Step: 9970... Loss: 3.1935... Val Loss: 3.2018\n",
      "Epoch: 10/20... Step: 9980... Loss: 3.2112... Val Loss: 3.1933\n",
      "Epoch: 10/20... Step: 9990... Loss: 3.1993... Val Loss: 3.2058\n",
      "Epoch: 11/20... Step: 10000... Loss: 3.2113... Val Loss: 3.2124\n",
      "Epoch: 11/20... Step: 10010... Loss: 3.1949... Val Loss: 3.2213\n",
      "Epoch: 11/20... Step: 10020... Loss: 3.2197... Val Loss: 3.2007\n",
      "Epoch: 11/20... Step: 10030... Loss: 3.2165... Val Loss: 3.2157\n",
      "Epoch: 11/20... Step: 10040... Loss: 3.2334... Val Loss: 3.1810\n",
      "Epoch: 11/20... Step: 10050... Loss: 3.2127... Val Loss: 3.1784\n",
      "Epoch: 11/20... Step: 10060... Loss: 3.1871... Val Loss: 3.2413\n",
      "Epoch: 11/20... Step: 10070... Loss: 3.2187... Val Loss: 3.1485\n",
      "Epoch: 11/20... Step: 10080... Loss: 3.2100... Val Loss: 3.1996\n",
      "Epoch: 11/20... Step: 10090... Loss: 3.2077... Val Loss: 3.1850\n",
      "Epoch: 11/20... Step: 10100... Loss: 3.2086... Val Loss: 3.2015\n",
      "Epoch: 11/20... Step: 10110... Loss: 3.1828... Val Loss: 3.1905\n",
      "Epoch: 11/20... Step: 10120... Loss: 3.1862... Val Loss: 3.2170\n",
      "Epoch: 11/20... Step: 10130... Loss: 3.1883... Val Loss: 3.1965\n",
      "Epoch: 11/20... Step: 10140... Loss: 3.2075... Val Loss: 3.2084\n",
      "Epoch: 11/20... Step: 10150... Loss: 3.2147... Val Loss: 3.1971\n",
      "Epoch: 11/20... Step: 10160... Loss: 3.2060... Val Loss: 3.2164\n",
      "Epoch: 11/20... Step: 10170... Loss: 3.1893... Val Loss: 3.2109\n",
      "Epoch: 11/20... Step: 10180... Loss: 3.2032... Val Loss: 3.2012\n",
      "Epoch: 11/20... Step: 10190... Loss: 3.2205... Val Loss: 3.1904\n",
      "Epoch: 11/20... Step: 10200... Loss: 3.2142... Val Loss: 3.2107\n",
      "Epoch: 11/20... Step: 10210... Loss: 3.2006... Val Loss: 3.1855\n",
      "Epoch: 11/20... Step: 10220... Loss: 3.2110... Val Loss: 3.1915\n",
      "Epoch: 11/20... Step: 10230... Loss: 3.2228... Val Loss: 3.2155\n",
      "Epoch: 11/20... Step: 10240... Loss: 3.2110... Val Loss: 3.1995\n",
      "Epoch: 11/20... Step: 10250... Loss: 3.2050... Val Loss: 3.2049\n",
      "Epoch: 11/20... Step: 10260... Loss: 3.2153... Val Loss: 3.2305\n",
      "Epoch: 11/20... Step: 10270... Loss: 3.2029... Val Loss: 3.2112\n",
      "Epoch: 11/20... Step: 10280... Loss: 3.1889... Val Loss: 3.1848\n",
      "Epoch: 11/20... Step: 10290... Loss: 3.2236... Val Loss: 3.2216\n",
      "Epoch: 11/20... Step: 10300... Loss: 3.2145... Val Loss: 3.2300\n",
      "Epoch: 11/20... Step: 10310... Loss: 3.2140... Val Loss: 3.2157\n",
      "Epoch: 11/20... Step: 10320... Loss: 3.1987... Val Loss: 3.2075\n",
      "Epoch: 11/20... Step: 10330... Loss: 3.2140... Val Loss: 3.2195\n",
      "Epoch: 11/20... Step: 10340... Loss: 3.2082... Val Loss: 3.1902\n",
      "Epoch: 11/20... Step: 10350... Loss: 3.2302... Val Loss: 3.2215\n",
      "Epoch: 11/20... Step: 10360... Loss: 3.2079... Val Loss: 3.2215\n",
      "Epoch: 11/20... Step: 10370... Loss: 3.2123... Val Loss: 3.1983\n",
      "Epoch: 11/20... Step: 10380... Loss: 3.1760... Val Loss: 3.2026\n",
      "Epoch: 11/20... Step: 10390... Loss: 3.1913... Val Loss: 3.2126\n",
      "Epoch: 11/20... Step: 10400... Loss: 3.2305... Val Loss: 3.1743\n",
      "Epoch: 11/20... Step: 10410... Loss: 3.2068... Val Loss: 3.2024\n",
      "Epoch: 11/20... Step: 10420... Loss: 3.1948... Val Loss: 3.1966\n",
      "Epoch: 11/20... Step: 10430... Loss: 3.2021... Val Loss: 3.2139\n",
      "Epoch: 11/20... Step: 10440... Loss: 3.2155... Val Loss: 3.2006\n",
      "Epoch: 11/20... Step: 10450... Loss: 3.2189... Val Loss: 3.1934\n",
      "Epoch: 11/20... Step: 10460... Loss: 3.2107... Val Loss: 3.1984\n",
      "Epoch: 11/20... Step: 10470... Loss: 3.2235... Val Loss: 3.1947\n",
      "Epoch: 11/20... Step: 10480... Loss: 3.1747... Val Loss: 3.1876\n",
      "Epoch: 11/20... Step: 10490... Loss: 3.2229... Val Loss: 3.1936\n",
      "Epoch: 11/20... Step: 10500... Loss: 3.2162... Val Loss: 3.2271\n",
      "Epoch: 11/20... Step: 10510... Loss: 3.1957... Val Loss: 3.1744\n",
      "Epoch: 11/20... Step: 10520... Loss: 3.2160... Val Loss: 3.1888\n",
      "Epoch: 11/20... Step: 10530... Loss: 3.1962... Val Loss: 3.1618\n",
      "Epoch: 11/20... Step: 10540... Loss: 3.2082... Val Loss: 3.2238\n",
      "Epoch: 11/20... Step: 10550... Loss: 3.2162... Val Loss: 3.2060\n",
      "Epoch: 11/20... Step: 10560... Loss: 3.1808... Val Loss: 3.2014\n",
      "Epoch: 11/20... Step: 10570... Loss: 3.2092... Val Loss: 3.2057\n",
      "Epoch: 11/20... Step: 10580... Loss: 3.2203... Val Loss: 3.2203\n",
      "Epoch: 11/20... Step: 10590... Loss: 3.1960... Val Loss: 3.2040\n",
      "Epoch: 11/20... Step: 10600... Loss: 3.1890... Val Loss: 3.2302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20... Step: 10610... Loss: 3.1917... Val Loss: 3.1964\n",
      "Epoch: 11/20... Step: 10620... Loss: 3.2216... Val Loss: 3.1865\n",
      "Epoch: 11/20... Step: 10630... Loss: 3.2169... Val Loss: 3.2175\n",
      "Epoch: 11/20... Step: 10640... Loss: 3.2047... Val Loss: 3.1714\n",
      "Epoch: 11/20... Step: 10650... Loss: 3.2031... Val Loss: 3.1968\n",
      "Epoch: 11/20... Step: 10660... Loss: 3.1995... Val Loss: 3.2279\n",
      "Epoch: 11/20... Step: 10670... Loss: 3.1880... Val Loss: 3.1916\n",
      "Epoch: 11/20... Step: 10680... Loss: 3.1798... Val Loss: 3.2077\n",
      "Epoch: 11/20... Step: 10690... Loss: 3.2016... Val Loss: 3.2168\n",
      "Epoch: 11/20... Step: 10700... Loss: 3.1946... Val Loss: 3.2167\n",
      "Epoch: 11/20... Step: 10710... Loss: 3.2001... Val Loss: 3.2209\n",
      "Epoch: 11/20... Step: 10720... Loss: 3.2067... Val Loss: 3.2302\n",
      "Epoch: 11/20... Step: 10730... Loss: 3.1813... Val Loss: 3.2027\n",
      "Epoch: 11/20... Step: 10740... Loss: 3.2184... Val Loss: 3.1896\n",
      "Epoch: 11/20... Step: 10750... Loss: 3.2036... Val Loss: 3.2091\n",
      "Epoch: 11/20... Step: 10760... Loss: 3.2137... Val Loss: 3.1755\n",
      "Epoch: 11/20... Step: 10770... Loss: 3.2084... Val Loss: 3.1970\n",
      "Epoch: 11/20... Step: 10780... Loss: 3.2146... Val Loss: 3.2038\n",
      "Epoch: 11/20... Step: 10790... Loss: 3.2186... Val Loss: 3.2476\n",
      "Epoch: 11/20... Step: 10800... Loss: 3.2009... Val Loss: 3.2087\n",
      "Epoch: 11/20... Step: 10810... Loss: 3.2137... Val Loss: 3.2045\n",
      "Epoch: 11/20... Step: 10820... Loss: 3.1842... Val Loss: 3.1924\n",
      "Epoch: 11/20... Step: 10830... Loss: 3.1757... Val Loss: 3.1999\n",
      "Epoch: 11/20... Step: 10840... Loss: 3.2016... Val Loss: 3.2054\n",
      "Epoch: 11/20... Step: 10850... Loss: 3.1929... Val Loss: 3.1946\n",
      "Epoch: 11/20... Step: 10860... Loss: 3.2112... Val Loss: 3.1917\n",
      "Epoch: 11/20... Step: 10870... Loss: 3.2156... Val Loss: 3.2041\n",
      "Epoch: 11/20... Step: 10880... Loss: 3.2105... Val Loss: 3.2077\n",
      "Epoch: 11/20... Step: 10890... Loss: 3.2129... Val Loss: 3.1909\n",
      "Epoch: 11/20... Step: 10900... Loss: 3.2148... Val Loss: 3.2054\n",
      "Epoch: 11/20... Step: 10910... Loss: 3.1975... Val Loss: 3.1920\n",
      "Epoch: 11/20... Step: 10920... Loss: 3.2278... Val Loss: 3.2104\n",
      "Epoch: 11/20... Step: 10930... Loss: 3.2140... Val Loss: 3.1852\n",
      "Epoch: 11/20... Step: 10940... Loss: 3.2054... Val Loss: 3.2087\n",
      "Epoch: 11/20... Step: 10950... Loss: 3.1860... Val Loss: 3.2207\n",
      "Epoch: 11/20... Step: 10960... Loss: 3.2193... Val Loss: 3.2085\n",
      "Epoch: 11/20... Step: 10970... Loss: 3.2089... Val Loss: 3.1932\n",
      "Epoch: 11/20... Step: 10980... Loss: 3.1946... Val Loss: 3.2018\n",
      "Epoch: 11/20... Step: 10990... Loss: 3.1942... Val Loss: 3.1845\n",
      "Epoch: 12/20... Step: 11000... Loss: 3.1836... Val Loss: 3.1910\n",
      "Epoch: 12/20... Step: 11010... Loss: 3.2173... Val Loss: 3.2135\n",
      "Epoch: 12/20... Step: 11020... Loss: 3.2291... Val Loss: 3.1977\n",
      "Epoch: 12/20... Step: 11030... Loss: 3.2055... Val Loss: 3.2238\n",
      "Epoch: 12/20... Step: 11040... Loss: 3.2111... Val Loss: 3.2024\n",
      "Epoch: 12/20... Step: 11050... Loss: 3.2501... Val Loss: 3.2056\n",
      "Epoch: 12/20... Step: 11060... Loss: 3.2062... Val Loss: 3.2164\n",
      "Epoch: 12/20... Step: 11070... Loss: 3.1966... Val Loss: 3.2049\n",
      "Epoch: 12/20... Step: 11080... Loss: 3.2078... Val Loss: 3.2142\n",
      "Epoch: 12/20... Step: 11090... Loss: 3.2004... Val Loss: 3.1945\n",
      "Epoch: 12/20... Step: 11100... Loss: 3.2197... Val Loss: 3.2109\n",
      "Epoch: 12/20... Step: 11110... Loss: 3.2046... Val Loss: 3.2237\n",
      "Epoch: 12/20... Step: 11120... Loss: 3.2040... Val Loss: 3.1855\n",
      "Epoch: 12/20... Step: 11130... Loss: 3.2129... Val Loss: 3.1798\n",
      "Epoch: 12/20... Step: 11140... Loss: 3.1994... Val Loss: 3.1845\n",
      "Epoch: 12/20... Step: 11150... Loss: 3.2090... Val Loss: 3.2022\n",
      "Epoch: 12/20... Step: 11160... Loss: 3.2027... Val Loss: 3.2281\n",
      "Epoch: 12/20... Step: 11170... Loss: 3.1976... Val Loss: 3.2033\n",
      "Epoch: 12/20... Step: 11180... Loss: 3.2024... Val Loss: 3.2184\n",
      "Epoch: 12/20... Step: 11190... Loss: 3.2355... Val Loss: 3.2059\n",
      "Epoch: 12/20... Step: 11200... Loss: 3.2012... Val Loss: 3.1879\n",
      "Epoch: 12/20... Step: 11210... Loss: 3.2063... Val Loss: 3.2053\n",
      "Epoch: 12/20... Step: 11220... Loss: 3.2204... Val Loss: 3.2181\n",
      "Epoch: 12/20... Step: 11230... Loss: 3.1963... Val Loss: 3.1954\n",
      "Epoch: 12/20... Step: 11240... Loss: 3.2109... Val Loss: 3.1708\n",
      "Epoch: 12/20... Step: 11250... Loss: 3.1942... Val Loss: 3.1862\n",
      "Epoch: 12/20... Step: 11260... Loss: 3.1887... Val Loss: 3.2051\n",
      "Epoch: 12/20... Step: 11270... Loss: 3.1976... Val Loss: 3.2373\n",
      "Epoch: 12/20... Step: 11280... Loss: 3.2082... Val Loss: 3.1820\n",
      "Epoch: 12/20... Step: 11290... Loss: 3.2177... Val Loss: 3.2064\n",
      "Epoch: 12/20... Step: 11300... Loss: 3.2040... Val Loss: 3.2155\n",
      "Epoch: 12/20... Step: 11310... Loss: 3.2141... Val Loss: 3.2161\n",
      "Epoch: 12/20... Step: 11320... Loss: 3.1866... Val Loss: 3.2083\n",
      "Epoch: 12/20... Step: 11330... Loss: 3.2269... Val Loss: 3.2135\n",
      "Epoch: 12/20... Step: 11340... Loss: 3.1915... Val Loss: 3.2070\n",
      "Epoch: 12/20... Step: 11350... Loss: 3.2014... Val Loss: 3.1984\n",
      "Epoch: 12/20... Step: 11360... Loss: 3.2025... Val Loss: 3.2149\n",
      "Epoch: 12/20... Step: 11370... Loss: 3.2121... Val Loss: 3.2050\n",
      "Epoch: 12/20... Step: 11380... Loss: 3.1871... Val Loss: 3.1867\n",
      "Epoch: 12/20... Step: 11390... Loss: 3.2145... Val Loss: 3.1831\n",
      "Epoch: 12/20... Step: 11400... Loss: 3.2035... Val Loss: 3.2194\n",
      "Epoch: 12/20... Step: 11410... Loss: 3.1998... Val Loss: 3.2098\n",
      "Epoch: 12/20... Step: 11420... Loss: 3.2010... Val Loss: 3.2057\n",
      "Epoch: 12/20... Step: 11430... Loss: 3.2175... Val Loss: 3.1770\n",
      "Epoch: 12/20... Step: 11440... Loss: 3.1955... Val Loss: 3.1871\n",
      "Epoch: 12/20... Step: 11450... Loss: 3.2198... Val Loss: 3.1876\n",
      "Epoch: 12/20... Step: 11460... Loss: 3.1942... Val Loss: 3.2129\n",
      "Epoch: 12/20... Step: 11470... Loss: 3.1948... Val Loss: 3.1979\n",
      "Epoch: 12/20... Step: 11480... Loss: 3.1990... Val Loss: 3.1904\n",
      "Epoch: 12/20... Step: 11490... Loss: 3.1935... Val Loss: 3.2104\n",
      "Epoch: 12/20... Step: 11500... Loss: 3.2224... Val Loss: 3.2172\n",
      "Epoch: 12/20... Step: 11510... Loss: 3.2100... Val Loss: 3.2119\n",
      "Epoch: 12/20... Step: 11520... Loss: 3.1685... Val Loss: 3.1937\n",
      "Epoch: 12/20... Step: 11530... Loss: 3.2062... Val Loss: 3.1872\n",
      "Epoch: 12/20... Step: 11540... Loss: 3.1853... Val Loss: 3.1989\n",
      "Epoch: 12/20... Step: 11550... Loss: 3.2039... Val Loss: 3.2277\n",
      "Epoch: 12/20... Step: 11560... Loss: 3.2154... Val Loss: 3.1991\n",
      "Epoch: 12/20... Step: 11570... Loss: 3.1841... Val Loss: 3.2246\n",
      "Epoch: 12/20... Step: 11580... Loss: 3.2123... Val Loss: 3.1864\n",
      "Epoch: 12/20... Step: 11590... Loss: 3.2076... Val Loss: 3.2090\n",
      "Epoch: 12/20... Step: 11600... Loss: 3.1896... Val Loss: 3.1825\n",
      "Epoch: 12/20... Step: 11610... Loss: 3.2278... Val Loss: 3.1878\n",
      "Epoch: 12/20... Step: 11620... Loss: 3.2173... Val Loss: 3.2030\n",
      "Epoch: 12/20... Step: 11630... Loss: 3.1953... Val Loss: 3.1918\n",
      "Epoch: 12/20... Step: 11640... Loss: 3.2029... Val Loss: 3.1999\n",
      "Epoch: 12/20... Step: 11650... Loss: 3.2125... Val Loss: 3.1857\n",
      "Epoch: 12/20... Step: 11660... Loss: 3.1933... Val Loss: 3.2278\n",
      "Epoch: 12/20... Step: 11670... Loss: 3.2013... Val Loss: 3.2006\n",
      "Epoch: 12/20... Step: 11680... Loss: 3.2184... Val Loss: 3.2016\n",
      "Epoch: 12/20... Step: 11690... Loss: 3.2078... Val Loss: 3.2107\n",
      "Epoch: 12/20... Step: 11700... Loss: 3.2096... Val Loss: 3.2071\n",
      "Epoch: 12/20... Step: 11710... Loss: 3.1955... Val Loss: 3.1835\n",
      "Epoch: 12/20... Step: 11720... Loss: 3.2195... Val Loss: 3.2364\n",
      "Epoch: 12/20... Step: 11730... Loss: 3.2087... Val Loss: 3.2099\n",
      "Epoch: 12/20... Step: 11740... Loss: 3.2263... Val Loss: 3.1895\n",
      "Epoch: 12/20... Step: 11750... Loss: 3.1940... Val Loss: 3.1770\n",
      "Epoch: 12/20... Step: 11760... Loss: 3.2287... Val Loss: 3.1944\n",
      "Epoch: 12/20... Step: 11770... Loss: 3.2142... Val Loss: 3.2033\n",
      "Epoch: 12/20... Step: 11780... Loss: 3.2040... Val Loss: 3.1984\n",
      "Epoch: 12/20... Step: 11790... Loss: 3.2072... Val Loss: 3.2078\n",
      "Epoch: 12/20... Step: 11800... Loss: 3.2013... Val Loss: 3.1786\n",
      "Epoch: 12/20... Step: 11810... Loss: 3.2022... Val Loss: 3.2153\n",
      "Epoch: 12/20... Step: 11820... Loss: 3.2052... Val Loss: 3.2030\n",
      "Epoch: 12/20... Step: 11830... Loss: 3.2216... Val Loss: 3.2112\n",
      "Epoch: 12/20... Step: 11840... Loss: 3.1897... Val Loss: 3.2008\n",
      "Epoch: 12/20... Step: 11850... Loss: 3.1959... Val Loss: 3.2114\n",
      "Epoch: 12/20... Step: 11860... Loss: 3.2110... Val Loss: 3.2037\n",
      "Epoch: 12/20... Step: 11870... Loss: 3.2195... Val Loss: 3.1800\n",
      "Epoch: 12/20... Step: 11880... Loss: 3.1815... Val Loss: 3.2298\n",
      "Epoch: 12/20... Step: 11890... Loss: 3.1840... Val Loss: 3.2384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20... Step: 11900... Loss: 3.1952... Val Loss: 3.2273\n",
      "Epoch: 12/20... Step: 11910... Loss: 3.2375... Val Loss: 3.1992\n",
      "Epoch: 12/20... Step: 11920... Loss: 3.2002... Val Loss: 3.1952\n",
      "Epoch: 12/20... Step: 11930... Loss: 3.1971... Val Loss: 3.1910\n",
      "Epoch: 12/20... Step: 11940... Loss: 3.2070... Val Loss: 3.1817\n",
      "Epoch: 12/20... Step: 11950... Loss: 3.1866... Val Loss: 3.1818\n",
      "Epoch: 12/20... Step: 11960... Loss: 3.1798... Val Loss: 3.2010\n",
      "Epoch: 12/20... Step: 11970... Loss: 3.2253... Val Loss: 3.1849\n",
      "Epoch: 12/20... Step: 11980... Loss: 3.2164... Val Loss: 3.2422\n",
      "Epoch: 12/20... Step: 11990... Loss: 3.1917... Val Loss: 3.2243\n",
      "Epoch: 13/20... Step: 12000... Loss: 3.2325... Val Loss: 3.1922\n",
      "Epoch: 13/20... Step: 12010... Loss: 3.2182... Val Loss: 3.1981\n",
      "Epoch: 13/20... Step: 12020... Loss: 3.1949... Val Loss: 3.2013\n",
      "Epoch: 13/20... Step: 12030... Loss: 3.1876... Val Loss: 3.1910\n",
      "Epoch: 13/20... Step: 12040... Loss: 3.2151... Val Loss: 3.2203\n",
      "Epoch: 13/20... Step: 12050... Loss: 3.2041... Val Loss: 3.2180\n",
      "Epoch: 13/20... Step: 12060... Loss: 3.2187... Val Loss: 3.1949\n",
      "Epoch: 13/20... Step: 12070... Loss: 3.1751... Val Loss: 3.2080\n",
      "Epoch: 13/20... Step: 12080... Loss: 3.1803... Val Loss: 3.2196\n",
      "Epoch: 13/20... Step: 12090... Loss: 3.1792... Val Loss: 3.2122\n",
      "Epoch: 13/20... Step: 12100... Loss: 3.1801... Val Loss: 3.1914\n",
      "Epoch: 13/20... Step: 12110... Loss: 3.2003... Val Loss: 3.2052\n",
      "Epoch: 13/20... Step: 12120... Loss: 3.1898... Val Loss: 3.2213\n",
      "Epoch: 13/20... Step: 12130... Loss: 3.1850... Val Loss: 3.1982\n",
      "Epoch: 13/20... Step: 12140... Loss: 3.2196... Val Loss: 3.2157\n",
      "Epoch: 13/20... Step: 12150... Loss: 3.2241... Val Loss: 3.2017\n",
      "Epoch: 13/20... Step: 12160... Loss: 3.1815... Val Loss: 3.1818\n",
      "Epoch: 13/20... Step: 12170... Loss: 3.1775... Val Loss: 3.1884\n",
      "Epoch: 13/20... Step: 12180... Loss: 3.2122... Val Loss: 3.2108\n",
      "Epoch: 13/20... Step: 12190... Loss: 3.2056... Val Loss: 3.1976\n",
      "Epoch: 13/20... Step: 12200... Loss: 3.2165... Val Loss: 3.2308\n",
      "Epoch: 13/20... Step: 12210... Loss: 3.1919... Val Loss: 3.2024\n",
      "Epoch: 13/20... Step: 12220... Loss: 3.2044... Val Loss: 3.2318\n",
      "Epoch: 13/20... Step: 12230... Loss: 3.2141... Val Loss: 3.1895\n",
      "Epoch: 13/20... Step: 12240... Loss: 3.2028... Val Loss: 3.2031\n",
      "Epoch: 13/20... Step: 12250... Loss: 3.1946... Val Loss: 3.2077\n",
      "Epoch: 13/20... Step: 12260... Loss: 3.2031... Val Loss: 3.2100\n",
      "Epoch: 13/20... Step: 12270... Loss: 3.2036... Val Loss: 3.1966\n",
      "Epoch: 13/20... Step: 12280... Loss: 3.2281... Val Loss: 3.2142\n",
      "Epoch: 13/20... Step: 12290... Loss: 3.2309... Val Loss: 3.1802\n",
      "Epoch: 13/20... Step: 12300... Loss: 3.2235... Val Loss: 3.2033\n",
      "Epoch: 13/20... Step: 12310... Loss: 3.1971... Val Loss: 3.2069\n",
      "Epoch: 13/20... Step: 12320... Loss: 3.1796... Val Loss: 3.2025\n",
      "Epoch: 13/20... Step: 12330... Loss: 3.2008... Val Loss: 3.1948\n",
      "Epoch: 13/20... Step: 12340... Loss: 3.2255... Val Loss: 3.2167\n",
      "Epoch: 13/20... Step: 12350... Loss: 3.2317... Val Loss: 3.1961\n",
      "Epoch: 13/20... Step: 12360... Loss: 3.1851... Val Loss: 3.2232\n",
      "Epoch: 13/20... Step: 12370... Loss: 3.2202... Val Loss: 3.2010\n",
      "Epoch: 13/20... Step: 12380... Loss: 3.1959... Val Loss: 3.2229\n",
      "Epoch: 13/20... Step: 12390... Loss: 3.2004... Val Loss: 3.2001\n",
      "Epoch: 13/20... Step: 12400... Loss: 3.1977... Val Loss: 3.1929\n",
      "Epoch: 13/20... Step: 12410... Loss: 3.2165... Val Loss: 3.1849\n",
      "Epoch: 13/20... Step: 12420... Loss: 3.1826... Val Loss: 3.1960\n",
      "Epoch: 13/20... Step: 12430... Loss: 3.2056... Val Loss: 3.2017\n",
      "Epoch: 13/20... Step: 12440... Loss: 3.2106... Val Loss: 3.2293\n",
      "Epoch: 13/20... Step: 12450... Loss: 3.2189... Val Loss: 3.1880\n",
      "Epoch: 13/20... Step: 12460... Loss: 3.1985... Val Loss: 3.2131\n",
      "Epoch: 13/20... Step: 12470... Loss: 3.1830... Val Loss: 3.2109\n",
      "Epoch: 13/20... Step: 12480... Loss: 3.2203... Val Loss: 3.2245\n",
      "Epoch: 13/20... Step: 12490... Loss: 3.2123... Val Loss: 3.1661\n",
      "Epoch: 13/20... Step: 12500... Loss: 3.2224... Val Loss: 3.1998\n",
      "Epoch: 13/20... Step: 12510... Loss: 3.2024... Val Loss: 3.2349\n",
      "Epoch: 13/20... Step: 12520... Loss: 3.2111... Val Loss: 3.1982\n",
      "Epoch: 13/20... Step: 12530... Loss: 3.2167... Val Loss: 3.2098\n",
      "Epoch: 13/20... Step: 12540... Loss: 3.1977... Val Loss: 3.1937\n",
      "Epoch: 13/20... Step: 12550... Loss: 3.1909... Val Loss: 3.2236\n",
      "Epoch: 13/20... Step: 12560... Loss: 3.1978... Val Loss: 3.1970\n",
      "Epoch: 13/20... Step: 12570... Loss: 3.1897... Val Loss: 3.1797\n",
      "Epoch: 13/20... Step: 12580... Loss: 3.2140... Val Loss: 3.2280\n",
      "Epoch: 13/20... Step: 12590... Loss: 3.1970... Val Loss: 3.2008\n",
      "Epoch: 13/20... Step: 12600... Loss: 3.2015... Val Loss: 3.2067\n",
      "Epoch: 13/20... Step: 12610... Loss: 3.1911... Val Loss: 3.2016\n",
      "Epoch: 13/20... Step: 12620... Loss: 3.2162... Val Loss: 3.2125\n",
      "Epoch: 13/20... Step: 12630... Loss: 3.2156... Val Loss: 3.1861\n",
      "Epoch: 13/20... Step: 12640... Loss: 3.2012... Val Loss: 3.2270\n",
      "Epoch: 13/20... Step: 12650... Loss: 3.2064... Val Loss: 3.1886\n",
      "Epoch: 13/20... Step: 12660... Loss: 3.1872... Val Loss: 3.2047\n",
      "Epoch: 13/20... Step: 12670... Loss: 3.1954... Val Loss: 3.1989\n",
      "Epoch: 13/20... Step: 12680... Loss: 3.2092... Val Loss: 3.2201\n",
      "Epoch: 13/20... Step: 12690... Loss: 3.2104... Val Loss: 3.1766\n",
      "Epoch: 13/20... Step: 12700... Loss: 3.2023... Val Loss: 3.1856\n",
      "Epoch: 13/20... Step: 12710... Loss: 3.2131... Val Loss: 3.1884\n",
      "Epoch: 13/20... Step: 12720... Loss: 3.2248... Val Loss: 3.2028\n",
      "Epoch: 13/20... Step: 12730... Loss: 3.2172... Val Loss: 3.1859\n",
      "Epoch: 13/20... Step: 12740... Loss: 3.2218... Val Loss: 3.2218\n",
      "Epoch: 13/20... Step: 12750... Loss: 3.2427... Val Loss: 3.2263\n",
      "Epoch: 13/20... Step: 12760... Loss: 3.1978... Val Loss: 3.2100\n",
      "Epoch: 13/20... Step: 12770... Loss: 3.2034... Val Loss: 3.2080\n",
      "Epoch: 13/20... Step: 12780... Loss: 3.1995... Val Loss: 3.2023\n",
      "Epoch: 13/20... Step: 12790... Loss: 3.1836... Val Loss: 3.1857\n",
      "Epoch: 13/20... Step: 12800... Loss: 3.2120... Val Loss: 3.2181\n",
      "Epoch: 13/20... Step: 12810... Loss: 3.2133... Val Loss: 3.1702\n",
      "Epoch: 13/20... Step: 12820... Loss: 3.1983... Val Loss: 3.2017\n",
      "Epoch: 13/20... Step: 12830... Loss: 3.2176... Val Loss: 3.1926\n",
      "Epoch: 13/20... Step: 12840... Loss: 3.1944... Val Loss: 3.1967\n",
      "Epoch: 13/20... Step: 12850... Loss: 3.1922... Val Loss: 3.1844\n",
      "Epoch: 13/20... Step: 12860... Loss: 3.2020... Val Loss: 3.2210\n",
      "Epoch: 13/20... Step: 12870... Loss: 3.1993... Val Loss: 3.2196\n",
      "Epoch: 13/20... Step: 12880... Loss: 3.2105... Val Loss: 3.2015\n",
      "Epoch: 13/20... Step: 12890... Loss: 3.2039... Val Loss: 3.1798\n",
      "Epoch: 13/20... Step: 12900... Loss: 3.1579... Val Loss: 3.2170\n",
      "Epoch: 13/20... Step: 12910... Loss: 3.2104... Val Loss: 3.2438\n",
      "Epoch: 13/20... Step: 12920... Loss: 3.2261... Val Loss: 3.2130\n",
      "Epoch: 13/20... Step: 12930... Loss: 3.2117... Val Loss: 3.2221\n",
      "Epoch: 13/20... Step: 12940... Loss: 3.2137... Val Loss: 3.1923\n",
      "Epoch: 13/20... Step: 12950... Loss: 3.1949... Val Loss: 3.2011\n",
      "Epoch: 13/20... Step: 12960... Loss: 3.2044... Val Loss: 3.2155\n",
      "Epoch: 13/20... Step: 12970... Loss: 3.1974... Val Loss: 3.2068\n",
      "Epoch: 13/20... Step: 12980... Loss: 3.2068... Val Loss: 3.2091\n",
      "Epoch: 13/20... Step: 12990... Loss: 3.1960... Val Loss: 3.2283\n",
      "Epoch: 14/20... Step: 13000... Loss: 3.1999... Val Loss: 3.2040\n",
      "Epoch: 14/20... Step: 13010... Loss: 3.2037... Val Loss: 3.1907\n",
      "Epoch: 14/20... Step: 13020... Loss: 3.2107... Val Loss: 3.2117\n",
      "Epoch: 14/20... Step: 13030... Loss: 3.2148... Val Loss: 3.1942\n",
      "Epoch: 14/20... Step: 13040... Loss: 3.2114... Val Loss: 3.1957\n",
      "Epoch: 14/20... Step: 13050... Loss: 3.2047... Val Loss: 3.2055\n",
      "Epoch: 14/20... Step: 13060... Loss: 3.1913... Val Loss: 3.2459\n",
      "Epoch: 14/20... Step: 13070... Loss: 3.2229... Val Loss: 3.1915\n",
      "Epoch: 14/20... Step: 13080... Loss: 3.1948... Val Loss: 3.2150\n",
      "Epoch: 14/20... Step: 13090... Loss: 3.2042... Val Loss: 3.1879\n",
      "Epoch: 14/20... Step: 13100... Loss: 3.2202... Val Loss: 3.2088\n",
      "Epoch: 14/20... Step: 13110... Loss: 3.1845... Val Loss: 3.1921\n",
      "Epoch: 14/20... Step: 13120... Loss: 3.2145... Val Loss: 3.2083\n",
      "Epoch: 14/20... Step: 13130... Loss: 3.1928... Val Loss: 3.1872\n",
      "Epoch: 14/20... Step: 13140... Loss: 3.1905... Val Loss: 3.2223\n",
      "Epoch: 14/20... Step: 13150... Loss: 3.1911... Val Loss: 3.2128\n",
      "Epoch: 14/20... Step: 13160... Loss: 3.2039... Val Loss: 3.1981\n",
      "Epoch: 14/20... Step: 13170... Loss: 3.2320... Val Loss: 3.2123\n",
      "Epoch: 14/20... Step: 13180... Loss: 3.1866... Val Loss: 3.2157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20... Step: 13190... Loss: 3.1986... Val Loss: 3.1905\n",
      "Epoch: 14/20... Step: 13200... Loss: 3.1901... Val Loss: 3.2126\n",
      "Epoch: 14/20... Step: 13210... Loss: 3.2078... Val Loss: 3.1993\n",
      "Epoch: 14/20... Step: 13220... Loss: 3.1953... Val Loss: 3.1965\n",
      "Epoch: 14/20... Step: 13230... Loss: 3.2197... Val Loss: 3.2178\n",
      "Epoch: 14/20... Step: 13240... Loss: 3.2194... Val Loss: 3.2188\n",
      "Epoch: 14/20... Step: 13250... Loss: 3.2011... Val Loss: 3.2124\n",
      "Epoch: 14/20... Step: 13260... Loss: 3.2166... Val Loss: 3.1961\n",
      "Epoch: 14/20... Step: 13270... Loss: 3.1972... Val Loss: 3.1962\n",
      "Epoch: 14/20... Step: 13280... Loss: 3.2231... Val Loss: 3.2072\n",
      "Epoch: 14/20... Step: 13290... Loss: 3.1839... Val Loss: 3.1867\n",
      "Epoch: 14/20... Step: 13300... Loss: 3.2059... Val Loss: 3.2025\n",
      "Epoch: 14/20... Step: 13310... Loss: 3.2366... Val Loss: 3.2104\n",
      "Epoch: 14/20... Step: 13320... Loss: 3.2239... Val Loss: 3.1950\n",
      "Epoch: 14/20... Step: 13330... Loss: 3.2088... Val Loss: 3.2166\n",
      "Epoch: 14/20... Step: 13340... Loss: 3.2094... Val Loss: 3.1990\n",
      "Epoch: 14/20... Step: 13350... Loss: 3.2175... Val Loss: 3.2174\n",
      "Epoch: 14/20... Step: 13360... Loss: 3.1922... Val Loss: 3.2040\n",
      "Epoch: 14/20... Step: 13370... Loss: 3.2308... Val Loss: 3.1880\n",
      "Epoch: 14/20... Step: 13380... Loss: 3.1940... Val Loss: 3.1932\n",
      "Epoch: 14/20... Step: 13390... Loss: 3.2055... Val Loss: 3.1741\n",
      "Epoch: 14/20... Step: 13400... Loss: 3.2080... Val Loss: 3.1981\n",
      "Epoch: 14/20... Step: 13410... Loss: 3.1987... Val Loss: 3.1832\n",
      "Epoch: 14/20... Step: 13420... Loss: 3.1995... Val Loss: 3.1865\n",
      "Epoch: 14/20... Step: 13430... Loss: 3.1993... Val Loss: 3.2007\n",
      "Epoch: 14/20... Step: 13440... Loss: 3.2518... Val Loss: 3.1889\n",
      "Epoch: 14/20... Step: 13450... Loss: 3.2211... Val Loss: 3.2011\n",
      "Epoch: 14/20... Step: 13460... Loss: 3.2000... Val Loss: 3.2114\n",
      "Epoch: 14/20... Step: 13470... Loss: 3.1981... Val Loss: 3.2354\n",
      "Epoch: 14/20... Step: 13480... Loss: 3.2264... Val Loss: 3.2318\n",
      "Epoch: 14/20... Step: 13490... Loss: 3.2028... Val Loss: 3.1890\n",
      "Epoch: 14/20... Step: 13500... Loss: 3.1979... Val Loss: 3.2012\n",
      "Epoch: 14/20... Step: 13510... Loss: 3.1954... Val Loss: 3.2039\n",
      "Epoch: 14/20... Step: 13520... Loss: 3.1996... Val Loss: 3.2216\n",
      "Epoch: 14/20... Step: 13530... Loss: 3.1979... Val Loss: 3.1781\n",
      "Epoch: 14/20... Step: 13540... Loss: 3.2170... Val Loss: 3.2184\n",
      "Epoch: 14/20... Step: 13550... Loss: 3.2132... Val Loss: 3.2001\n",
      "Epoch: 14/20... Step: 13560... Loss: 3.1656... Val Loss: 3.1969\n",
      "Epoch: 14/20... Step: 13570... Loss: 3.2042... Val Loss: 3.2041\n",
      "Epoch: 14/20... Step: 13580... Loss: 3.2212... Val Loss: 3.2251\n",
      "Epoch: 14/20... Step: 13590... Loss: 3.1849... Val Loss: 3.1928\n",
      "Epoch: 14/20... Step: 13600... Loss: 3.2107... Val Loss: 3.1928\n",
      "Epoch: 14/20... Step: 13610... Loss: 3.1956... Val Loss: 3.2044\n",
      "Epoch: 14/20... Step: 13620... Loss: 3.1982... Val Loss: 3.2113\n",
      "Epoch: 14/20... Step: 13630... Loss: 3.2273... Val Loss: 3.1869\n",
      "Epoch: 14/20... Step: 13640... Loss: 3.2297... Val Loss: 3.1938\n",
      "Epoch: 14/20... Step: 13650... Loss: 3.2138... Val Loss: 3.1757\n",
      "Epoch: 14/20... Step: 13660... Loss: 3.2362... Val Loss: 3.1950\n",
      "Epoch: 14/20... Step: 13670... Loss: 3.2190... Val Loss: 3.2241\n",
      "Epoch: 14/20... Step: 13680... Loss: 3.1976... Val Loss: 3.2130\n",
      "Epoch: 14/20... Step: 13690... Loss: 3.1951... Val Loss: 3.2140\n",
      "Epoch: 14/20... Step: 13700... Loss: 3.1993... Val Loss: 3.2093\n",
      "Epoch: 14/20... Step: 13710... Loss: 3.2332... Val Loss: 3.2098\n",
      "Epoch: 14/20... Step: 13720... Loss: 3.2215... Val Loss: 3.2593\n",
      "Epoch: 14/20... Step: 13730... Loss: 3.2067... Val Loss: 3.1874\n",
      "Epoch: 14/20... Step: 13740... Loss: 3.2104... Val Loss: 3.1936\n",
      "Epoch: 14/20... Step: 13750... Loss: 3.1957... Val Loss: 3.2230\n",
      "Epoch: 14/20... Step: 13760... Loss: 3.2033... Val Loss: 3.2156\n",
      "Epoch: 14/20... Step: 13770... Loss: 3.2097... Val Loss: 3.2025\n",
      "Epoch: 14/20... Step: 13780... Loss: 3.1938... Val Loss: 3.1884\n",
      "Epoch: 14/20... Step: 13790... Loss: 3.2249... Val Loss: 3.1710\n",
      "Epoch: 14/20... Step: 13800... Loss: 3.2163... Val Loss: 3.1770\n",
      "Epoch: 14/20... Step: 13810... Loss: 3.1980... Val Loss: 3.1780\n",
      "Epoch: 14/20... Step: 13820... Loss: 3.2133... Val Loss: 3.2199\n",
      "Epoch: 14/20... Step: 13830... Loss: 3.1956... Val Loss: 3.1993\n",
      "Epoch: 14/20... Step: 13840... Loss: 3.2292... Val Loss: 3.2065\n",
      "Epoch: 14/20... Step: 13850... Loss: 3.2006... Val Loss: 3.2046\n",
      "Epoch: 14/20... Step: 13860... Loss: 3.1983... Val Loss: 3.2065\n",
      "Epoch: 14/20... Step: 13870... Loss: 3.2075... Val Loss: 3.2327\n",
      "Epoch: 14/20... Step: 13880... Loss: 3.2468... Val Loss: 3.2010\n",
      "Epoch: 14/20... Step: 13890... Loss: 3.2022... Val Loss: 3.1900\n",
      "Epoch: 14/20... Step: 13900... Loss: 3.1973... Val Loss: 3.2072\n",
      "Epoch: 14/20... Step: 13910... Loss: 3.2080... Val Loss: 3.2005\n",
      "Epoch: 14/20... Step: 13920... Loss: 3.2139... Val Loss: 3.2222\n",
      "Epoch: 14/20... Step: 13930... Loss: 3.1992... Val Loss: 3.2080\n",
      "Epoch: 14/20... Step: 13940... Loss: 3.2168... Val Loss: 3.1817\n",
      "Epoch: 14/20... Step: 13950... Loss: 3.1868... Val Loss: 3.2070\n",
      "Epoch: 14/20... Step: 13960... Loss: 3.1933... Val Loss: 3.1881\n",
      "Epoch: 14/20... Step: 13970... Loss: 3.2225... Val Loss: 3.1888\n",
      "Epoch: 14/20... Step: 13980... Loss: 3.2005... Val Loss: 3.2006\n",
      "Epoch: 14/20... Step: 13990... Loss: 3.1854... Val Loss: 3.2113\n",
      "Epoch: 15/20... Step: 14000... Loss: 3.2277... Val Loss: 3.1869\n",
      "Epoch: 15/20... Step: 14010... Loss: 3.2001... Val Loss: 3.2058\n",
      "Epoch: 15/20... Step: 14020... Loss: 3.1958... Val Loss: 3.2247\n",
      "Epoch: 15/20... Step: 14030... Loss: 3.2057... Val Loss: 3.1873\n",
      "Epoch: 15/20... Step: 14040... Loss: 3.1748... Val Loss: 3.1862\n",
      "Epoch: 15/20... Step: 14050... Loss: 3.1713... Val Loss: 3.1869\n",
      "Epoch: 15/20... Step: 14060... Loss: 3.2043... Val Loss: 3.2190\n",
      "Epoch: 15/20... Step: 14070... Loss: 3.1965... Val Loss: 3.2244\n",
      "Epoch: 15/20... Step: 14080... Loss: 3.2280... Val Loss: 3.2169\n",
      "Epoch: 15/20... Step: 14090... Loss: 3.2065... Val Loss: 3.1960\n",
      "Epoch: 15/20... Step: 14100... Loss: 3.2419... Val Loss: 3.1967\n",
      "Epoch: 15/20... Step: 14110... Loss: 3.1944... Val Loss: 3.1845\n",
      "Epoch: 15/20... Step: 14120... Loss: 3.2191... Val Loss: 3.2211\n",
      "Epoch: 15/20... Step: 14130... Loss: 3.2182... Val Loss: 3.1883\n",
      "Epoch: 15/20... Step: 14140... Loss: 3.2080... Val Loss: 3.2004\n",
      "Epoch: 15/20... Step: 14150... Loss: 3.2113... Val Loss: 3.1851\n",
      "Epoch: 15/20... Step: 14160... Loss: 3.1899... Val Loss: 3.1990\n",
      "Epoch: 15/20... Step: 14170... Loss: 3.1928... Val Loss: 3.2232\n",
      "Epoch: 15/20... Step: 14180... Loss: 3.2103... Val Loss: 3.2392\n",
      "Epoch: 15/20... Step: 14190... Loss: 3.1978... Val Loss: 3.2068\n",
      "Epoch: 15/20... Step: 14200... Loss: 3.2041... Val Loss: 3.2128\n",
      "Epoch: 15/20... Step: 14210... Loss: 3.2161... Val Loss: 3.2130\n",
      "Epoch: 15/20... Step: 14220... Loss: 3.1980... Val Loss: 3.2121\n",
      "Epoch: 15/20... Step: 14230... Loss: 3.1823... Val Loss: 3.1913\n",
      "Epoch: 15/20... Step: 14240... Loss: 3.2103... Val Loss: 3.2134\n",
      "Epoch: 15/20... Step: 14250... Loss: 3.2195... Val Loss: 3.2118\n",
      "Epoch: 15/20... Step: 14260... Loss: 3.2125... Val Loss: 3.2120\n",
      "Epoch: 15/20... Step: 14270... Loss: 3.2026... Val Loss: 3.2061\n",
      "Epoch: 15/20... Step: 14280... Loss: 3.2241... Val Loss: 3.2147\n",
      "Epoch: 15/20... Step: 14290... Loss: 3.2407... Val Loss: 3.1990\n",
      "Epoch: 15/20... Step: 14300... Loss: 3.2012... Val Loss: 3.2008\n",
      "Epoch: 15/20... Step: 14310... Loss: 3.1935... Val Loss: 3.2019\n",
      "Epoch: 15/20... Step: 14320... Loss: 3.1902... Val Loss: 3.2170\n",
      "Epoch: 15/20... Step: 14330... Loss: 3.1928... Val Loss: 3.2058\n",
      "Epoch: 15/20... Step: 14340... Loss: 3.2068... Val Loss: 3.1894\n",
      "Epoch: 15/20... Step: 14350... Loss: 3.1995... Val Loss: 3.1938\n",
      "Epoch: 15/20... Step: 14360... Loss: 3.1956... Val Loss: 3.1845\n",
      "Epoch: 15/20... Step: 14370... Loss: 3.1910... Val Loss: 3.2151\n",
      "Epoch: 15/20... Step: 14380... Loss: 3.2121... Val Loss: 3.1932\n",
      "Epoch: 15/20... Step: 14390... Loss: 3.2349... Val Loss: 3.2189\n",
      "Epoch: 15/20... Step: 14400... Loss: 3.1954... Val Loss: 3.1980\n",
      "Epoch: 15/20... Step: 14410... Loss: 3.1940... Val Loss: 3.2134\n",
      "Epoch: 15/20... Step: 14420... Loss: 3.1975... Val Loss: 3.2342\n",
      "Epoch: 15/20... Step: 14430... Loss: 3.2060... Val Loss: 3.1962\n",
      "Epoch: 15/20... Step: 14440... Loss: 3.1960... Val Loss: 3.1693\n",
      "Epoch: 15/20... Step: 14450... Loss: 3.2040... Val Loss: 3.2106\n",
      "Epoch: 15/20... Step: 14460... Loss: 3.1869... Val Loss: 3.2169\n",
      "Epoch: 15/20... Step: 14470... Loss: 3.2008... Val Loss: 3.1774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20... Step: 14480... Loss: 3.2003... Val Loss: 3.1967\n",
      "Epoch: 15/20... Step: 14490... Loss: 3.1685... Val Loss: 3.2152\n",
      "Epoch: 15/20... Step: 14500... Loss: 3.2075... Val Loss: 3.2083\n",
      "Epoch: 15/20... Step: 14510... Loss: 3.2273... Val Loss: 3.1975\n",
      "Epoch: 15/20... Step: 14520... Loss: 3.1982... Val Loss: 3.2063\n",
      "Epoch: 15/20... Step: 14530... Loss: 3.1788... Val Loss: 3.2239\n",
      "Epoch: 15/20... Step: 14540... Loss: 3.2230... Val Loss: 3.1894\n",
      "Epoch: 15/20... Step: 14550... Loss: 3.1834... Val Loss: 3.1816\n",
      "Epoch: 15/20... Step: 14560... Loss: 3.1690... Val Loss: 3.1911\n",
      "Epoch: 15/20... Step: 14570... Loss: 3.2018... Val Loss: 3.2165\n",
      "Epoch: 15/20... Step: 14580... Loss: 3.2449... Val Loss: 3.2094\n",
      "Epoch: 15/20... Step: 14590... Loss: 3.2293... Val Loss: 3.1988\n",
      "Epoch: 15/20... Step: 14600... Loss: 3.2173... Val Loss: 3.1755\n",
      "Epoch: 15/20... Step: 14610... Loss: 3.2129... Val Loss: 3.1879\n",
      "Epoch: 15/20... Step: 14620... Loss: 3.2057... Val Loss: 3.1912\n",
      "Epoch: 15/20... Step: 14630... Loss: 3.2053... Val Loss: 3.2159\n",
      "Epoch: 15/20... Step: 14640... Loss: 3.2201... Val Loss: 3.2176\n",
      "Epoch: 15/20... Step: 14650... Loss: 3.2143... Val Loss: 3.2268\n",
      "Epoch: 15/20... Step: 14660... Loss: 3.1965... Val Loss: 3.2080\n",
      "Epoch: 15/20... Step: 14670... Loss: 3.2076... Val Loss: 3.1933\n",
      "Epoch: 15/20... Step: 14680... Loss: 3.2036... Val Loss: 3.2092\n",
      "Epoch: 15/20... Step: 14690... Loss: 3.2132... Val Loss: 3.2061\n",
      "Epoch: 15/20... Step: 14700... Loss: 3.2108... Val Loss: 3.1998\n",
      "Epoch: 15/20... Step: 14710... Loss: 3.2303... Val Loss: 3.2267\n",
      "Epoch: 15/20... Step: 14720... Loss: 3.1989... Val Loss: 3.2155\n",
      "Epoch: 15/20... Step: 14730... Loss: 3.1967... Val Loss: 3.2013\n",
      "Epoch: 15/20... Step: 14740... Loss: 3.2054... Val Loss: 3.1975\n",
      "Epoch: 15/20... Step: 14750... Loss: 3.2015... Val Loss: 3.2110\n",
      "Epoch: 15/20... Step: 14760... Loss: 3.1949... Val Loss: 3.1939\n",
      "Epoch: 15/20... Step: 14770... Loss: 3.2181... Val Loss: 3.1813\n",
      "Epoch: 15/20... Step: 14780... Loss: 3.2114... Val Loss: 3.1897\n",
      "Epoch: 15/20... Step: 14790... Loss: 3.2286... Val Loss: 3.1955\n",
      "Epoch: 15/20... Step: 14800... Loss: 3.2423... Val Loss: 3.1918\n",
      "Epoch: 15/20... Step: 14810... Loss: 3.2188... Val Loss: 3.1909\n",
      "Epoch: 15/20... Step: 14820... Loss: 3.2022... Val Loss: 3.2274\n",
      "Epoch: 15/20... Step: 14830... Loss: 3.1886... Val Loss: 3.2064\n",
      "Epoch: 15/20... Step: 14840... Loss: 3.2000... Val Loss: 3.1812\n",
      "Epoch: 15/20... Step: 14850... Loss: 3.2143... Val Loss: 3.2226\n",
      "Epoch: 15/20... Step: 14860... Loss: 3.1825... Val Loss: 3.2221\n",
      "Epoch: 15/20... Step: 14870... Loss: 3.2055... Val Loss: 3.2185\n",
      "Epoch: 15/20... Step: 14880... Loss: 3.2113... Val Loss: 3.2105\n",
      "Epoch: 15/20... Step: 14890... Loss: 3.2010... Val Loss: 3.2147\n",
      "Epoch: 15/20... Step: 14900... Loss: 3.2012... Val Loss: 3.1998\n",
      "Epoch: 15/20... Step: 14910... Loss: 3.2218... Val Loss: 3.1953\n",
      "Epoch: 15/20... Step: 14920... Loss: 3.1866... Val Loss: 3.1682\n",
      "Epoch: 15/20... Step: 14930... Loss: 3.2052... Val Loss: 3.2017\n",
      "Epoch: 15/20... Step: 14940... Loss: 3.2004... Val Loss: 3.1994\n",
      "Epoch: 15/20... Step: 14950... Loss: 3.1982... Val Loss: 3.1945\n",
      "Epoch: 15/20... Step: 14960... Loss: 3.1780... Val Loss: 3.2130\n",
      "Epoch: 15/20... Step: 14970... Loss: 3.1768... Val Loss: 3.2269\n",
      "Epoch: 15/20... Step: 14980... Loss: 3.2049... Val Loss: 3.2017\n",
      "Epoch: 15/20... Step: 14990... Loss: 3.1997... Val Loss: 3.2278\n",
      "Epoch: 16/20... Step: 15000... Loss: 3.2053... Val Loss: 3.2223\n",
      "Epoch: 16/20... Step: 15010... Loss: 3.1922... Val Loss: 3.2232\n",
      "Epoch: 16/20... Step: 15020... Loss: 3.1683... Val Loss: 3.1951\n",
      "Epoch: 16/20... Step: 15030... Loss: 3.1896... Val Loss: 3.1984\n",
      "Epoch: 16/20... Step: 15040... Loss: 3.1938... Val Loss: 3.1918\n",
      "Epoch: 16/20... Step: 15050... Loss: 3.2022... Val Loss: 3.2093\n",
      "Epoch: 16/20... Step: 15060... Loss: 3.1841... Val Loss: 3.2021\n",
      "Epoch: 16/20... Step: 15070... Loss: 3.2171... Val Loss: 3.2029\n",
      "Epoch: 16/20... Step: 15080... Loss: 3.2155... Val Loss: 3.1849\n",
      "Epoch: 16/20... Step: 15090... Loss: 3.2003... Val Loss: 3.2057\n",
      "Epoch: 16/20... Step: 15100... Loss: 3.1941... Val Loss: 3.2129\n",
      "Epoch: 16/20... Step: 15110... Loss: 3.1963... Val Loss: 3.2132\n",
      "Epoch: 16/20... Step: 15120... Loss: 3.2276... Val Loss: 3.2212\n",
      "Epoch: 16/20... Step: 15130... Loss: 3.2244... Val Loss: 3.1795\n",
      "Epoch: 16/20... Step: 15140... Loss: 3.2029... Val Loss: 3.2254\n",
      "Epoch: 16/20... Step: 15150... Loss: 3.1965... Val Loss: 3.2107\n",
      "Epoch: 16/20... Step: 15160... Loss: 3.1965... Val Loss: 3.1796\n",
      "Epoch: 16/20... Step: 15170... Loss: 3.1861... Val Loss: 3.2144\n",
      "Epoch: 16/20... Step: 15180... Loss: 3.1940... Val Loss: 3.1878\n",
      "Epoch: 16/20... Step: 15190... Loss: 3.1955... Val Loss: 3.1922\n",
      "Epoch: 16/20... Step: 15200... Loss: 3.1932... Val Loss: 3.1895\n",
      "Epoch: 16/20... Step: 15210... Loss: 3.2077... Val Loss: 3.2119\n",
      "Epoch: 16/20... Step: 15220... Loss: 3.2045... Val Loss: 3.1935\n",
      "Epoch: 16/20... Step: 15230... Loss: 3.2048... Val Loss: 3.2065\n",
      "Epoch: 16/20... Step: 15240... Loss: 3.2188... Val Loss: 3.2080\n",
      "Epoch: 16/20... Step: 15250... Loss: 3.2092... Val Loss: 3.2199\n",
      "Epoch: 16/20... Step: 15260... Loss: 3.1991... Val Loss: 3.2101\n",
      "Epoch: 16/20... Step: 15270... Loss: 3.2186... Val Loss: 3.2044\n",
      "Epoch: 16/20... Step: 15280... Loss: 3.2244... Val Loss: 3.2107\n",
      "Epoch: 16/20... Step: 15290... Loss: 3.2101... Val Loss: 3.2063\n",
      "Epoch: 16/20... Step: 15300... Loss: 3.2002... Val Loss: 3.2109\n",
      "Epoch: 16/20... Step: 15310... Loss: 3.2014... Val Loss: 3.1775\n",
      "Epoch: 16/20... Step: 15320... Loss: 3.2208... Val Loss: 3.1990\n",
      "Epoch: 16/20... Step: 15330... Loss: 3.1767... Val Loss: 3.1934\n",
      "Epoch: 16/20... Step: 15340... Loss: 3.2015... Val Loss: 3.1989\n",
      "Epoch: 16/20... Step: 15350... Loss: 3.2012... Val Loss: 3.2082\n",
      "Epoch: 16/20... Step: 15360... Loss: 3.1927... Val Loss: 3.2204\n",
      "Epoch: 16/20... Step: 15370... Loss: 3.1945... Val Loss: 3.2000\n",
      "Epoch: 16/20... Step: 15380... Loss: 3.2019... Val Loss: 3.2151\n",
      "Epoch: 16/20... Step: 15390... Loss: 3.1966... Val Loss: 3.2162\n",
      "Epoch: 16/20... Step: 15400... Loss: 3.1879... Val Loss: 3.2119\n",
      "Epoch: 16/20... Step: 15410... Loss: 3.2180... Val Loss: 3.2214\n",
      "Epoch: 16/20... Step: 15420... Loss: 3.1923... Val Loss: 3.1978\n",
      "Epoch: 16/20... Step: 15430... Loss: 3.1828... Val Loss: 3.2038\n",
      "Epoch: 16/20... Step: 15440... Loss: 3.2058... Val Loss: 3.2128\n",
      "Epoch: 16/20... Step: 15450... Loss: 3.2197... Val Loss: 3.1827\n",
      "Epoch: 16/20... Step: 15460... Loss: 3.2129... Val Loss: 3.1906\n",
      "Epoch: 16/20... Step: 15470... Loss: 3.2026... Val Loss: 3.1998\n",
      "Epoch: 16/20... Step: 15480... Loss: 3.2073... Val Loss: 3.2022\n",
      "Epoch: 16/20... Step: 15490... Loss: 3.1997... Val Loss: 3.2078\n",
      "Epoch: 16/20... Step: 15500... Loss: 3.2340... Val Loss: 3.2078\n",
      "Epoch: 16/20... Step: 15510... Loss: 3.1949... Val Loss: 3.2044\n",
      "Epoch: 16/20... Step: 15520... Loss: 3.1903... Val Loss: 3.2054\n",
      "Epoch: 16/20... Step: 15530... Loss: 3.1853... Val Loss: 3.1918\n",
      "Epoch: 16/20... Step: 15540... Loss: 3.2172... Val Loss: 3.2029\n",
      "Epoch: 16/20... Step: 15550... Loss: 3.1877... Val Loss: 3.1794\n",
      "Epoch: 16/20... Step: 15560... Loss: 3.2091... Val Loss: 3.2108\n",
      "Epoch: 16/20... Step: 15570... Loss: 3.1920... Val Loss: 3.2049\n",
      "Epoch: 16/20... Step: 15580... Loss: 3.2506... Val Loss: 3.2232\n",
      "Epoch: 16/20... Step: 15590... Loss: 3.2134... Val Loss: 3.2073\n",
      "Epoch: 16/20... Step: 15600... Loss: 3.2172... Val Loss: 3.1943\n",
      "Epoch: 16/20... Step: 15610... Loss: 3.1970... Val Loss: 3.1631\n",
      "Epoch: 16/20... Step: 15620... Loss: 3.2116... Val Loss: 3.2128\n",
      "Epoch: 16/20... Step: 15630... Loss: 3.2050... Val Loss: 3.1932\n",
      "Epoch: 16/20... Step: 15640... Loss: 3.2193... Val Loss: 3.2228\n",
      "Epoch: 16/20... Step: 15650... Loss: 3.2039... Val Loss: 3.1877\n",
      "Epoch: 16/20... Step: 15660... Loss: 3.2113... Val Loss: 3.2207\n",
      "Epoch: 16/20... Step: 15670... Loss: 3.2038... Val Loss: 3.1945\n",
      "Epoch: 16/20... Step: 15680... Loss: 3.2209... Val Loss: 3.1910\n",
      "Epoch: 16/20... Step: 15690... Loss: 3.2133... Val Loss: 3.1882\n",
      "Epoch: 16/20... Step: 15700... Loss: 3.1864... Val Loss: 3.2038\n",
      "Epoch: 16/20... Step: 15710... Loss: 3.2030... Val Loss: 3.1996\n",
      "Epoch: 16/20... Step: 15720... Loss: 3.1958... Val Loss: 3.1744\n",
      "Epoch: 16/20... Step: 15730... Loss: 3.1886... Val Loss: 3.2075\n",
      "Epoch: 16/20... Step: 15740... Loss: 3.2089... Val Loss: 3.1841\n",
      "Epoch: 16/20... Step: 15750... Loss: 3.2065... Val Loss: 3.2280\n",
      "Epoch: 16/20... Step: 15760... Loss: 3.2186... Val Loss: 3.2095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20... Step: 15770... Loss: 3.2223... Val Loss: 3.1850\n",
      "Epoch: 16/20... Step: 15780... Loss: 3.2133... Val Loss: 3.1858\n",
      "Epoch: 16/20... Step: 15790... Loss: 3.2165... Val Loss: 3.1843\n",
      "Epoch: 16/20... Step: 15800... Loss: 3.2101... Val Loss: 3.2279\n",
      "Epoch: 16/20... Step: 15810... Loss: 3.1998... Val Loss: 3.1936\n",
      "Epoch: 16/20... Step: 15820... Loss: 3.2018... Val Loss: 3.2065\n",
      "Epoch: 16/20... Step: 15830... Loss: 3.2152... Val Loss: 3.2042\n",
      "Epoch: 16/20... Step: 15840... Loss: 3.1925... Val Loss: 3.2145\n",
      "Epoch: 16/20... Step: 15850... Loss: 3.2050... Val Loss: 3.2181\n",
      "Epoch: 16/20... Step: 15860... Loss: 3.2319... Val Loss: 3.1911\n",
      "Epoch: 16/20... Step: 15870... Loss: 3.2080... Val Loss: 3.2010\n",
      "Epoch: 16/20... Step: 15880... Loss: 3.1824... Val Loss: 3.2087\n",
      "Epoch: 16/20... Step: 15890... Loss: 3.2134... Val Loss: 3.2046\n",
      "Epoch: 16/20... Step: 15900... Loss: 3.1856... Val Loss: 3.2167\n",
      "Epoch: 16/20... Step: 15910... Loss: 3.2031... Val Loss: 3.2107\n",
      "Epoch: 16/20... Step: 15920... Loss: 3.2308... Val Loss: 3.2338\n",
      "Epoch: 16/20... Step: 15930... Loss: 3.2145... Val Loss: 3.2127\n",
      "Epoch: 16/20... Step: 15940... Loss: 3.2166... Val Loss: 3.1851\n",
      "Epoch: 16/20... Step: 15950... Loss: 3.1904... Val Loss: 3.1863\n",
      "Epoch: 16/20... Step: 15960... Loss: 3.1940... Val Loss: 3.2046\n",
      "Epoch: 16/20... Step: 15970... Loss: 3.2167... Val Loss: 3.2102\n",
      "Epoch: 16/20... Step: 15980... Loss: 3.1902... Val Loss: 3.1912\n",
      "Epoch: 16/20... Step: 15990... Loss: 3.2000... Val Loss: 3.2149\n",
      "Epoch: 17/20... Step: 16000... Loss: 3.2070... Val Loss: 3.1791\n",
      "Epoch: 17/20... Step: 16010... Loss: 3.2000... Val Loss: 3.2223\n",
      "Epoch: 17/20... Step: 16020... Loss: 3.1985... Val Loss: 3.2041\n",
      "Epoch: 17/20... Step: 16030... Loss: 3.1925... Val Loss: 3.1915\n",
      "Epoch: 17/20... Step: 16040... Loss: 3.2162... Val Loss: 3.1906\n",
      "Epoch: 17/20... Step: 16050... Loss: 3.2109... Val Loss: 3.2024\n",
      "Epoch: 17/20... Step: 16060... Loss: 3.1992... Val Loss: 3.2052\n",
      "Epoch: 17/20... Step: 16070... Loss: 3.2222... Val Loss: 3.1922\n",
      "Epoch: 17/20... Step: 16080... Loss: 3.1832... Val Loss: 3.1908\n",
      "Epoch: 17/20... Step: 16090... Loss: 3.2072... Val Loss: 3.2072\n",
      "Epoch: 17/20... Step: 16100... Loss: 3.2140... Val Loss: 3.2017\n",
      "Epoch: 17/20... Step: 16110... Loss: 3.2070... Val Loss: 3.2025\n",
      "Epoch: 17/20... Step: 16120... Loss: 3.1978... Val Loss: 3.1709\n",
      "Epoch: 17/20... Step: 16130... Loss: 3.2361... Val Loss: 3.2001\n",
      "Epoch: 17/20... Step: 16140... Loss: 3.2098... Val Loss: 3.1877\n",
      "Epoch: 17/20... Step: 16150... Loss: 3.2015... Val Loss: 3.2117\n",
      "Epoch: 17/20... Step: 16160... Loss: 3.1935... Val Loss: 3.1901\n",
      "Epoch: 17/20... Step: 16170... Loss: 3.1779... Val Loss: 3.1812\n",
      "Epoch: 17/20... Step: 16180... Loss: 3.1987... Val Loss: 3.1866\n",
      "Epoch: 17/20... Step: 16190... Loss: 3.2109... Val Loss: 3.2137\n",
      "Epoch: 17/20... Step: 16200... Loss: 3.1997... Val Loss: 3.2022\n",
      "Epoch: 17/20... Step: 16210... Loss: 3.2007... Val Loss: 3.2167\n",
      "Epoch: 17/20... Step: 16220... Loss: 3.2167... Val Loss: 3.2106\n",
      "Epoch: 17/20... Step: 16230... Loss: 3.2233... Val Loss: 3.1851\n",
      "Epoch: 17/20... Step: 16240... Loss: 3.1934... Val Loss: 3.1834\n",
      "Epoch: 17/20... Step: 16250... Loss: 3.1893... Val Loss: 3.2050\n",
      "Epoch: 17/20... Step: 16260... Loss: 3.2272... Val Loss: 3.2106\n",
      "Epoch: 17/20... Step: 16270... Loss: 3.1989... Val Loss: 3.1965\n",
      "Epoch: 17/20... Step: 16280... Loss: 3.2050... Val Loss: 3.1975\n",
      "Epoch: 17/20... Step: 16290... Loss: 3.2205... Val Loss: 3.1946\n",
      "Epoch: 17/20... Step: 16300... Loss: 3.1856... Val Loss: 3.1960\n",
      "Epoch: 17/20... Step: 16310... Loss: 3.2117... Val Loss: 3.2075\n",
      "Epoch: 17/20... Step: 16320... Loss: 3.1870... Val Loss: 3.2120\n",
      "Epoch: 17/20... Step: 16330... Loss: 3.2087... Val Loss: 3.2112\n",
      "Epoch: 17/20... Step: 16340... Loss: 3.1825... Val Loss: 3.2275\n",
      "Epoch: 17/20... Step: 16350... Loss: 3.2210... Val Loss: 3.2104\n",
      "Epoch: 17/20... Step: 16360... Loss: 3.2031... Val Loss: 3.2008\n",
      "Epoch: 17/20... Step: 16370... Loss: 3.1889... Val Loss: 3.2189\n",
      "Epoch: 17/20... Step: 16380... Loss: 3.2056... Val Loss: 3.2144\n",
      "Epoch: 17/20... Step: 16390... Loss: 3.1972... Val Loss: 3.1961\n",
      "Epoch: 17/20... Step: 16400... Loss: 3.1876... Val Loss: 3.1938\n",
      "Epoch: 17/20... Step: 16410... Loss: 3.1986... Val Loss: 3.2050\n",
      "Epoch: 17/20... Step: 16420... Loss: 3.2160... Val Loss: 3.2034\n",
      "Epoch: 17/20... Step: 16430... Loss: 3.2040... Val Loss: 3.2057\n",
      "Epoch: 17/20... Step: 16440... Loss: 3.2143... Val Loss: 3.2060\n",
      "Epoch: 17/20... Step: 16450... Loss: 3.1961... Val Loss: 3.2254\n",
      "Epoch: 17/20... Step: 16460... Loss: 3.1997... Val Loss: 3.1889\n",
      "Epoch: 17/20... Step: 16470... Loss: 3.1860... Val Loss: 3.2222\n",
      "Epoch: 17/20... Step: 16480... Loss: 3.1803... Val Loss: 3.1976\n",
      "Epoch: 17/20... Step: 16490... Loss: 3.2278... Val Loss: 3.1987\n",
      "Epoch: 17/20... Step: 16500... Loss: 3.2140... Val Loss: 3.1960\n",
      "Epoch: 17/20... Step: 16510... Loss: 3.2172... Val Loss: 3.1916\n",
      "Epoch: 17/20... Step: 16520... Loss: 3.2063... Val Loss: 3.2376\n",
      "Epoch: 17/20... Step: 16530... Loss: 3.2251... Val Loss: 3.1986\n",
      "Epoch: 17/20... Step: 16540... Loss: 3.1946... Val Loss: 3.2082\n",
      "Epoch: 17/20... Step: 16550... Loss: 3.2157... Val Loss: 3.2119\n",
      "Epoch: 17/20... Step: 16560... Loss: 3.2270... Val Loss: 3.2437\n",
      "Epoch: 17/20... Step: 16570... Loss: 3.1912... Val Loss: 3.1889\n",
      "Epoch: 17/20... Step: 16580... Loss: 3.1992... Val Loss: 3.1852\n",
      "Epoch: 17/20... Step: 16590... Loss: 3.1896... Val Loss: 3.2045\n",
      "Epoch: 17/20... Step: 16600... Loss: 3.1837... Val Loss: 3.1986\n",
      "Epoch: 17/20... Step: 16610... Loss: 3.1813... Val Loss: 3.2090\n",
      "Epoch: 17/20... Step: 16620... Loss: 3.1655... Val Loss: 3.1998\n",
      "Epoch: 17/20... Step: 16630... Loss: 3.1860... Val Loss: 3.1845\n",
      "Epoch: 17/20... Step: 16640... Loss: 3.2105... Val Loss: 3.2049\n",
      "Epoch: 17/20... Step: 16650... Loss: 3.2020... Val Loss: 3.2344\n",
      "Epoch: 17/20... Step: 16660... Loss: 3.2021... Val Loss: 3.2012\n",
      "Epoch: 17/20... Step: 16670... Loss: 3.2074... Val Loss: 3.2084\n",
      "Epoch: 17/20... Step: 16680... Loss: 3.1942... Val Loss: 3.1908\n",
      "Epoch: 17/20... Step: 16690... Loss: 3.1924... Val Loss: 3.2001\n",
      "Epoch: 17/20... Step: 16700... Loss: 3.2204... Val Loss: 3.2021\n",
      "Epoch: 17/20... Step: 16710... Loss: 3.2127... Val Loss: 3.1863\n",
      "Epoch: 17/20... Step: 16720... Loss: 3.2326... Val Loss: 3.2027\n",
      "Epoch: 17/20... Step: 16730... Loss: 3.1964... Val Loss: 3.2397\n",
      "Epoch: 17/20... Step: 16740... Loss: 3.2143... Val Loss: 3.2200\n",
      "Epoch: 17/20... Step: 16750... Loss: 3.2469... Val Loss: 3.1900\n",
      "Epoch: 17/20... Step: 16760... Loss: 3.2164... Val Loss: 3.2116\n",
      "Epoch: 17/20... Step: 16770... Loss: 3.1873... Val Loss: 3.2099\n",
      "Epoch: 17/20... Step: 16780... Loss: 3.2046... Val Loss: 3.1884\n",
      "Epoch: 17/20... Step: 16790... Loss: 3.2099... Val Loss: 3.2094\n",
      "Epoch: 17/20... Step: 16800... Loss: 3.1906... Val Loss: 3.2093\n",
      "Epoch: 17/20... Step: 16810... Loss: 3.2131... Val Loss: 3.1839\n",
      "Epoch: 17/20... Step: 16820... Loss: 3.2011... Val Loss: 3.2034\n",
      "Epoch: 17/20... Step: 16830... Loss: 3.1956... Val Loss: 3.2224\n",
      "Epoch: 17/20... Step: 16840... Loss: 3.2125... Val Loss: 3.2042\n",
      "Epoch: 17/20... Step: 16850... Loss: 3.2135... Val Loss: 3.2048\n",
      "Epoch: 17/20... Step: 16860... Loss: 3.2294... Val Loss: 3.1926\n",
      "Epoch: 17/20... Step: 16870... Loss: 3.2107... Val Loss: 3.1939\n",
      "Epoch: 17/20... Step: 16880... Loss: 3.2108... Val Loss: 3.1918\n",
      "Epoch: 17/20... Step: 16890... Loss: 3.2193... Val Loss: 3.1907\n",
      "Epoch: 17/20... Step: 16900... Loss: 3.2207... Val Loss: 3.2064\n",
      "Epoch: 17/20... Step: 16910... Loss: 3.2086... Val Loss: 3.2131\n",
      "Epoch: 17/20... Step: 16920... Loss: 3.2120... Val Loss: 3.1890\n",
      "Epoch: 17/20... Step: 16930... Loss: 3.2118... Val Loss: 3.2147\n",
      "Epoch: 17/20... Step: 16940... Loss: 3.2156... Val Loss: 3.2388\n",
      "Epoch: 17/20... Step: 16950... Loss: 3.1870... Val Loss: 3.1845\n",
      "Epoch: 17/20... Step: 16960... Loss: 3.1947... Val Loss: 3.2002\n",
      "Epoch: 17/20... Step: 16970... Loss: 3.2138... Val Loss: 3.1966\n",
      "Epoch: 17/20... Step: 16980... Loss: 3.1959... Val Loss: 3.1979\n",
      "Epoch: 17/20... Step: 16990... Loss: 3.2083... Val Loss: 3.1962\n",
      "Epoch: 18/20... Step: 17000... Loss: 3.2082... Val Loss: 3.2034\n",
      "Epoch: 18/20... Step: 17010... Loss: 3.1953... Val Loss: 3.1942\n",
      "Epoch: 18/20... Step: 17020... Loss: 3.2688... Val Loss: 3.2005\n",
      "Epoch: 18/20... Step: 17030... Loss: 3.2021... Val Loss: 3.2079\n",
      "Epoch: 18/20... Step: 17040... Loss: 3.1770... Val Loss: 3.1835\n",
      "Epoch: 18/20... Step: 17050... Loss: 3.2042... Val Loss: 3.1950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20... Step: 17060... Loss: 3.2319... Val Loss: 3.2093\n",
      "Epoch: 18/20... Step: 17070... Loss: 3.2012... Val Loss: 3.2143\n",
      "Epoch: 18/20... Step: 17080... Loss: 3.2153... Val Loss: 3.1848\n",
      "Epoch: 18/20... Step: 17090... Loss: 3.2069... Val Loss: 3.2081\n",
      "Epoch: 18/20... Step: 17100... Loss: 3.2040... Val Loss: 3.1919\n",
      "Epoch: 18/20... Step: 17110... Loss: 3.2182... Val Loss: 3.2127\n",
      "Epoch: 18/20... Step: 17120... Loss: 3.2141... Val Loss: 3.1815\n",
      "Epoch: 18/20... Step: 17130... Loss: 3.2064... Val Loss: 3.1969\n",
      "Epoch: 18/20... Step: 17140... Loss: 3.2169... Val Loss: 3.1996\n",
      "Epoch: 18/20... Step: 17150... Loss: 3.2129... Val Loss: 3.2053\n",
      "Epoch: 18/20... Step: 17160... Loss: 3.2232... Val Loss: 3.1940\n",
      "Epoch: 18/20... Step: 17170... Loss: 3.2178... Val Loss: 3.1945\n",
      "Epoch: 18/20... Step: 17180... Loss: 3.2072... Val Loss: 3.1989\n",
      "Epoch: 18/20... Step: 17190... Loss: 3.1915... Val Loss: 3.1707\n",
      "Epoch: 18/20... Step: 17200... Loss: 3.2012... Val Loss: 3.2017\n",
      "Epoch: 18/20... Step: 17210... Loss: 3.1826... Val Loss: 3.2016\n",
      "Epoch: 18/20... Step: 17220... Loss: 3.2099... Val Loss: 3.1976\n",
      "Epoch: 18/20... Step: 17230... Loss: 3.2000... Val Loss: 3.2046\n",
      "Epoch: 18/20... Step: 17240... Loss: 3.2116... Val Loss: 3.2321\n",
      "Epoch: 18/20... Step: 17250... Loss: 3.1781... Val Loss: 3.2007\n",
      "Epoch: 18/20... Step: 17260... Loss: 3.2041... Val Loss: 3.2051\n",
      "Epoch: 18/20... Step: 17270... Loss: 3.2513... Val Loss: 3.2069\n",
      "Epoch: 18/20... Step: 17280... Loss: 3.1816... Val Loss: 3.1811\n",
      "Epoch: 18/20... Step: 17290... Loss: 3.1968... Val Loss: 3.1811\n",
      "Epoch: 18/20... Step: 17300... Loss: 3.1950... Val Loss: 3.1869\n",
      "Epoch: 18/20... Step: 17310... Loss: 3.2156... Val Loss: 3.1947\n",
      "Epoch: 18/20... Step: 17320... Loss: 3.2299... Val Loss: 3.2104\n",
      "Epoch: 18/20... Step: 17330... Loss: 3.1845... Val Loss: 3.2082\n",
      "Epoch: 18/20... Step: 17340... Loss: 3.2027... Val Loss: 3.1934\n",
      "Epoch: 18/20... Step: 17350... Loss: 3.2536... Val Loss: 3.2160\n",
      "Epoch: 18/20... Step: 17360... Loss: 3.2212... Val Loss: 3.1971\n",
      "Epoch: 18/20... Step: 17370... Loss: 3.1683... Val Loss: 3.2338\n",
      "Epoch: 18/20... Step: 17380... Loss: 3.2074... Val Loss: 3.2156\n",
      "Epoch: 18/20... Step: 17390... Loss: 3.2021... Val Loss: 3.2031\n",
      "Epoch: 18/20... Step: 17400... Loss: 3.1894... Val Loss: 3.2182\n",
      "Epoch: 18/20... Step: 17410... Loss: 3.2072... Val Loss: 3.1925\n",
      "Epoch: 18/20... Step: 17420... Loss: 3.2220... Val Loss: 3.1914\n",
      "Epoch: 18/20... Step: 17430... Loss: 3.1998... Val Loss: 3.2131\n",
      "Epoch: 18/20... Step: 17440... Loss: 3.2207... Val Loss: 3.2063\n",
      "Epoch: 18/20... Step: 17450... Loss: 3.1933... Val Loss: 3.2231\n",
      "Epoch: 18/20... Step: 17460... Loss: 3.2289... Val Loss: 3.2402\n",
      "Epoch: 18/20... Step: 17470... Loss: 3.2012... Val Loss: 3.2149\n",
      "Epoch: 18/20... Step: 17480... Loss: 3.2132... Val Loss: 3.2099\n",
      "Epoch: 18/20... Step: 17490... Loss: 3.2006... Val Loss: 3.2094\n",
      "Epoch: 18/20... Step: 17500... Loss: 3.2123... Val Loss: 3.2093\n",
      "Epoch: 18/20... Step: 17510... Loss: 3.2016... Val Loss: 3.2290\n",
      "Epoch: 18/20... Step: 17520... Loss: 3.1866... Val Loss: 3.2027\n",
      "Epoch: 18/20... Step: 17530... Loss: 3.1986... Val Loss: 3.2135\n",
      "Epoch: 18/20... Step: 17540... Loss: 3.1973... Val Loss: 3.1831\n",
      "Epoch: 18/20... Step: 17550... Loss: 3.2121... Val Loss: 3.1869\n",
      "Epoch: 18/20... Step: 17560... Loss: 3.2034... Val Loss: 3.2050\n",
      "Epoch: 18/20... Step: 17570... Loss: 3.2034... Val Loss: 3.2078\n",
      "Epoch: 18/20... Step: 17580... Loss: 3.1941... Val Loss: 3.1821\n",
      "Epoch: 18/20... Step: 17590... Loss: 3.1791... Val Loss: 3.1957\n",
      "Epoch: 18/20... Step: 17600... Loss: 3.2263... Val Loss: 3.2155\n",
      "Epoch: 18/20... Step: 17610... Loss: 3.2269... Val Loss: 3.2200\n",
      "Epoch: 18/20... Step: 17620... Loss: 3.2194... Val Loss: 3.2076\n",
      "Epoch: 18/20... Step: 17630... Loss: 3.1987... Val Loss: 3.2055\n",
      "Epoch: 18/20... Step: 17640... Loss: 3.2123... Val Loss: 3.2025\n",
      "Epoch: 18/20... Step: 17650... Loss: 3.1981... Val Loss: 3.1968\n",
      "Epoch: 18/20... Step: 17660... Loss: 3.2014... Val Loss: 3.2145\n",
      "Epoch: 18/20... Step: 17670... Loss: 3.2006... Val Loss: 3.1999\n",
      "Epoch: 18/20... Step: 17680... Loss: 3.2101... Val Loss: 3.2204\n",
      "Epoch: 18/20... Step: 17690... Loss: 3.2007... Val Loss: 3.2037\n",
      "Epoch: 18/20... Step: 17700... Loss: 3.2061... Val Loss: 3.2222\n",
      "Epoch: 18/20... Step: 17710... Loss: 3.1926... Val Loss: 3.2213\n",
      "Epoch: 18/20... Step: 17720... Loss: 3.2353... Val Loss: 3.1926\n",
      "Epoch: 18/20... Step: 17730... Loss: 3.2060... Val Loss: 3.2117\n",
      "Epoch: 18/20... Step: 17740... Loss: 3.2173... Val Loss: 3.1994\n",
      "Epoch: 18/20... Step: 17750... Loss: 3.2094... Val Loss: 3.2062\n",
      "Epoch: 18/20... Step: 17760... Loss: 3.2035... Val Loss: 3.2070\n",
      "Epoch: 18/20... Step: 17770... Loss: 3.2040... Val Loss: 3.2029\n",
      "Epoch: 18/20... Step: 17780... Loss: 3.1847... Val Loss: 3.1923\n",
      "Epoch: 18/20... Step: 17790... Loss: 3.2201... Val Loss: 3.2113\n",
      "Epoch: 18/20... Step: 17800... Loss: 3.1894... Val Loss: 3.2104\n",
      "Epoch: 18/20... Step: 17810... Loss: 3.1914... Val Loss: 3.1949\n",
      "Epoch: 18/20... Step: 17820... Loss: 3.1959... Val Loss: 3.2263\n",
      "Epoch: 18/20... Step: 17830... Loss: 3.1879... Val Loss: 3.2022\n",
      "Epoch: 18/20... Step: 17840... Loss: 3.2279... Val Loss: 3.1891\n",
      "Epoch: 18/20... Step: 17850... Loss: 3.2206... Val Loss: 3.1790\n",
      "Epoch: 18/20... Step: 17860... Loss: 3.2111... Val Loss: 3.1942\n",
      "Epoch: 18/20... Step: 17870... Loss: 3.1959... Val Loss: 3.2197\n",
      "Epoch: 18/20... Step: 17880... Loss: 3.2032... Val Loss: 3.2019\n",
      "Epoch: 18/20... Step: 17890... Loss: 3.1956... Val Loss: 3.2187\n",
      "Epoch: 18/20... Step: 17900... Loss: 3.2262... Val Loss: 3.2029\n",
      "Epoch: 18/20... Step: 17910... Loss: 3.2234... Val Loss: 3.1859\n",
      "Epoch: 18/20... Step: 17920... Loss: 3.2080... Val Loss: 3.2239\n",
      "Epoch: 18/20... Step: 17930... Loss: 3.2189... Val Loss: 3.2074\n",
      "Epoch: 18/20... Step: 17940... Loss: 3.1918... Val Loss: 3.2095\n",
      "Epoch: 18/20... Step: 17950... Loss: 3.2291... Val Loss: 3.1990\n",
      "Epoch: 18/20... Step: 17960... Loss: 3.2085... Val Loss: 3.1973\n",
      "Epoch: 18/20... Step: 17970... Loss: 3.2400... Val Loss: 3.2097\n",
      "Epoch: 18/20... Step: 17980... Loss: 3.1887... Val Loss: 3.1950\n",
      "Epoch: 18/20... Step: 17990... Loss: 3.1968... Val Loss: 3.1935\n",
      "Epoch: 19/20... Step: 18000... Loss: 3.1909... Val Loss: 3.2299\n",
      "Epoch: 19/20... Step: 18010... Loss: 3.1966... Val Loss: 3.1929\n",
      "Epoch: 19/20... Step: 18020... Loss: 3.2202... Val Loss: 3.2092\n",
      "Epoch: 19/20... Step: 18030... Loss: 3.2077... Val Loss: 3.2070\n",
      "Epoch: 19/20... Step: 18040... Loss: 3.2085... Val Loss: 3.1843\n",
      "Epoch: 19/20... Step: 18050... Loss: 3.2030... Val Loss: 3.1968\n",
      "Epoch: 19/20... Step: 18060... Loss: 3.1954... Val Loss: 3.1997\n",
      "Epoch: 19/20... Step: 18070... Loss: 3.1916... Val Loss: 3.1917\n",
      "Epoch: 19/20... Step: 18080... Loss: 3.1856... Val Loss: 3.2125\n",
      "Epoch: 19/20... Step: 18090... Loss: 3.2053... Val Loss: 3.1950\n",
      "Epoch: 19/20... Step: 18100... Loss: 3.2074... Val Loss: 3.1966\n",
      "Epoch: 19/20... Step: 18110... Loss: 3.1992... Val Loss: 3.1977\n",
      "Epoch: 19/20... Step: 18120... Loss: 3.2082... Val Loss: 3.2134\n",
      "Epoch: 19/20... Step: 18130... Loss: 3.2051... Val Loss: 3.1983\n",
      "Epoch: 19/20... Step: 18140... Loss: 3.2098... Val Loss: 3.2166\n",
      "Epoch: 19/20... Step: 18150... Loss: 3.1938... Val Loss: 3.2157\n",
      "Epoch: 19/20... Step: 18160... Loss: 3.1967... Val Loss: 3.2235\n",
      "Epoch: 19/20... Step: 18170... Loss: 3.2390... Val Loss: 3.2045\n",
      "Epoch: 19/20... Step: 18180... Loss: 3.2002... Val Loss: 3.1897\n",
      "Epoch: 19/20... Step: 18190... Loss: 3.2010... Val Loss: 3.1801\n",
      "Epoch: 19/20... Step: 18200... Loss: 3.1938... Val Loss: 3.1948\n",
      "Epoch: 19/20... Step: 18210... Loss: 3.2101... Val Loss: 3.1857\n",
      "Epoch: 19/20... Step: 18220... Loss: 3.2115... Val Loss: 3.2344\n",
      "Epoch: 19/20... Step: 18230... Loss: 3.2197... Val Loss: 3.2134\n",
      "Epoch: 19/20... Step: 18240... Loss: 3.1935... Val Loss: 3.2017\n",
      "Epoch: 19/20... Step: 18250... Loss: 3.1942... Val Loss: 3.1921\n",
      "Epoch: 19/20... Step: 18260... Loss: 3.1961... Val Loss: 3.2032\n",
      "Epoch: 19/20... Step: 18270... Loss: 3.1821... Val Loss: 3.2308\n",
      "Epoch: 19/20... Step: 18280... Loss: 3.2021... Val Loss: 3.2238\n",
      "Epoch: 19/20... Step: 18290... Loss: 3.2041... Val Loss: 3.2138\n",
      "Epoch: 19/20... Step: 18300... Loss: 3.1913... Val Loss: 3.1947\n",
      "Epoch: 19/20... Step: 18310... Loss: 3.2227... Val Loss: 3.2211\n",
      "Epoch: 19/20... Step: 18320... Loss: 3.1987... Val Loss: 3.2038\n",
      "Epoch: 19/20... Step: 18330... Loss: 3.1973... Val Loss: 3.2121\n",
      "Epoch: 19/20... Step: 18340... Loss: 3.2078... Val Loss: 3.2529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20... Step: 18350... Loss: 3.1909... Val Loss: 3.1908\n",
      "Epoch: 19/20... Step: 18360... Loss: 3.1834... Val Loss: 3.1843\n",
      "Epoch: 19/20... Step: 18370... Loss: 3.1752... Val Loss: 3.1805\n",
      "Epoch: 19/20... Step: 18380... Loss: 3.2094... Val Loss: 3.2100\n",
      "Epoch: 19/20... Step: 18390... Loss: 3.1929... Val Loss: 3.1989\n",
      "Epoch: 19/20... Step: 18400... Loss: 3.2052... Val Loss: 3.2116\n",
      "Epoch: 19/20... Step: 18410... Loss: 3.2096... Val Loss: 3.2033\n",
      "Epoch: 19/20... Step: 18420... Loss: 3.2333... Val Loss: 3.2068\n",
      "Epoch: 19/20... Step: 18430... Loss: 3.2134... Val Loss: 3.1964\n",
      "Epoch: 19/20... Step: 18440... Loss: 3.2091... Val Loss: 3.2227\n",
      "Epoch: 19/20... Step: 18450... Loss: 3.1980... Val Loss: 3.2135\n",
      "Epoch: 19/20... Step: 18460... Loss: 3.2222... Val Loss: 3.1932\n",
      "Epoch: 19/20... Step: 18470... Loss: 3.2145... Val Loss: 3.2195\n",
      "Epoch: 19/20... Step: 18480... Loss: 3.2185... Val Loss: 3.2003\n",
      "Epoch: 19/20... Step: 18490... Loss: 3.1896... Val Loss: 3.2285\n",
      "Epoch: 19/20... Step: 18500... Loss: 3.2001... Val Loss: 3.2039\n",
      "Epoch: 19/20... Step: 18510... Loss: 3.1806... Val Loss: 3.1817\n",
      "Epoch: 19/20... Step: 18520... Loss: 3.1789... Val Loss: 3.2446\n",
      "Epoch: 19/20... Step: 18530... Loss: 3.1957... Val Loss: 3.2229\n",
      "Epoch: 19/20... Step: 18540... Loss: 3.2095... Val Loss: 3.1928\n",
      "Epoch: 19/20... Step: 18550... Loss: 3.2064... Val Loss: 3.2243\n",
      "Epoch: 19/20... Step: 18560... Loss: 3.1949... Val Loss: 3.1843\n",
      "Epoch: 19/20... Step: 18570... Loss: 3.1961... Val Loss: 3.1992\n",
      "Epoch: 19/20... Step: 18580... Loss: 3.1955... Val Loss: 3.2068\n",
      "Epoch: 19/20... Step: 18590... Loss: 3.1906... Val Loss: 3.1951\n",
      "Epoch: 19/20... Step: 18600... Loss: 3.2051... Val Loss: 3.1898\n",
      "Epoch: 19/20... Step: 18610... Loss: 3.1899... Val Loss: 3.1982\n",
      "Epoch: 19/20... Step: 18620... Loss: 3.1933... Val Loss: 3.2148\n",
      "Epoch: 19/20... Step: 18630... Loss: 3.2106... Val Loss: 3.2146\n",
      "Epoch: 19/20... Step: 18640... Loss: 3.2016... Val Loss: 3.2226\n",
      "Epoch: 19/20... Step: 18650... Loss: 3.1966... Val Loss: 3.2250\n",
      "Epoch: 19/20... Step: 18660... Loss: 3.2086... Val Loss: 3.1943\n",
      "Epoch: 19/20... Step: 18670... Loss: 3.1994... Val Loss: 3.1965\n",
      "Epoch: 19/20... Step: 18680... Loss: 3.2177... Val Loss: 3.2176\n",
      "Epoch: 19/20... Step: 18690... Loss: 3.1981... Val Loss: 3.2263\n",
      "Epoch: 19/20... Step: 18700... Loss: 3.2116... Val Loss: 3.1653\n",
      "Epoch: 19/20... Step: 18710... Loss: 3.2153... Val Loss: 3.1994\n",
      "Epoch: 19/20... Step: 18720... Loss: 3.2190... Val Loss: 3.2047\n",
      "Epoch: 19/20... Step: 18730... Loss: 3.1883... Val Loss: 3.2195\n",
      "Epoch: 19/20... Step: 18740... Loss: 3.1980... Val Loss: 3.2111\n",
      "Epoch: 19/20... Step: 18750... Loss: 3.2004... Val Loss: 3.1682\n",
      "Epoch: 19/20... Step: 18760... Loss: 3.2098... Val Loss: 3.2068\n",
      "Epoch: 19/20... Step: 18770... Loss: 3.1947... Val Loss: 3.2061\n",
      "Epoch: 19/20... Step: 18780... Loss: 3.1909... Val Loss: 3.1809\n",
      "Epoch: 19/20... Step: 18790... Loss: 3.1922... Val Loss: 3.2043\n",
      "Epoch: 19/20... Step: 18800... Loss: 3.1975... Val Loss: 3.1988\n",
      "Epoch: 19/20... Step: 18810... Loss: 3.2043... Val Loss: 3.2115\n",
      "Epoch: 19/20... Step: 18820... Loss: 3.1975... Val Loss: 3.2207\n",
      "Epoch: 19/20... Step: 18830... Loss: 3.2019... Val Loss: 3.2256\n",
      "Epoch: 19/20... Step: 18840... Loss: 3.1927... Val Loss: 3.1789\n",
      "Epoch: 19/20... Step: 18850... Loss: 3.1977... Val Loss: 3.2138\n",
      "Epoch: 19/20... Step: 18860... Loss: 3.1907... Val Loss: 3.2197\n",
      "Epoch: 19/20... Step: 18870... Loss: 3.2027... Val Loss: 3.1914\n",
      "Epoch: 19/20... Step: 18880... Loss: 3.2104... Val Loss: 3.2066\n",
      "Epoch: 19/20... Step: 18890... Loss: 3.1953... Val Loss: 3.2088\n",
      "Epoch: 19/20... Step: 18900... Loss: 3.1902... Val Loss: 3.1972\n",
      "Epoch: 19/20... Step: 18910... Loss: 3.1969... Val Loss: 3.2223\n",
      "Epoch: 19/20... Step: 18920... Loss: 3.2288... Val Loss: 3.2364\n",
      "Epoch: 19/20... Step: 18930... Loss: 3.2029... Val Loss: 3.1854\n",
      "Epoch: 19/20... Step: 18940... Loss: 3.1992... Val Loss: 3.2348\n",
      "Epoch: 19/20... Step: 18950... Loss: 3.1957... Val Loss: 3.2259\n",
      "Epoch: 19/20... Step: 18960... Loss: 3.2120... Val Loss: 3.2123\n",
      "Epoch: 19/20... Step: 18970... Loss: 3.1999... Val Loss: 3.1852\n",
      "Epoch: 19/20... Step: 18980... Loss: 3.1893... Val Loss: 3.1814\n",
      "Epoch: 19/20... Step: 18990... Loss: 3.2063... Val Loss: 3.2110\n",
      "Epoch: 20/20... Step: 19000... Loss: 3.1809... Val Loss: 3.1843\n",
      "Epoch: 20/20... Step: 19010... Loss: 3.1983... Val Loss: 3.2252\n",
      "Epoch: 20/20... Step: 19020... Loss: 3.2297... Val Loss: 3.1974\n",
      "Epoch: 20/20... Step: 19030... Loss: 3.1980... Val Loss: 3.2040\n",
      "Epoch: 20/20... Step: 19040... Loss: 3.1706... Val Loss: 3.1886\n",
      "Epoch: 20/20... Step: 19050... Loss: 3.2286... Val Loss: 3.2048\n",
      "Epoch: 20/20... Step: 19060... Loss: 3.2061... Val Loss: 3.1906\n",
      "Epoch: 20/20... Step: 19070... Loss: 3.2026... Val Loss: 3.2328\n",
      "Epoch: 20/20... Step: 19080... Loss: 3.2156... Val Loss: 3.1963\n",
      "Epoch: 20/20... Step: 19090... Loss: 3.1866... Val Loss: 3.1800\n",
      "Epoch: 20/20... Step: 19100... Loss: 3.1916... Val Loss: 3.1886\n",
      "Epoch: 20/20... Step: 19110... Loss: 3.1853... Val Loss: 3.2062\n",
      "Epoch: 20/20... Step: 19120... Loss: 3.1793... Val Loss: 3.1676\n",
      "Epoch: 20/20... Step: 19130... Loss: 3.1931... Val Loss: 3.2057\n",
      "Epoch: 20/20... Step: 19140... Loss: 3.2151... Val Loss: 3.1932\n",
      "Epoch: 20/20... Step: 19150... Loss: 3.1948... Val Loss: 3.2154\n",
      "Epoch: 20/20... Step: 19160... Loss: 3.2263... Val Loss: 3.1782\n",
      "Epoch: 20/20... Step: 19170... Loss: 3.2122... Val Loss: 3.2112\n",
      "Epoch: 20/20... Step: 19180... Loss: 3.1882... Val Loss: 3.2091\n",
      "Epoch: 20/20... Step: 19190... Loss: 3.1866... Val Loss: 3.2151\n",
      "Epoch: 20/20... Step: 19200... Loss: 3.2190... Val Loss: 3.2161\n",
      "Epoch: 20/20... Step: 19210... Loss: 3.2138... Val Loss: 3.2444\n",
      "Epoch: 20/20... Step: 19220... Loss: 3.2227... Val Loss: 3.1978\n",
      "Epoch: 20/20... Step: 19230... Loss: 3.1686... Val Loss: 3.1850\n",
      "Epoch: 20/20... Step: 19240... Loss: 3.1944... Val Loss: 3.2041\n",
      "Epoch: 20/20... Step: 19250... Loss: 3.2311... Val Loss: 3.2172\n",
      "Epoch: 20/20... Step: 19260... Loss: 3.2129... Val Loss: 3.2019\n",
      "Epoch: 20/20... Step: 19270... Loss: 3.2075... Val Loss: 3.2022\n",
      "Epoch: 20/20... Step: 19280... Loss: 3.1939... Val Loss: 3.2061\n",
      "Epoch: 20/20... Step: 19290... Loss: 3.2081... Val Loss: 3.2191\n",
      "Epoch: 20/20... Step: 19300... Loss: 3.1843... Val Loss: 3.1931\n",
      "Epoch: 20/20... Step: 19310... Loss: 3.1877... Val Loss: 3.2184\n",
      "Epoch: 20/20... Step: 19320... Loss: 3.1997... Val Loss: 3.1819\n",
      "Epoch: 20/20... Step: 19330... Loss: 3.2098... Val Loss: 3.2138\n",
      "Epoch: 20/20... Step: 19340... Loss: 3.1865... Val Loss: 3.2090\n",
      "Epoch: 20/20... Step: 19350... Loss: 3.2217... Val Loss: 3.2051\n",
      "Epoch: 20/20... Step: 19360... Loss: 3.2152... Val Loss: 3.2197\n",
      "Epoch: 20/20... Step: 19370... Loss: 3.2303... Val Loss: 3.2050\n",
      "Epoch: 20/20... Step: 19380... Loss: 3.2052... Val Loss: 3.1977\n",
      "Epoch: 20/20... Step: 19390... Loss: 3.1951... Val Loss: 3.2250\n",
      "Epoch: 20/20... Step: 19400... Loss: 3.2088... Val Loss: 3.2133\n",
      "Epoch: 20/20... Step: 19410... Loss: 3.2079... Val Loss: 3.1909\n",
      "Epoch: 20/20... Step: 19420... Loss: 3.1956... Val Loss: 3.2000\n",
      "Epoch: 20/20... Step: 19430... Loss: 3.2000... Val Loss: 3.2208\n",
      "Epoch: 20/20... Step: 19440... Loss: 3.2077... Val Loss: 3.2053\n",
      "Epoch: 20/20... Step: 19450... Loss: 3.1949... Val Loss: 3.1898\n",
      "Epoch: 20/20... Step: 19460... Loss: 3.1804... Val Loss: 3.2044\n",
      "Epoch: 20/20... Step: 19470... Loss: 3.1839... Val Loss: 3.2009\n",
      "Epoch: 20/20... Step: 19480... Loss: 3.2122... Val Loss: 3.1844\n",
      "Epoch: 20/20... Step: 19490... Loss: 3.1951... Val Loss: 3.1845\n",
      "Epoch: 20/20... Step: 19500... Loss: 3.1927... Val Loss: 3.2045\n",
      "Epoch: 20/20... Step: 19510... Loss: 3.2164... Val Loss: 3.1663\n",
      "Epoch: 20/20... Step: 19520... Loss: 3.1924... Val Loss: 3.1995\n",
      "Epoch: 20/20... Step: 19530... Loss: 3.2148... Val Loss: 3.1989\n",
      "Epoch: 20/20... Step: 19540... Loss: 3.1929... Val Loss: 3.2103\n",
      "Epoch: 20/20... Step: 19550... Loss: 3.2460... Val Loss: 3.1950\n",
      "Epoch: 20/20... Step: 19560... Loss: 3.1942... Val Loss: 3.1959\n",
      "Epoch: 20/20... Step: 19570... Loss: 3.2045... Val Loss: 3.2004\n",
      "Epoch: 20/20... Step: 19580... Loss: 3.2104... Val Loss: 3.1894\n",
      "Epoch: 20/20... Step: 19590... Loss: 3.2154... Val Loss: 3.2062\n",
      "Epoch: 20/20... Step: 19600... Loss: 3.1860... Val Loss: 3.2024\n",
      "Epoch: 20/20... Step: 19610... Loss: 3.2213... Val Loss: 3.2073\n",
      "Epoch: 20/20... Step: 19620... Loss: 3.2217... Val Loss: 3.2401\n",
      "Epoch: 20/20... Step: 19630... Loss: 3.1885... Val Loss: 3.1901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20... Step: 19640... Loss: 3.1992... Val Loss: 3.1887\n",
      "Epoch: 20/20... Step: 19650... Loss: 3.2180... Val Loss: 3.1894\n",
      "Epoch: 20/20... Step: 19660... Loss: 3.2195... Val Loss: 3.2012\n",
      "Epoch: 20/20... Step: 19670... Loss: 3.1972... Val Loss: 3.1962\n",
      "Epoch: 20/20... Step: 19680... Loss: 3.1922... Val Loss: 3.1865\n",
      "Epoch: 20/20... Step: 19690... Loss: 3.1803... Val Loss: 3.1849\n",
      "Epoch: 20/20... Step: 19700... Loss: 3.1884... Val Loss: 3.1970\n",
      "Epoch: 20/20... Step: 19710... Loss: 3.1728... Val Loss: 3.2211\n",
      "Epoch: 20/20... Step: 19720... Loss: 3.2001... Val Loss: 3.2091\n",
      "Epoch: 20/20... Step: 19730... Loss: 3.1895... Val Loss: 3.2030\n",
      "Epoch: 20/20... Step: 19740... Loss: 3.1911... Val Loss: 3.2103\n",
      "Epoch: 20/20... Step: 19750... Loss: 3.2128... Val Loss: 3.1875\n",
      "Epoch: 20/20... Step: 19760... Loss: 3.2036... Val Loss: 3.2084\n",
      "Epoch: 20/20... Step: 19770... Loss: 3.2027... Val Loss: 3.2015\n",
      "Epoch: 20/20... Step: 19780... Loss: 3.2131... Val Loss: 3.2095\n",
      "Epoch: 20/20... Step: 19790... Loss: 3.1962... Val Loss: 3.2156\n",
      "Epoch: 20/20... Step: 19800... Loss: 3.1977... Val Loss: 3.2073\n",
      "Epoch: 20/20... Step: 19810... Loss: 3.1997... Val Loss: 3.1899\n",
      "Epoch: 20/20... Step: 19820... Loss: 3.1970... Val Loss: 3.2207\n",
      "Epoch: 20/20... Step: 19830... Loss: 3.2138... Val Loss: 3.1876\n",
      "Epoch: 20/20... Step: 19840... Loss: 3.1783... Val Loss: 3.2143\n",
      "Epoch: 20/20... Step: 19850... Loss: 3.2215... Val Loss: 3.2024\n",
      "Epoch: 20/20... Step: 19860... Loss: 3.1853... Val Loss: 3.2221\n",
      "Epoch: 20/20... Step: 19870... Loss: 3.1963... Val Loss: 3.2064\n",
      "Epoch: 20/20... Step: 19880... Loss: 3.1957... Val Loss: 3.2197\n",
      "Epoch: 20/20... Step: 19890... Loss: 3.2083... Val Loss: 3.2210\n",
      "Epoch: 20/20... Step: 19900... Loss: 3.2208... Val Loss: 3.2044\n",
      "Epoch: 20/20... Step: 19910... Loss: 3.1824... Val Loss: 3.1932\n",
      "Epoch: 20/20... Step: 19920... Loss: 3.1967... Val Loss: 3.2138\n",
      "Epoch: 20/20... Step: 19930... Loss: 3.1972... Val Loss: 3.1973\n",
      "Epoch: 20/20... Step: 19940... Loss: 3.1775... Val Loss: 3.1795\n",
      "Epoch: 20/20... Step: 19950... Loss: 3.1904... Val Loss: 3.2224\n",
      "Epoch: 20/20... Step: 19960... Loss: 3.1832... Val Loss: 3.1790\n",
      "Epoch: 20/20... Step: 19970... Loss: 3.2028... Val Loss: 3.1721\n",
      "Epoch: 20/20... Step: 19980... Loss: 3.2123... Val Loss: 3.1944\n",
      "Epoch: 20/20... Step: 19990... Loss: 3.2233... Val Loss: 3.2141\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    net,\n",
    "    full_train=True,\n",
    "    train_data=train_dataloader,\n",
    "    val_data=validation_dataloader,\n",
    "    epochs=20,\n",
    "    batches_per_epoch = 1000,\n",
    "    batch_size=batch_size,\n",
    "    seq_length=seq_length,\n",
    "    lr=0.01,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cabf547",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'output_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sample(net, 400, prime='хехехе ', temperature = 0.4, top_k=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a99cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_hidden=1024\n",
    "# n_layers=5\n",
    "\n",
    "# net = CharRNN(len(alphabet),n_hidden, n_layers).cuda()\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5491c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(\n",
    "#     net,\n",
    "#     full_train=True,\n",
    "#     train_data=train_dataloader,\n",
    "#     val_data=validation_dataloader,\n",
    "#     epochs=50,\n",
    "#     batches_per_epoch = 1000,\n",
    "#     batch_size=batch_size,\n",
    "#     seq_length=seq_length,\n",
    "#     lr=0.001,\n",
    "#     print_every=10,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5110daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ebbbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'anek_rnn_shtirliz_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe0b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = {'n_hidden': net.n_hidden,\n",
    "#               'n_layers': net.n_layers,\n",
    "#               'state_dict': net.state_dict(),\n",
    "#               'tokens': net.chars}\n",
    "\n",
    "# with open('rnn_poetry_10_epoch', 'wb') as f:\n",
    "#     torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample(net, 600, prime='Почему в России', temperature = 0.7, top_k=7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
